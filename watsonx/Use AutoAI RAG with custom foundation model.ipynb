{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://raw.githubusercontent.com/IBM/watson-machine-learning-samples/master/cloud/notebooks/headers/watsonx-Prompt_Lab-Notebook.png)\n",
    "# AutoAI RAG experiment with custom foundation model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Disclaimers\n",
    "\n",
    "- Use only Projects and Spaces that are available in the watsonx context.\n",
    "\n",
    "\n",
    "## Notebook content\n",
    "\n",
    "This notebook demonstrates how to deploy custom foundation model and use this model in AutoAI RAG experiment.\n",
    "The data used in this notebook is from the [Granite Code Models paper](https://arxiv.org/pdf/2405.04324).\n",
    "\n",
    "Some familiarity with Python is helpful. This notebook uses Python 3.11.\n",
    "\n",
    "\n",
    "## Learning goal\n",
    "\n",
    "The learning goals of this notebook are:\n",
    "\n",
    "- How to deploy your own foundation models with huggingface hub\n",
    "- Create an AutoAI RAG job that will find the best RAG pattern based on custom foundation model used during the experiment\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook contains the following parts:\n",
    "- [Set up the environment](#Set-up-the-environment)\n",
    "- [Prerequisites](#Prerequisites)\n",
    "- [Create API Client instance.](#Create-API-Client-instance.)\n",
    "- [Download custom model from hugging face](#Download-custom-model-from-hugging-face)\n",
    "- [Deploy the model](#Deploy-the-model)\n",
    "- [Prepare the data for the AutoAI RAG experiment](#Prepare-the-data-for-the-AutoAI-RAG-experiment)\n",
    "- [Run the AutoAI RAG experiment](#Run-the-AutoAI-RAG-experiment)\n",
    "- [Query generated pattern locally](#Query-generated-pattern-locally)\n",
    "- [Summary](#Summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U wget | tail -n 1\n",
    "%pip install -U 'ibm-watsonx-ai[rag]>=1.3.12' | tail -n 1\n",
    "%pip install -U 'huggingface-hub==0.30.2' | tail -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prerequisites\"></a>\n",
    "\n",
    "## Prerequisites\n",
    "Please fill below values to be able to move forward:\n",
    "- API_KEY - your api key to IBM Cloud, more information about API keys can be found [here](https://cloud.ibm.com/docs/account?topic=account-userapikey&interface=ui).\n",
    "- WML_ENDPOINT - endpoint url associated with your api key, to see the list of available endpoints please refer to this [documentation](https://cloud.ibm.com/docs/cloud-object-storage?topic=cloud-object-storage-endpoints).\n",
    "- PROJECT_ID - ID of the project associated with your api key and endpoint, to find your project id please refer to this [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-project-id.html?context=wx).\n",
    "- DATASOURCE_CONNECTION_ASSET_ID - connection asset ID to your data source which will store custom foundation model files, please refer to this [documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/manage-data/create-conn.html?context=cpdaas) to get to know how to create this kind of asset. In the example below you will be using the connection to `S3 Cloud Object Storage`.\n",
    "- BUCKET_NAME - bucket which will store your custom foundation models files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"PUT YOUR API KEY HERE\" # API key to your IBM cloud or Cloud Pack for Data instance\n",
    "WML_ENDPOINT = \"PUT YOUR WML ENDPOINT HERE\" # endpoint associated with your API key\n",
    "PROJECT_ID = \"PUT YOUR PROJECT ID HERE\" # project ID associated with your API key and endpoint\n",
    "\n",
    "DATASOURCE_CONNECTION_ASSET_ID = \"PUT YOUR COS CONNECTION ID HERE\" # datasource connection inside your project\n",
    "BUCKET_NAME = \"PUT YOUR BUCKET NAME HERE\" # bucket name in your Cloud Object Storage\n",
    "BUCKET_MODEL_DIR_NAME = \"PLLuM12BInstruct\" # dir name inside the bucket which will store your custom model files\n",
    "\n",
    "HUGGING_FACE_MODEL_REPOSITORY = \"CYFRAGOVPL/PLLuM-12B-instruct\" # HuggingFace model repository\n",
    "BUCKET_BENCHMARK_JSON_FILE_PATH = \"benchmark.json\" # path inside bucket where your benchmark.json file is stored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create API Client instance.\n",
    "This client will allow us to connect with the IBM services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from ibm_watsonx_ai import APIClient, Credentials\n",
    "\n",
    "credentials = Credentials(\n",
    "                api_key=API_KEY,\n",
    "                url=WML_ENDPOINT\n",
    "            )\n",
    "\n",
    "client = APIClient(credentials=credentials,  project_id=PROJECT_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download custom model from hugging face\n",
    "\n",
    "`byom_cache_dir` - path to your local directory where you want to download foundation model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "byom_cache_dir = Path(\"your\", \"model\", \"cache\", \"dir\")\n",
    "\n",
    "if not byom_cache_dir.exists():\n",
    "    raise FileExistsError(\"Please use the path which exists.\")\n",
    "\n",
    "if byom_cache_dir.is_file():\n",
    "    raise NotADirectoryError(\"Please use the path which points to a directory.\")\n",
    "\n",
    "snapshot_download(HUGGING_FACE_MODEL_REPOSITORY, cache_dir=byom_cache_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create S3Location instance to connect to your COS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.helpers.connections import DataConnection, S3Location\n",
    "\n",
    "location = S3Location(bucket=BUCKET_NAME, path=BUCKET_MODEL_DIR_NAME)\n",
    "data_connection = DataConnection(location=location, connection_asset_id=DATASOURCE_CONNECTION_ASSET_ID)\n",
    "data_connection.set_client(api_client=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload foundation model files to your bucket\n",
    "\n",
    "`model_files` - this path has to navigate to your local directory with downloaded model, locate there `snapshots` directory and copy the subdirectory name which should look similar to this example: `61aafd9fccad1606b5c462196cd111e734b60781`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_files = byom_cache_dir / \"model_dir_name\" / \"snapshots\" / \"snapshot_id\"\n",
    "\n",
    "if not model_files.exists():\n",
    "    raise FileExistsError(\"Please use the snapshot path which exists.\")\n",
    "\n",
    "if model_files.is_file():\n",
    "    raise NotADirectoryError(\"Please use the snapshot path which points to a directory.\")\n",
    "\n",
    "for model_file in model_files.iterdir():\n",
    "    \n",
    "    # avoid uploading unnecessary files\n",
    "    if model_file.name.startswith(\".\"):\n",
    "        continue\n",
    "\n",
    "    data_connection.write(data=str(model_file), remote_name=model_file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the model\n",
    "Check the docs to avoid any problems during model deployment [here](https://ibm.github.io/watsonx-ai-python-sdk/fm_custom_models.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create custom model repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "software_spec = client.software_specifications.get_id_by_name('watsonx-cfm-caikit-1.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {\n",
    "    client.repository.ModelMetaNames.NAME: \"PLLuM deployment\",\n",
    "    client.repository.ModelMetaNames.SOFTWARE_SPEC_ID: software_spec,\n",
    "    client.repository.ModelMetaNames.TYPE: client.repository.ModelAssetTypes.CUSTOM_FOUNDATION_MODEL_1_0,\n",
    "    client.repository.ModelMetaNames.MODEL_LOCATION: {\n",
    "        \"file_path\": BUCKET_MODEL_DIR_NAME,\n",
    "        \"bucket\": BUCKET_NAME,\n",
    "        \"connection_id\": DATASOURCE_CONNECTION_ASSET_ID,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_model_details = client.repository.store_model(model=BUCKET_MODEL_DIR_NAME, meta_props=metadata)\n",
    "stored_model_asset_id = client.repository.get_model_id(stored_model_details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.repository.list(framework_filter='custom_foundation_model_1.1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store client task credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.task_credentials.store()\n",
    "except Exception:\n",
    "    print(\"Client task credentials have been already stored.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform custom model deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 32_000\n",
    "MAX_NEW_TOKENS = 1000\n",
    "MIN_NEW_TOKENS = 1\n",
    "MAX_BATCH_SIZE = 1024\n",
    "\n",
    "meta_props = {\n",
    "    client.deployments.ConfigurationMetaNames.NAME: \"PLLuM deployment\",\n",
    "    client.deployments.ConfigurationMetaNames.DESCRIPTION: \"PLLuM deployment\",\n",
    "    client.deployments.ConfigurationMetaNames.ONLINE: {},\n",
    "    client.deployments.ConfigurationMetaNames.HARDWARE_REQUEST: {\n",
    "        'size': client.deployments.HardwareRequestSizes.Small,\n",
    "        'num_nodes': 1\n",
    "    },\n",
    "    # optionally overwrite model parameters here\n",
    "    client.deployments.ConfigurationMetaNames.FOUNDATION_MODEL: {\"max_sequence_length\": MAX_SEQUENCE_LENGTH, \"max_new_tokens\": MAX_NEW_TOKENS, \"max_batch_size\": MAX_BATCH_SIZE},\n",
    "    client.deployments.ConfigurationMetaNames.SERVING_NAME: \"pllum_12b_instruct\" # must be unique\n",
    "}\n",
    "deployment_details = client.deployments.create(stored_model_asset_id, meta_props)\n",
    "deployment_id = deployment_details[\"metadata\"][\"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for the AutoAI RAG experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download `granite_code_models.pdf` document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wget\n",
    "\n",
    "data_url = \"https://arxiv.org/pdf/2405.04324\"\n",
    "byom_input_filename = \"granite_code_models.pdf\"\n",
    "wget.download(data_url, byom_input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save document in your desired COS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_dir_location = S3Location(bucket=BUCKET_NAME, path=byom_input_filename)\n",
    "documents_dir_data_connection = DataConnection(location=documents_dir_location, connection_asset_id=DATASOURCE_CONNECTION_ASSET_ID)\n",
    "documents_dir_data_connection.set_client(api_client=client)\n",
    "documents_dir_data_connection.write(data=byom_input_filename, remote_name=byom_input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create your own benchmark.json file to ask the questions related to the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "local_benchmark_json_filename = \"benchmark.json\"\n",
    "\n",
    "benchmarking_data = [\n",
    "     {\n",
    "        \"question\": \"What are the two main variants of Granite Code models?\",\n",
    "        \"correct_answer\": \"The two main variants are Granite Code Base and Granite Code Instruct.\",\n",
    "        \"correct_answer_document_ids\": [byom_input_filename]\n",
    "     },\n",
    "     {\n",
    "        \"question\": \"What is the purpose of Granite Code Instruct models?\",\n",
    "        \"correct_answer\": \"Granite Code Instruct models are finetuned for instruction-following tasks using datasets like CommitPack, OASST, HelpSteer, and synthetic code instruction datasets, aiming to improve reasoning and instruction-following capabilities.\",\n",
    "        \"correct_answer_document_ids\": [byom_input_filename]\n",
    "     },\n",
    "     {\n",
    "        \"question\": \"What is the licensing model for Granite Code models?\",\n",
    "        \"correct_answer\": \"Granite Code models are released under the Apache 2.0 license, ensuring permissive and enterprise-friendly usage.\",\n",
    "        \"correct_answer_document_ids\": [byom_input_filename]\n",
    "     },\n",
    "]\n",
    "\n",
    "with open(local_benchmark_json_filename, mode=\"w\", encoding=\"utf-8\") as fp:\n",
    "    json.dump(benchmarking_data, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save your benchmark.json in your COS bucket file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_file_location = S3Location(bucket=BUCKET_NAME, path=BUCKET_BENCHMARK_JSON_FILE_PATH)\n",
    "benchmark_file_data_connection = DataConnection(location=benchmark_file_location, connection_asset_id=DATASOURCE_CONNECTION_ASSET_ID)\n",
    "benchmark_file_data_connection.set_client(api_client=client)\n",
    "benchmark_file_data_connection.write(data=local_benchmark_json_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the AutoAI RAG experiment\n",
    "\n",
    "Provide the input information for AutoAI RAG optimizer:\n",
    "- `custom_prompt_template_text` - custom prompt template text which will be used to query your own foundation model\n",
    "- `custom_context_template_text` - custom context template text which will be used to query your own foundation model\n",
    "- `name` - experiment name\n",
    "- `description` - experiment description\n",
    "- `max_number_of_rag_patterns` - maximum number of RAG patterns to create\n",
    "- `optimization_metrics` - target optimization metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watsonx_ai.experiment import AutoAI\n",
    "from ibm_watsonx_ai.helpers.connections import ContainerLocation\n",
    "from ibm_watsonx_ai.foundation_models.schema import (\n",
    "        AutoAIRAGCustomModelConfig,\n",
    "        AutoAIRAGModelParams\n",
    ")\n",
    "\n",
    "experiment = AutoAI(credentials, project_id=PROJECT_ID)\n",
    "\n",
    "custom_prompt_template_text = \"Answer my question {question} related to these documents {reference_documents}.\"\n",
    "custom_context_template_text = \"My document {document}\"\n",
    "\n",
    "parameters = AutoAIRAGModelParams(max_sequence_length=32_000)\n",
    "pllum_foundation_model_config = AutoAIRAGCustomModelConfig(\n",
    "    deployment_id=deployment_id, \n",
    "    project_id=PROJECT_ID, \n",
    "    prompt_template_text=custom_prompt_template_text, \n",
    "    context_template_text=custom_context_template_text, \n",
    "    parameters=parameters\n",
    ")\n",
    "\n",
    "rag_optimizer = experiment.rag_optimizer(\n",
    "    name='AutoAI RAG - Custom foundation model experiment',\n",
    "    description = \"AutoAI RAG experiment using custom foundation model.\",\n",
    "    max_number_of_rag_patterns=4,\n",
    "    optimization_metrics=['faithfulness'],\n",
    "    foundation_models=[pllum_foundation_model_config]\n",
    ") \n",
    "\n",
    "\n",
    "container_data_location = DataConnection(\n",
    "        type=\"container\",\n",
    "        location=ContainerLocation(\n",
    "           path=\"autorag/results\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "container_data_location.set_client(api_client=client)\n",
    "\n",
    "rag_optimizer.run(\n",
    "    test_data_references=[benchmark_file_data_connection],\n",
    "    input_data_references=[documents_dir_data_connection],\n",
    "    results_reference=container_data_location,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_optimizer.get_run_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = rag_optimizer.summary()\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_pattern_name = summary.index.values[0]\n",
    "print('Best pattern is:', best_pattern_name)\n",
    "\n",
    "best_pattern = rag_optimizer.get_pattern()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_optimizer.get_pattern_details(pattern_name=best_pattern_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query generated pattern locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"What training objectives are used for the models?\"]\n",
    "\n",
    "payload = {\n",
    "    client.deployments.ScoringMetaNames.INPUT_DATA: [\n",
    "        {\n",
    "            \"values\": questions,\n",
    "            \"access_token\": client.service_instance._get_token()\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "resp = best_pattern.inference_function()(payload)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    " You successfully completed this notebook!\n",
    " \n",
    " You learned how to use AutoAI RAG with your own foundation model.\n",
    " \n",
    "Check out our _<a href=\"https://ibm.github.io/watsonx-ai-python-sdk/samples.html\" target=\"_blank\" rel=\"noopener no referrer\">Online Documentation</a>_ for more samples, tutorials, documentation, how-tos, and blog posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author:\n",
    " **Michał Steczko**, Software Engineer at watsonx.ai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2025 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autoai_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
