{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Create and run a data flow with IBM Watson Data APIs\n",
    "\n",
    "## Introduction\n",
    "Use the IBM Watson Data Flows service to create and run data flows in a runtime engine. A flow can read data from a large variety of sources, process that data using pre-defined operations or custom code, and then write it to one or more targets. The runtime engine can handle large amounts of data so it's ideally suited for reading, processing, and writing data at volume.\n",
    "\n",
    "The sources and targets that are supported include both Cloud and on-premises offerings as well as data assets in projects. Cloud offerings include IBM Cloud Object Storage, Amazon S3, and Azure, among others. On-premises offerings include IBM Db2, Microsoft SQL Server, and Oracle, among others.\n",
    "\n",
    "This notebook contains the steps and code to get you started with creating, running and debugging data flows using the Watson Data APIs. \n",
    "\n",
    "For a list of the supported connectivity and the properties they support see [IBM Watson Data Flows Service - Data Asset and Connection Properties](https://api.dataplatform.ibm.com/v2/data_flows/doc/dataasset_and_connection_properties.html).\n",
    "\n",
    "#### Language and Spark Versions\n",
    "Python 3.5<br>\n",
    "Spark 2.1\n",
    "\n",
    "#### Prerequisites\n",
    "This tutorial uses the [Customer demographics and sales](https://dataplatform.ibm.com/exchange/public/entry/view/f8ccaf607372882403a37d9019b3abf4) data set. You can add this to your project from the tile in the [Watson Studio](https://dataplatform.ibm.com/community?context=analytics)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Setup](#setup)<br>\n",
    "    1.1 [Environments](#setup1)<br>\n",
    "    1.2 [Project Token](#setup2)<br>\n",
    "    1.3 [Authorization](#setup3)<br>\n",
    "2. [Creating a data flow](#create)<br>\n",
    "    2.1 [Retrieving a data asset](#create1)<br>\n",
    "    2.2 [Defining a source in a data flow](#create2)<br>\n",
    "    2.3 [Defining an operation in a data flow](#create3)<br>\n",
    "    2.4 [Defining a target in a data flow](#create4)<br>\n",
    "    2.5 [Creating the data flow](#create5)<br>\n",
    "3. [Working with data flow runs](#run)<br>\n",
    "    3.1 [What is a data flow run?](#run1)<br>\n",
    "    3.2 [Run state life cycle](#run2)<br>\n",
    "    3.3 [Run a data flow](#run3)<br>\n",
    "    3.4 [Get a data flow run summary](#run4)<br>\n",
    "    3.5 [Troubleshooting a failed run](#run5)<br>\n",
    "4. [Resources](#resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "def pretty_print(json_content):\n",
    "    parsed_json = json.loads(json_content)\n",
    "    print(json.dumps(parsed_json, indent=4, sort_keys=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <a id=\"setup1\"></a>1.1 Environments\n",
    "The data flows service is currently deployed only to the US South region of IBM Cloud. Use this environment URL in place of {service_URL} in the examples below:\n",
    "\n",
    "    US south https://api.dataplatform.ibm.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_URL = \"https://api.dataplatform.ibm.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <a id=\"setup2\"></a>1.2 Project Token\n",
    "Insert a project token from the action bar (more > Insert project token). Project tokens are used to access project resources like data sources and connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @hidden_cell\n",
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_id = pc.projectID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  <a id=\"setup3\"></a>1.3 Authorization\n",
    "An IAM Bearer token is required in order to access IBM Watson Data APIs. For information on how to generate an IAM token see <a href=\"http://ibm.biz/wdp-api#getting\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace <IAM Access Token> with your generated IAM Access Token\n",
    "authorization = \"Bearer <IAM Access Token>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <a id=\"create\"></a>2. Creating a data flow ##\n",
    "The following example shows how to create a data flow that reads data from an existing data asset in an Watson Studio project, filters the data, and writes the data to a data asset in the same project. The data flow created for this example will contain a linear pipeline, although in the general case, the pipeline forms a directed acyclic graph (DAG).\n",
    "\n",
    "####  <a id=\"create1\"></a>2.1 Retrieving a data asset ####\n",
    "Begin by retrieving a list of data assets from a Watson Studio project and choose one to use as the source of the data flow. For further information on the data asset service, see <a href=\"http://ibm.biz/wdp-api#create-a-data-asset\" target=\"_blank\" rel=\"noopener noreferrer\">IBM Watson Data API documentation</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"results\": [\n",
      "        {\n",
      "            \"href\": \"https://catalogs-yp-prod.mybluemix.net/v2/assets/3621bfda-a92b-4802-9c0d-3eea9c2c0b82?project_id=d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "            \"metadata\": {\n",
      "                \"asset_attributes\": [\n",
      "                    \"data_asset\"\n",
      "                ],\n",
      "                \"asset_id\": \"3621bfda-a92b-4802-9c0d-3eea9c2c0b82\",\n",
      "                \"asset_state\": \"available\",\n",
      "                \"asset_type\": \"data_asset\",\n",
      "                \"catalog_id\": \"92f22af3-523b-4a35-8e16-e399d1c0e53e\",\n",
      "                \"created_at\": \"2017-12-19T16:34:47Z\",\n",
      "                \"name\": \"customers_orders1_opt.csv\",\n",
      "                \"origin_country\": \"us\",\n",
      "                \"owner\": \"******\",\n",
      "                \"rating\": 0,\n",
      "                \"rov\": {\n",
      "                    \"mode\": 0\n",
      "                },\n",
      "                \"sandbox_id\": \"d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "                \"size\": 5648773,\n",
      "                \"usage\": {\n",
      "                    \"access_count\": 0.0,\n",
      "                    \"last_access_time\": 1513701287935.0,\n",
      "                    \"last_accessor\": \"******\",\n",
      "                    \"last_update_time\": 1513701287935.0,\n",
      "                    \"last_updater\": \"******\"\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"total_rows\": 1\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# POST https://api.dataplatform.ibm.com/v2/asset_types/data_asset/search?project_id=d12f9685-2693-4c84-af48-3eb6c71e3013\n",
    "\n",
    "query = {\n",
    "    \"query\": \"asset.asset_type:data_asset\"\n",
    "}\n",
    "request = requests.post(service_URL + \"/v2/asset_types/data_asset/search?project_id=\" + project_id, headers={'Authorization': authorization}, json=query)\n",
    "pretty_print(request.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the response you can see that the data asset was created with the ID `3621bfda-a92b-4802-9c0d-3eea9c2c0b82`, which you'll need to use later and specify in the data flow you create.\n",
    "\n",
    "####  <a id=\"create2\"></a>2.2 Defining a source in a data flow ####\n",
    "A data flow can contain one or more data sources. A data source is defined as a *binding node* in the data flow *pipeline*, which has one output and no inputs. The *binding node* must reference either a connection or a data asset. Depending on the type of connection or data asset, additional *properties* might also need to be specified. Refer to [IBM Watson Data Flows Service - Data Asset and Connection Properties](https://api.dataplatform.ibm.com/v2/data_flows/doc/dataasset_and_connection_properties.html) to determine which properties are applicable for a given connection, and which of those are required. \n",
    "\n",
    "For the following example, reference the data asset you retrieved earlier. The *binding node* for the data flow's source is:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_binding_node = {  \n",
    "  \"id\":\"source1\",\n",
    "  \"type\":\"binding\",\n",
    "  \"output\":{  \n",
    "     \"id\":\"source1Output\"\n",
    "  },\n",
    "  \"data_asset\":{  \n",
    "     \"ref\":\"3621bfda-a92b-4802-9c0d-3eea9c2c0b82\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `output` attribute declares the ID of the *output port* of this source as `source1Output` so that other nodes can read from it. You can see the data asset with ID `3621bfda-a92b-4802-9c0d-3eea9c2c0b82` is being referenced.\n",
    "\n",
    "####  <a id=\"create3\"></a>2.3 Defining an operation in a data flow ####\n",
    "A data flow can contain zero or more operations, with a typical operation having one or more inputs and one or more outputs. An operation input is linked to the output of a source or another operation. An operation can also have additional parameters which define how the operation performs its work. An operation is defined as an *execution node* in the data flow *pipeline*.\n",
    "\n",
    "The following example creates a filter operation so that only rows where field `STATE` is not `\"\"` or empty are retained. The *execution node* for our filter operation is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_operation = {  \n",
    "  \"id\":\"operation1\",\n",
    "  \"type\":\"execution_node\",\n",
    "  \"op\":\"com.ibm.wdp.transformer.FreeformCode\",\n",
    "  \"parameters\":{  \n",
    "     \"FREEFORM_CODE\":\"filter(STATE!=\\\"\\\")\"\n",
    "  },\n",
    "  \"inputs\":[  \n",
    "     {  \n",
    "        \"id\":\"inputPort1\",\n",
    "        \"links\":[  \n",
    "           {  \n",
    "              \"node_id_ref\":\"source1\",\n",
    "              \"port_id_ref\":\"source1Output\"\n",
    "           }\n",
    "        ]\n",
    "     }\n",
    "  ],\n",
    "  \"outputs\":[  \n",
    "     {  \n",
    "        \"id\":\"outputPort1\"\n",
    "     }\n",
    "  ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `inputs` attribute declares an *input port* with ID `inputPort1` which references the *output port* of the source node (node ID `source1` and port ID `source1Output`). The `outputs` attribute declares the ID of the *output port* of this operation as `outputPort1` so that other nodes can read from it. For this example, the operation is defined as a freeform operation, denoted by the `op` attribute value of `com.ibm.wdp.transformer.FreeformCode`. A freeform operation has only a single parameter named `FREEFORM_CODE` whose value is a snippet of Sparklyr code. In this snippet of code, a filter function is called with the arguments to retain only those rows with a non empty value in the `STATE` field.\n",
    "\n",
    "The `outputs` attribute declares the ID of the output of this operation as `outputPort1` so that other nodes can read from it.\n",
    "\n",
    "####  <a id=\"create4\"></a>2.4 Defining a target in a data flow ####\n",
    "A data flow can contain zero or more targets. A target is defined as a *binding node* in the data flow *pipeline* which has one input and no outputs. As with the source, the *binding node* must reference either a connection or a data asset. When using a data asset as a target, specify either the ID or name of an existing data asset.\n",
    "\n",
    "In the following example, a data asset is referenced by its name. The *binding node* for the data flow's target is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_binding_node = {  \n",
    "  \"id\":\"target1\",\n",
    "  \"type\":\"binding\",\n",
    "  \"input\":{  \n",
    "     \"id\":\"target1Input\",\n",
    "     \"link\":{  \n",
    "        \"node_id_ref\":\"operation1\",\n",
    "        \"port_id_ref\":\"outputPort1\"\n",
    "     }\n",
    "  },\n",
    "  \"data_asset\":{  \n",
    "     \"properties\":{  \n",
    "        \"name\":\"my_shapedFile.csv\"\n",
    "     }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `input` attribute declares an *input port* with ID `target1Input` which references the *output port* of the operation node (node ID `operation1` and port ID `outputPort1`). The name of the data asset to create or update is specified as `my_shapedFile.csv`. Unless otherwise specified, this data asset is assumed to be in the same catalog or project as that which contains the data flow.\n",
    "\n",
    "####  <a id=\"create5\"></a>2.5 Creating the data flow ####\n",
    "Putting it all together, you can now call the API to create the data flow with the following POST method:\n",
    "\n",
    "```POST https://{service_URL}/v2/data_flows```\n",
    "\n",
    "The new data flow can be stored in a catalog or project. Use either the `catalog_id` **or** `project_id` query parameter, depending on where you want to store the data flow. An example request to create a data flow is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"entity\": {\n",
      "        \"name\": \"my_dataflow_0a67cdf4-0047-4ab0-8352-30ea2009af1a\",\n",
      "        \"pipeline\": {\n",
      "            \"doc_type\": \"pipeline\",\n",
      "            \"id\": \"53be959f-8cfd-4186-ba4a-d2bf869277a7\",\n",
      "            \"pipelines\": [\n",
      "                {\n",
      "                    \"id\": \"pipeline1\",\n",
      "                    \"nodes\": [\n",
      "                        {\n",
      "                            \"data_asset\": {\n",
      "                                \"ref\": \"3621bfda-a92b-4802-9c0d-3eea9c2c0b82\"\n",
      "                            },\n",
      "                            \"id\": \"source1\",\n",
      "                            \"output\": {\n",
      "                                \"id\": \"source1Output\"\n",
      "                            },\n",
      "                            \"type\": \"binding\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"id\": \"operation1\",\n",
      "                            \"inputs\": [\n",
      "                                {\n",
      "                                    \"id\": \"inputPort1\",\n",
      "                                    \"links\": [\n",
      "                                        {\n",
      "                                            \"node_id_ref\": \"source1\",\n",
      "                                            \"port_id_ref\": \"source1Output\"\n",
      "                                        }\n",
      "                                    ]\n",
      "                                }\n",
      "                            ],\n",
      "                            \"op\": \"com.ibm.wdp.transformer.FreeformCode\",\n",
      "                            \"outputs\": [\n",
      "                                {\n",
      "                                    \"id\": \"outputPort1\"\n",
      "                                }\n",
      "                            ],\n",
      "                            \"parameters\": {\n",
      "                                \"FREEFORM_CODE\": \"filter(STATE!=\\\"\\\")\"\n",
      "                            },\n",
      "                            \"type\": \"execution_node\"\n",
      "                        },\n",
      "                        {\n",
      "                            \"data_asset\": {\n",
      "                                \"properties\": {\n",
      "                                    \"name\": \"my_shapedFile.csv\"\n",
      "                                }\n",
      "                            },\n",
      "                            \"id\": \"target1\",\n",
      "                            \"input\": {\n",
      "                                \"id\": \"target1Input\",\n",
      "                                \"link\": {\n",
      "                                    \"node_id_ref\": \"operation1\",\n",
      "                                    \"port_id_ref\": \"outputPort1\"\n",
      "                                }\n",
      "                            },\n",
      "                            \"type\": \"binding\"\n",
      "                        }\n",
      "                    ],\n",
      "                    \"runtime\": \"Spark\"\n",
      "                }\n",
      "            ],\n",
      "            \"primary_pipeline\": \"pipeline1\",\n",
      "            \"version\": \"1.0\"\n",
      "        },\n",
      "        \"tags\": []\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"asset_id\": \"4dd043be-2bdb-4312-9d75-ddd2b045db55\",\n",
      "        \"asset_type\": \"data_flow\",\n",
      "        \"create_time\": \"2018-01-31T12:09:23.000Z\",\n",
      "        \"creator\": \"******\",\n",
      "        \"href\": \"https://api.dataplatform.ibm.com/v2/data_flows/4dd043be-2bdb-4312-9d75-ddd2b045db55?project_id=d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "        \"project_id\": \"d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "        \"usage\": {\n",
      "            \"access_count\": 0,\n",
      "            \"last_access_time\": \"2018-01-31T12:09:23.382Z\",\n",
      "            \"last_accessor\": \"******\",\n",
      "            \"last_modification_time\": \"2018-01-31T12:09:23.382Z\",\n",
      "            \"last_modifier\": \"******\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataflow = {  \n",
    "   \"name\":\"my_dataflow_\" + str(uuid.uuid4()),\n",
    "   \"pipeline\":{  \n",
    "      \"doc_type\":\"pipeline\",\n",
    "      \"version\":\"1.0\",\n",
    "      \"primary_pipeline\":\"pipeline1\",\n",
    "      \"pipelines\":[  \n",
    "         {  \n",
    "            \"id\":\"pipeline1\",\n",
    "            \"runtime\":\"Spark\",\n",
    "            \"nodes\":[  \n",
    "            ]\n",
    "         }\n",
    "      ]\n",
    "   }\n",
    "}\n",
    "\n",
    "dataflow[\"pipeline\"][\"pipelines\"][0][\"nodes\"].append(source_binding_node)\n",
    "dataflow[\"pipeline\"][\"pipelines\"][0][\"nodes\"].append(filter_operation)\n",
    "dataflow[\"pipeline\"][\"pipelines\"][0][\"nodes\"].append(target_binding_node)\n",
    "\n",
    "dataflow_response = requests.post(service_URL + \"/v2/data_flows?project_id=\" + project_id, headers={'Authorization': authorization}, json=dataflow)\n",
    "data_flow_id = json.loads(dataflow_response.text)[\"metadata\"][\"asset_id\"]\n",
    "pretty_print(dataflow_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The response shows that the data flow was created with an ID of `4dd043be-2bdb-4312-9d75-ddd2b045db55`, which you will need later to run the data flow you created.\n",
    "\n",
    "##  <a id=\"run\"></a>3. Working with data flow runs ##\n",
    "\n",
    "#### <a id=\"run1\"></a>3.1 What is a data flow run? ####\n",
    "Each time a data flow is run, a new data flow run asset is created and stored in the project or catalog to record this event. This asset stores detailed metrics such as how many rows were read and written, a copy of the data flow that was run, and any logs from the engine. During a run, the information in this asset is updated to reflect the current state of the run. When the run completes (successfully or not), the information in the asset is updated one final time. If and when the data flow is deleted, any run assets of that data flow are also deleted.\n",
    "\n",
    "There are four components of a data flow run, which are accessible using different APIs.\n",
    "\n",
    "- Summary (`GET /v2/data_flows/{data_flow_id}/runs/{data_flow_run_id}`). A quick, at-a-glance view of a run with a summary of how many rows in total were read and written.\n",
    "- Detailed metrics (`GET /v2/data_flows/{data_flow_id}/runs/{data_flow_run_id}/metrics`). Detailed metrics for each binding node in the data flow (link sources and targets).\n",
    "- Data flow (`GET /v2/data_flows/{data_flow_id}/runs/{data_flow_run_id}/origin`). A copy of the data flow that was run at that point in time. (Remember that data flows can be modified between runs.)\n",
    "- Logs (`GET /v2/data_flows/{data_flow_id}/runs/{data_flow_run_id}/logs`). The logs from the engine, which are useful for diagnosing run failures.\n",
    "\n",
    "#### <a id=\"run2\"></a>3.2 Run state life cycle ####\n",
    "A data flow run has a defined life cycle, which is shown by its `state` attribute. The `state` attribute can have one of the following values:\n",
    "\n",
    "- `starting` The run was created but was not yet submitted to the engine.\n",
    "- `queued` The run was submitted to the engine and it is pending.\n",
    "- `running` The run is currently in progress.\n",
    "- `finished` The run finished and was successful.\n",
    "- `error` The run did not complete. An error occurred either before the run was sent to the engine or while the run was in progress.\n",
    "- `stopping` The run was canceled but it is still running.\n",
    "- `stopped` The run is no longer in progress.\n",
    "\n",
    "The run states that define phases of progress are: `starting`, `queued`, `running`, `stopping`. The run states that define states of completion are:  `finished`, `error`, `stopped`.\n",
    "\n",
    "The following are typical state transitions you would expect to see:\n",
    "\n",
    "1. The run completed successfully: `starting` -> `queued` -> `running` -> `finished`.\n",
    "2. The run failed (for example, connection credentials were incorrect): `starting` -> `queued` -> `running` -> `error`.\n",
    "3. The run could not be sent to the engine (for example, the connection referenced does not exist): `starting` -> `error`.\n",
    "4. The run was stopped (for example, at users request): `starting` -> `queued` -> `running` -> `stopping` -> `stopped`.\n",
    "\n",
    "#### <a id=\"run3\"></a>3.3 Run a data flow ####\n",
    "To run a data flow, call the following POST API:\n",
    "\n",
    "```\n",
    "POST https://{service_URL}/v2/data_flows/{data_flow_id}/runs?project_id={project_id}\n",
    "```\n",
    "\n",
    "The value of `data_flow_id` is the `metadata.asset_id` from your data flow. An example response from this API call could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"entity\": {\n",
      "        \"configuration\": {},\n",
      "        \"data_flow_ref\": \"4dd043be-2bdb-4312-9d75-ddd2b045db55\",\n",
      "        \"name\": \"my_dataflow_0a67cdf4-0047-4ab0-8352-30ea2009af1a\",\n",
      "        \"rov\": {\n",
      "            \"members\": [],\n",
      "            \"mode\": 0\n",
      "        },\n",
      "        \"state\": \"starting\",\n",
      "        \"tags\": []\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"asset_id\": \"9f1fb8ca-5bd3-4bc3-8f23-00d53356883e\",\n",
      "        \"asset_type\": \"data_flow_run\",\n",
      "        \"create_time\": \"2018-01-31T12:10:00.000Z\",\n",
      "        \"creator\": \"******\",\n",
      "        \"href\": \"https://api.dataplatform.ibm.com/v2/data_flows/4dd043be-2bdb-4312-9d75-ddd2b045db55/runs/9f1fb8ca-5bd3-4bc3-8f23-00d53356883e?project_id=d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "        \"project_id\": \"d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "        \"usage\": {\n",
      "            \"access_count\": 0,\n",
      "            \"last_access_time\": \"2018-01-31T12:10:00.597Z\",\n",
      "            \"last_accessor\": \"******\",\n",
      "            \"last_modification_time\": \"2018-01-31T12:10:00.597Z\",\n",
      "            \"last_modifier\": \"******\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataflow_run_response = requests.post(service_URL + \"/v2/data_flows/\" + data_flow_id + \"/runs?project_id=\" + project_id, headers={'Authorization': authorization}, json={})\n",
    "data_flow_run_id = json.loads(dataflow_run_response.text)[\"metadata\"][\"asset_id\"]\n",
    "pretty_print(dataflow_run_response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"run4\"></a>3.4 Get a data flow run summary ####\n",
    "To retrieve the latest summary of a data flow run, call the following GET method:\n",
    "```\n",
    "GET https://{service_URL}/v2/data_flows/{data_flow_id}/runs/{data_flow_run_id}?project_id={project_id}\n",
    "```\n",
    "\n",
    "The value of `data_flow_id` is the `metadata.asset_id` from your data flow. The value of `data_flow_run_id` is the `metadata.asset_id` from your data flow run. An example response from this API call could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"entity\": {\n",
      "        \"configuration\": {},\n",
      "        \"data_flow_ref\": \"4dd043be-2bdb-4312-9d75-ddd2b045db55\",\n",
      "        \"engine_state\": {\n",
      "            \"engine_run_id\": \"f9790715-a987-40d2-a009-286990d928bd\",\n",
      "            \"session_cookie\": \"route=Spark; HttpOnly; Secure\"\n",
      "        },\n",
      "        \"name\": \"my_dataflow_0a67cdf4-0047-4ab0-8352-30ea2009af1a\",\n",
      "        \"rov\": {\n",
      "            \"members\": [],\n",
      "            \"mode\": 0\n",
      "        },\n",
      "        \"state\": \"finished\",\n",
      "        \"summary\": {\n",
      "            \"completed_date\": \"2018-01-31T12:10:40.259Z\",\n",
      "            \"engine_completed_date\": \"2018-01-31T12:10:39.621Z\",\n",
      "            \"engine_elapsed_secs\": 23,\n",
      "            \"engine_started_date\": \"2018-01-31T12:10:16.562Z\",\n",
      "            \"engine_status_date\": \"2018-01-31T12:10:39.622Z\",\n",
      "            \"engine_submitted_date\": \"2018-01-31T12:10:08.489Z\",\n",
      "            \"total_bytes_read\": 4351424,\n",
      "            \"total_bytes_written\": 3850572,\n",
      "            \"total_rows_read\": 13733,\n",
      "            \"total_rows_written\": 12186\n",
      "        },\n",
      "        \"tags\": []\n",
      "    },\n",
      "    \"metadata\": {\n",
      "        \"asset_id\": \"9f1fb8ca-5bd3-4bc3-8f23-00d53356883e\",\n",
      "        \"asset_type\": \"data_flow_run\",\n",
      "        \"create_time\": \"2018-01-31T12:10:00.000Z\",\n",
      "        \"creator\": \"******\",\n",
      "        \"href\": \"https://api.dataplatform.ibm.com/v2/data_flows/4dd043be-2bdb-4312-9d75-ddd2b045db55/runs/9f1fb8ca-5bd3-4bc3-8f23-00d53356883e?project_id=d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "        \"project_id\": \"d12f9685-2693-4c84-af48-3eb6c71e3013\",\n",
      "        \"usage\": {\n",
      "            \"access_count\": 0,\n",
      "            \"last_access_time\": \"2018-01-31T12:10:41.948Z\",\n",
      "            \"last_accessor\": \"serviceid-df61270e-5da1-4d99-9a0f-4e69e5965908\",\n",
      "            \"last_modification_time\": \"2018-01-31T12:10:41.948Z\",\n",
      "            \"last_modifier\": \"serviceid-df61270e-5da1-4d99-9a0f-4e69e5965908\"\n",
      "        }\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataflow_run_summary = requests.get(service_URL + \"/v2/data_flows/\" + data_flow_id + \"/runs/\" + data_flow_run_id + \"?project_id=\" + project_id, headers={'Authorization': authorization})\n",
    "pretty_print(dataflow_run_summary.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <a id=\"run5\"></a>3.5 Troubleshooting a failed run ####\n",
    "If a data flow run fails, the `state` attribute is set to the value `error`. In addition to this, the run asset itself has an attribute called `error` which is set to a concise description of the error (where available from the engine). If this information is not available from the engine, a more general message is set in the `error` attribute. This means that the `error` attribute is never left unset if a run fails. The following example shows the `error` payload produced if a schema specified in a source connection's properties doesn't exist:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"error\": {\n",
    "        \"trace\": \"1c09deb8-c3f9-4dc1-ad5a-0fc4e7c97071\",\n",
    "        \"errors\": [\n",
    "            {\n",
    "                \"code\": \"runtime_failed\",\n",
    "                \"message\": \"While the process was running a fatal error occurred in the engine (see logs for more details): SCAPI: CDICO2005E: Table could not be found: \\\"BADSCHEMAGOSALESHR.EMPLOYEE\\\" is an undefined name.. SQLCODE=-204, SQLSTATE=42704, DRIVER=4.20.4\\ncom.ibm.connect.api.SCAPIException: CDICO2005E: Table could not be found: \\\"BADSCHEMAGOSALESHR.EMPLOYEE\\\" is an undefined name.. SQLCODE=-204, SQLSTATE=42704, DRIVER=4.20.4\\n\\tat com.ibm.connect.jdbc.JdbcInputInteraction.init(JdbcInputInteraction.java:158)\\n\\t...\",\n",
    "                \"extra\": {\n",
    "                    \"account\": \"2d0d29d5b8d2701036042ca4cab8b613\",\n",
    "                    \"diagnostics\": \"[PROJECT_ID-ff1ab70b-0553-409a-93f9-ccc31471c218] [DATA_FLOW_ID-cfdacdb4-3180-466f-8d4c-be7badea5d64] [DATA_FLOW_NAME-my_dataflow] [DATA_FLOW_RUN_ID-ed09488c-6d51-48c4-b190-7096f25645d5]\",\n",
    "                    \"environment_name\": \"ypprod\",\n",
    "                    \"http_status\": 400,\n",
    "                    \"id\": \"CDIWA0129E\",\n",
    "                    \"source_cluster\": \"NULL\",\n",
    "                    \"service_version\": \"1.0.471\",\n",
    "                    \"source_component\": \"WDP-DataFlows\",\n",
    "                    \"timestamp\": \"2017-12-19T19:52:09.438Z\",\n",
    "                    \"transaction_id\": \"71c7d19b-a91b-40b1-9a14-4535d76e9e16\",\n",
    "                    \"user\": \"******\"\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the logs produced by the engine, use the following API:\n",
    "\n",
    "```\n",
    "GET https://{service_URL}/v2/data_flows/{data_flow_id}/runs/{data_flow_run_id}/logs?project_id={project_id}\n",
    "```\n",
    "\n",
    "An example response from this API call could be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"first\": {\n",
      "        \"href\": \"https://api.dataplatform.ibm.com/v2/data_flows/4dd043be-2bdb-4312-9d75-ddd2b045db55/runs/9f1fb8ca-5bd3-4bc3-8f23-00d53356883e/logs?project_id=d12f9685-2693-4c84-af48-3eb6c71e3013&offset=0&limit=100&raw_logs=false\"\n",
      "    },\n",
      "    \"last\": {\n",
      "        \"href\": \"https://api.dataplatform.ibm.com/v2/data_flows/4dd043be-2bdb-4312-9d75-ddd2b045db55/runs/9f1fb8ca-5bd3-4bc3-8f23-00d53356883e/logs?project_id=d12f9685-2693-4c84-af48-3eb6c71e3013&offset=0&limit=100&raw_logs=false\"\n",
      "    },\n",
      "    \"limit\": 100,\n",
      "    \"logs\": [\n",
      "        {\n",
      "            \"date\": \"2018-01-31T12:10:15.000Z\",\n",
      "            \"event_id\": \"0\",\n",
      "            \"message_text\": \"Job requested for activity '4dd043be-2bdb-4312-9d75-ddd2b045db55' with run id 'f9790715-a987-40d2-a009-286990d928bd' by user '*****'\",\n",
      "            \"type\": \"info\"\n",
      "        },\n",
      "        {\n",
      "            \"date\": \"2018-01-31T12:10:15.000Z\",\n",
      "            \"event_id\": \"1\",\n",
      "            \"message_text\": \"Job submitted to cluster 'spark'\",\n",
      "            \"type\": \"info\"\n",
      "        },\n",
      "        {\n",
      "            \"date\": \"2018-01-31T12:10:15.000Z\",\n",
      "            \"event_id\": \"2\",\n",
      "            \"message_text\": \"Job execution started\",\n",
      "            \"type\": \"info\"\n",
      "        },\n",
      "        {\n",
      "            \"date\": \"2018-01-31T12:10:16.000Z\",\n",
      "            \"event_id\": \"4\",\n",
      "            \"message_text\": \"SparkContext: Running Spark version 2.1.0\",\n",
      "            \"type\": \"info\"\n",
      "        },\n",
      "        {\n",
      "            \"date\": \"2018-01-31T12:10:16.000Z\",\n",
      "            \"event_id\": \"5\",\n",
      "            \"message_text\": \"StatusManager: Job executing on node 'wdp-dr-dal09-env5-ypprod-bm03.bluemix.net'\",\n",
      "            \"type\": \"info\"\n",
      "        },\n",
      "        {\n",
      "            \"date\": \"2018-01-31T12:10:39.000Z\",\n",
      "            \"event_id\": \"14\",\n",
      "            \"message_text\": \"Job execution ended\",\n",
      "            \"type\": \"info\"\n",
      "        }\n",
      "    ],\n",
      "    \"offset\": 0,\n",
      "    \"total_count\": 6\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "dataflow_run_logs = requests.get(service_URL + \"/v2/data_flows/\" + data_flow_id + \"/runs/\" + data_flow_run_id + \"/logs?project_id=\" + project_id, headers={'Authorization': authorization})\n",
    "pretty_print(dataflow_run_logs.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"resources\"></a>4. Resources ##\n",
    "For further information, see <a href=\"http://ibm.biz/wdp-api\" target=\"_blank\" rel=\"noopener noreferrer\">IBM Watson Core Services</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n",
    "**Damian Cummins** is a Cloud Application Developer with the Data Refinery and IBM Watson teams at IBM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
