{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "<h1>Spatial Queries in PySpark</h1>"}, {"metadata": {}, "cell_type": "markdown", "source": "\nThis notebook shows you show to use use spatial queries in Spark environments. The notebook uses the spatio-temporal library that is pre-installed on all Spark environments in Watson Studio. You will learn how to perform common spatial queries in Spark. \n\nThe types of spatial queries you will learn to use are:\n- In a set of points, find all the points that are within a certain distance to a particular point. For example, find all the hospitals that are within a certain distance to a given location.\n- In a set of polygons, find all the polygons that contain a particular point. For example, find all the risk areas for fires, floods, or hurricanes that contain a particular location.\n- In a set of points, find all the points that are contained within a particular polygon. For example, find all the retail outlets in a particular region.\n\nOften, a spatial function has one parameter that refers to a spatial column in one table and a second parameter that refers to a spatial constant or to a spatial column in another table. This notebook shows you how to use functions to access and combine data of different types to perform spatial queries.\n\nThis notebook runs on Python and Spark.\n\n\n## Table of Contents\n\n\n1.\t[Register the Spark SQL spatial functions](#register)\n2.\t[Get sample data](#getData)\n3.\t[Create a geometry column](#createColumn)\n4.\t[Register the data frames](#registerDataframe)\n5.  [Run spatial queries](#runQueries)  \n6.\t[Summary](#summary)\n\n\n"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"register\"></a>\n## 1. Register the Spark SQL spatial functions\n\nRegister the Spark SQL spatial functions:"}, {"metadata": {}, "cell_type": "code", "source": "spark._jvm.org.apache.spark.sql.types.SqlGeometry.registerAll(spark._jsparkSession)", "execution_count": 1, "outputs": [{"output_type": "stream", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20191106224935-0001\nKERNEL_ID = 5db42561-439c-461b-9e7e-be2d192c5454\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"getData\"></a>\n## 2. Get sample data\n\nThis notebook uses a sample data set that is available in the IBM Watson Studio Gallery. Direct links are used by default to make sure this notebook is publicly runnable.\n\nIn your own cases, you should use your preferred way of loading data into a Spark dataframe, depending on where your data source sits.\n\nHere are some hints if you are using IBM Cloud Object Storage:\n- If your data is uploaded directly into the current project, you can simply click `Find and Add Data` button in the top right corner on the menu bar, then click `Insert SparkSession DataFrame` on the data you want. A code will be generated automatically and returns a Spark data frame.\n- If your data is hosted in a designated bucket, you can use `ibmos2spark` to read the data into a Spark data frame.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Read the hospital data where each hospital's location is a latitude-longitude point:"}, {"metadata": {}, "cell_type": "code", "source": "import pandas as pd\nfrom pyspark.sql.types import *", "execution_count": 2, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "hospital_pdf = pd.read_csv('https://api.dataplatform.cloud.ibm.com/v2/gallery-assets/entries/5562ced564e776edc5f91e13d48d8309/data?accessKey=4d77701840fcb2f21587e39fdb063eeb')", "execution_count": 3, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "If you run into an error running the above code due to link not found, please download the `hospitals.csv` data set in the Watson Studio gallery and insert it manually using the method given above."}, {"metadata": {}, "cell_type": "code", "source": "hospital_schema = StructType([StructField('id', IntegerType()),\n                              StructField('name', StringType()),\n                              StructField('city', StringType()),\n                              StructField('state', StringType()),\n                              StructField('lon', DoubleType()),\n                              StructField('lat', DoubleType())])", "execution_count": 4, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "hospital_df = spark.createDataFrame(hospital_pdf, hospital_schema)", "execution_count": 5, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "hospital_df.show(3)", "execution_count": 6, "outputs": [{"output_type": "stream", "text": "+---+--------------------+------------+-----+----------+------------------+\n| id|                name|        city|state|       lon|               lat|\n+---+--------------------+------------+-----+----------+------------------+\n|  1|Southern Hills Me...|  BERRY HILL|   TN|-86.721939|         36.077843|\n|  2|Sycamore Shoals H...|ELIZABETHTON|   TN|-82.247635|         36.346218|\n|  3|     Tokona Hospital| GREENEVILLE|   TN|-82.845711|36.151771999999994|\n+---+--------------------+------------+-----+----------+------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Read the county data where each county is a polygon/multipolygon:"}, {"metadata": {}, "cell_type": "code", "source": "counties_pdf = pd.read_csv('https://api.dataplatform.cloud.ibm.com/v2/gallery-assets/entries/c8cc28f4c30dc4d8c0b13f18c50c3244/data?accessKey=c8cc28f4c30dc4d8c0b13f18c50fa2d5'\n                          )[['NAME', 'STATE_NAME', 'POP2000', 'shape_WKT']]", "execution_count": 7, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "counties_schema = StructType([StructField('NAME', StringType()),\n                              StructField('STATE_NAME', StringType()),\n                              StructField('POP2000', IntegerType()),\n                              StructField('shape_WKT', StringType())])", "execution_count": 8, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "counties_df = spark.createDataFrame(counties_pdf, counties_schema)", "execution_count": 9, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "counties_df.show(3)", "execution_count": 10, "outputs": [{"output_type": "stream", "text": "+-----------------+----------+-------+--------------------+\n|             NAME|STATE_NAME|POP2000|           shape_WKT|\n+-----------------+----------+-------+--------------------+\n|Lake of the Woods| Minnesota|   4522|MULTIPOLYGON (((-...|\n|            Ferry|Washington|   7260|MULTIPOLYGON (((-...|\n|          Stevens|Washington|  40066|MULTIPOLYGON (((-...|\n+-----------------+----------+-------+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"createColumn\"></a>\n## 3. Create a geometry column for hospital and county data"}, {"metadata": {}, "cell_type": "markdown", "source": "The raw spatial data in the data frame can be of various types, for example **columns indicating latitude and longitude** or **column indicating wkt string for the geometry**, and so on.\n\nTherefore, the first step is to use a spatial query to generate a new spatial column that combines the data in these columns.  \nFor example, use the function:\n- `ST_Point(lon_col, lat_col)` if the raw spatial data is in a latitude column and a longitude column  \n- `ST_WKTTOSQL(wkt_col)` if the raw spatial data is in a column containing the wkt string form of the geometry  \n\nFor the full list of possible query functions, see [Geospatial Toolkit functions](https://www.ibm.com/support/knowledgecenter/en/SSCJDQ/com.ibm.swg.im.dashdb.analytics.doc/doc/geo_functions.html)."}, {"metadata": {}, "cell_type": "markdown", "source": "Create a geometry column for the hospital data using `ST_Point(lon, lat)`:"}, {"metadata": {}, "cell_type": "code", "source": "hospital_df.createOrReplaceTempView(\"hospitals\")\nhospital_df = spark.sql(\"SELECT *, ST_Point(lon, lat) as location from hospitals\")\nhospital_df.show(3, False)", "execution_count": 11, "outputs": [{"output_type": "stream", "text": "+---+-----------------------------+------------+-----+----------+------------------+------------------------------------------------+\n|id |name                         |city        |state|lon       |lat               |location                                        |\n+---+-----------------------------+------------+-----+----------+------------------+------------------------------------------------+\n|1  |Southern Hills Medical Center|BERRY HILL  |TN   |-86.721939|36.077843         |PointEG: lat=36.077843, long=-86.721939         |\n|2  |Sycamore Shoals Hospital     |ELIZABETHTON|TN   |-82.247635|36.346218         |PointEG: lat=36.346218, long=-82.247635         |\n|3  |Tokona Hospital              |GREENEVILLE |TN   |-82.845711|36.151771999999994|PointEG: lat=36.151771999999994, long=-82.845711|\n+---+-----------------------------+------------+-----+----------+------------------+------------------------------------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Create a geometry column for the county data using `ST_WKTToSQL(wkt_string)`:"}, {"metadata": {}, "cell_type": "code", "source": "counties_df.createOrReplaceTempView('counties')\ncounties_df = spark.sql(\"SELECT NAME, STATE_NAME, POP2000, ST_WKTToSQL(shape_WKT) as shape from counties\")\ncounties_df.show(3)", "execution_count": 12, "outputs": [{"output_type": "stream", "text": "+-----------------+----------+-------+--------------------+\n|             NAME|STATE_NAME|POP2000|               shape|\n+-----------------+----------+-------+--------------------+\n|Lake of the Woods| Minnesota|   4522|AcceleratedMultiP...|\n|            Ferry|Washington|   7260|AcceleratedMultiP...|\n|          Stevens|Washington|  40066|AcceleratedMultiP...|\n+-----------------+----------+-------+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"registerDataframe\"></a>\n## 4. Register the hospital and county data frames as a temporary view\n\nA data frame can also be used to create a temporary view. Registering a data frame as a table allows you to run SQL queries over its data. Register the hospital and county data frames as a temporary view: "}, {"metadata": {}, "cell_type": "code", "source": "hospital_df.createOrReplaceTempView('hospitals')\ncounties_df.createOrReplaceTempView('counties')", "execution_count": 13, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"runQueries\"></a>\n## 5. Run spatial queries\n\n1. [Example 1: Query to determine points closest to another point](#ex1)\n1. [Example 2: Queries to determine which polygon contains a point](#ex2)\n1. [Example 3: Queries to determine the points in a polygon](#ex3)\n1. [Example 4: Spatial join queries to determine points in a polygon](#ex4)\n1. [Example 5: Spatial join queries with additional predicates and aggregation](#ex5)\n1. [Example 6: Window queries](#ex6)\n1. [Example 7: Distance queries](#ex7)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id = \"ex1\"></a>\n### Example 1: Query to determine points closest to another point\n\nThis sample query shows you how to find the hospitals that are within a certain distance of a given location (which is constructed using the `ST_Point` constructor)."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name, city, state\nFROM hospitals\nWHERE ST_Distance(location, ST_Point(-77.574722, 43.146732)) < 10000.0\n\"\"\").show()", "execution_count": 14, "outputs": [{"output_type": "stream", "text": "+--------------------+-----------+-----+\n|                name|       city|state|\n+--------------------+-----------+-----+\n|Park Avenue Hospital|   BRIGHTON|   NY|\n|County Home and I...|   BRIGHTON|   NY|\n|    Genesee Hospital|   BRIGHTON|   NY|\n|   Highland Hospital|   BRIGHTON|   NY|\n|Rochester General...|IRONDEQUOIT|   NY|\n|Saint Marys Hospital|   BRIGHTON|   NY|\n|Strong Memorial H...|   BRIGHTON|   NY|\n+--------------------+-----------+-----+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id = \"ex2\"></a>\n### Example 2: Queries to determine which polygon contains a point\n\nThe following sample queries show you how to use spatial functions to determine which polygon contains a given point. The examples use the following functions:\n\n1. `ST_Contains(geom1, geom2)`: returns TRUE if the `geom2` values are completely contained by the polygons identified by `geom1`.\n2. `ST_Within(geom1, geom2)`: returns TRUE if the `geom1` values are within the polygons identified by `geom2`.\n3. `ST_Intersects(geom1, geom2)`: returns TRUE if `geom1` and `geom2` intersect spatially in any way. This can be that they  touch, cross, or contain one other."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT NAME \nFROM counties \nWHERE ST_Contains(shape, ST_Point(-74.237, 42.037))\n\"\"\").show()", "execution_count": 15, "outputs": [{"output_type": "stream", "text": "+------+\n|  NAME|\n+------+\n|Ulster|\n+------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT NAME\nFROM counties\nWHERE\nST_Within(ST_Point(-74.237, 42.037), shape)\n\"\"\").show()", "execution_count": 16, "outputs": [{"output_type": "stream", "text": "+------+\n|  NAME|\n+------+\n|Ulster|\n+------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT NAME\nFROM counties\nWHERE\nST_Intersects(shape, ST_Point(-74.237, 42.037))\n\"\"\").show()", "execution_count": 17, "outputs": [{"output_type": "stream", "text": "+------+\n|  NAME|\n+------+\n|Ulster|\n+------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id = \"ex3\"></a>\n### Example 3: Queries to determine the points in a polygon\n\nEach of the following queries determine which hospitals are located within the specified polygon, which is defined as a constant using the  well-known text (WKT) representation. The polygon definition consists of the character string POLYGON followed by a pair of $x$ and $y$ coordinates for each vertex, separated by a comma. The individual $x$ and $y$ values are separated by a space. The entire list of coordinate pairs must be in parentheses."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name\nFROM hospitals\nWHERE\nST_Contains(ST_WKTToSQL('POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'), location)\n\"\"\").show(3)", "execution_count": 18, "outputs": [{"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|   Marshall Hospital|\n|Southwestern Medi...|\n|  Hillcrest Hospital|\n+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name \nFROM hospitals \nWHERE ST_Within(location, ST_WKTToSQL('POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n\"\"\").show(3)", "execution_count": 19, "outputs": [{"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|   Marshall Hospital|\n|Southwestern Medi...|\n|  Hillcrest Hospital|\n+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name \nFROM hospitals \nWHERE ST_Intersects(location, ST_WKTToSQL('POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n\"\"\").show(3)", "execution_count": 20, "outputs": [{"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|   Marshall Hospital|\n|Southwestern Medi...|\n|  Hillcrest Hospital|\n+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id = \"ex4\"></a>\n### Example 4: Spatial join queries to determine points in a polygon\n\nJust as a regular join function can join two tables based on the values in columns that contain character or numeric data, spatial join functions can be used to join tables based on the values in the columns that contain spatial data. The following examples use the **counties** and **hospitals** tables.\n\nYou can use the spatial join function to find the hospitals located within a specific county. For example, the following query returns a list of all the hospitals in the Dutchess county:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT c.NAME, h.name \nFROM counties AS c, hospitals AS h \nWHERE c.NAME = 'Dutchess' \nAND ST_Intersects(c.shape, h.location)\n\"\"\").show()", "execution_count": 21, "outputs": [{"output_type": "stream", "text": "+--------+--------------------+\n|    NAME|                name|\n+--------+--------------------+\n|Dutchess|Hudson River Stat...|\n|Dutchess|Vassar Brothers H...|\n|Dutchess|      Bowne Hospital|\n|Dutchess|Harlem Valley Sta...|\n|Dutchess|Matteawan State H...|\n|Dutchess|New York State Ho...|\n|Dutchess|Saint Francis Hos...|\n|Dutchess|United States Vet...|\n+--------+--------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Alternatively, you can use the SQL `JOIN ... ON ...` notation, which is equivalent to a spatial predicate in the `WHERE` clause. For example, the following query produces the same result set as the previous query:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT h.name, c.NAME\nFROM counties AS c\nJOIN hospitals AS h\nON c.NAME = 'Dutchess'\nAND ST_Intersects(h.location, c.shape)\n\"\"\").show()", "execution_count": 22, "outputs": [{"output_type": "stream", "text": "+--------------------+--------+\n|                name|    NAME|\n+--------------------+--------+\n|Hudson River Stat...|Dutchess|\n|Vassar Brothers H...|Dutchess|\n|      Bowne Hospital|Dutchess|\n|Harlem Valley Sta...|Dutchess|\n|Matteawan State H...|Dutchess|\n|New York State Ho...|Dutchess|\n|Saint Francis Hos...|Dutchess|\n|United States Vet...|Dutchess|\n+--------------------+--------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The following query returns the name of the county in which a particular hospital is located:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT c.NAME, h.name\nFROM hospitals AS h, counties AS c\nWHERE ST_Intersects(h.location, c.shape)\nAND h.name = 'Vassar Brothers Hospital'\n\"\"\").show()", "execution_count": 23, "outputs": [{"output_type": "stream", "text": "+--------+--------------------+\n|    NAME|                name|\n+--------+--------------------+\n|Dutchess|Vassar Brothers H...|\n+--------+--------------------+\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id = \"ex5\"></a>\n### Example 5: Spatial join queries with additional predicates and aggregation\n\nThis example shows you how to use spatial joins in conjunction with additional predicates and aggregation, which can address business problems. These examples continue to use the hospitals and counties tables, but the same principles could be applied to any other type of data.\n\nThe following example queries the hospitals within each county in New York state, qualifying by the state name in the counties table."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT c.NAME, h.name\nFROM counties AS c, hospitals AS h\nWHERE ST_Intersects(h.location, c.shape)\nAND c.STATE_NAME='New York'\nORDER BY c.NAME, h.name\n\"\"\").show(3)", "execution_count": 24, "outputs": [{"output_type": "stream", "text": "+------+--------------------+\n|  NAME|                name|\n+------+--------------------+\n|Albany|     Albany Hospital|\n|Albany|Albany Hospital F...|\n|Albany|Albany Hospital S...|\n+------+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The same results can be obtained by rewriting the above query and using the fields from the hospitals table:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT c.NAME, h.name\nFROM hospitals AS h, counties AS c\nWHERE ST_Intersects(h.location, c.shape)\nAND h.state='NY'\nORDER BY c.NAME, h.name\n\"\"\").show(3)", "execution_count": 25, "outputs": [{"output_type": "stream", "text": "+------+--------------------+\n|  NAME|                name|\n+------+--------------------+\n|Albany|     Albany Hospital|\n|Albany|Albany Hospital F...|\n|Albany|Albany Hospital S...|\n+------+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The following example lists the number of hospitals per county in New York:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT c.NAME, COUNT(h.name) AS hospital_count\nFROM counties AS c, hospitals AS h\nWHERE ST_Intersects(h.location, c.shape)\nAND c.STATE_NAME='New York'\nGROUP BY c.NAME\n\"\"\").show(3)", "execution_count": 26, "outputs": [{"output_type": "stream", "text": "+------+--------------+\n|  NAME|hospital_count|\n+------+--------------+\n|Cayuga|             1|\n| Kings|            26|\n|Monroe|             9|\n+------+--------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "To identify counties where the population is underserved by hospitals, an interesting metric might be the number of people per hospital in each county. Using the population of each county in the year 2000, you can calculate this number."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT c.NAME, \nCOUNT(h.name) AS hospital_count, \nc.POP2000 AS Population, \nc.POP2000/COUNT(h.name) AS people_per_hospital\nFROM counties AS c, hospitals AS h\nWHERE c.STATE_NAME='New York'\nAND ST_Intersects(h.location, c.shape)\nGROUP BY c.NAME, c.POP2000\nORDER BY people_per_hospital DESC\n\"\"\").show(3)", "execution_count": 27, "outputs": [{"output_type": "stream", "text": "+----------+--------------+----------+-------------------+\n|      NAME|hospital_count|Population|people_per_hospital|\n+----------+--------------+----------+-------------------+\n|     Bronx|             9|   1332650| 148072.22222222222|\n|Chautauqua|             1|    139750|           139750.0|\n|    Oswego|             1|    122377|           122377.0|\n+----------+--------------+----------+-------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "With additional detail, such as number of beds, number of doctors per hospital, you could determine a better measure for health care coverage per state and population."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id = \"ex6\"></a>\n### Example 6: Window queries\n\nA common use case for mapping applications, and in particular for web mapping, is to select objects that fall within a specific rectangular region. This can be done by creating a polygon to represent the rectangle and using the `ST_Intersects` spatial predicate."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name\nFROM hospitals\nWHERE ST_Intersects(location, ST_WKTToSQL(\n 'POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n\"\"\").show(3)", "execution_count": 28, "outputs": [{"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|   Marshall Hospital|\n|Southwestern Medi...|\n|  Hillcrest Hospital|\n+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Another spatial predicate that does the same is `EnvelopesIntersect`, which can be used to select objects whose envelope intersects a rectangular region called a window. `EnvelopesIntersect` takes the name of the spatial column as a parameter and four Double values representing the lower-left, and upper-right corners of the rectangle. This spatial predicate is simpler to use and is more efficient than `ST_Intersects` for rectangular windows."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name\nFROM hospitals\nWHERE EnvelopesIntersect(location, -74.0, 42.0, -73.0, 43.0)\n\"\"\").show(3)", "execution_count": 29, "outputs": [{"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|   Marshall Hospital|\n|Southwestern Medi...|\n|  Hillcrest Hospital|\n+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "Because this predicate is true if any portion of a line or polygon geometry falls within the specified window, parts of the line or polygon might lie outside of the window. This is generally not a problem with mapping applications, which will discard geometries that lie outside the display window.\n\nWhen building web-mapping applications with widely used web-mapping APIs from providers such as Google Maps, Yahoo! Maps, Bing Maps, and others, it is necessary to provide the longitude and latitude values to be used in placing custom markers on the map. You can get this information by using a query such as the following:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name, ST_X(location) AS longitude, ST_Y(location) AS latitude\nFROM hospitals\nWHERE EnvelopesIntersect(location, -74.0, 42.0, -73.0, 43.0)\n\"\"\").show(3)", "execution_count": 30, "outputs": [{"output_type": "stream", "text": "+--------------------+----------+---------+\n|                name| longitude| latitude|\n+--------------------+----------+---------+\n|   Marshall Hospital|-73.678177|42.718971|\n|Southwestern Medi...|-73.206772|42.874249|\n|  Hillcrest Hospital|-73.280113|42.457863|\n+--------------------+----------+---------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id = \"ex7\"></a>\n### Example 7: Distance queries\n\nAnother common spatial query is to find things within a specified distance of a particular location. You have probably used web-mapping applications to get this kind of information. You can issue SQL queries from your application for questions like:\n\n- Find customers within 10 miles of a store\n- Find ATMs within 500 meters of the current location\n- Find competitive stores within 10 kilometers of a proposed store location"}, {"metadata": {}, "cell_type": "markdown", "source": "The spatial function used for these queries is `ST_Distance`, which computes the distance between the spatial values and returns a result in meters. \n\nThe following query generates eight results:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name\nFROM hospitals\nWHERE ST_Intersects(location, ST_WKTToSQL(\n 'POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n\"\"\").show(3)", "execution_count": 31, "outputs": [{"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|   Marshall Hospital|\n|Southwestern Medi...|\n|  Hillcrest Hospital|\n+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "A different way of querying the same location above is to use the `ST_Buffer` function, where a circular buffer is created around the given geometry and the desired geometries within that buffer are determined. The `ST_Buffer` function takes as parameters a spatial geometry and a distance in meters to the buffer around this spatial value. The results are the same as when you us `ST_Intersects`."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name\nFROM hospitals\nWHERE\nST_Intersects(location,\n  ST_Buffer(ST_Point(-74.237, 42.037), 46800.0))\nORDER BY name\n\"\"\").show(3)", "execution_count": 32, "outputs": [{"output_type": "stream", "text": "+--------------------+\n|                name|\n+--------------------+\n|      Adventist Home|\n|      Bowne Hospital|\n|Columbia Memorial...|\n+--------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "The following query returns the distance from a specified point to each object within a 30 mile (or approximately 46800m) radius:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name, ST_Distance(location, ST_Point(-74.237, 42.037)) AS distance\nFROM hospitals\nWHERE ST_Distance(location, ST_Point(-74.237, 42.037)) < 46800.0\nORDER BY distance\n\"\"\").show(3)", "execution_count": 33, "outputs": [{"output_type": "stream", "text": "+--------------------+------------------+\n|                name|          distance|\n+--------------------+------------------+\n|Greene County Mem...| 36634.88406671428|\n|      Adventist Home|39014.090792071736|\n|Hudson River Stat...| 43362.54508885234|\n+--------------------+------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "You could also use `ST_Buffer` to compute the spatial relation and then determine the distance as is shown in the following query:"}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name, ST_Distance(location, ST_Point(-74.237, 42.037)) AS distance\nFROM hospitals\nWHERE\n  ST_Intersects(location,\n  ST_Buffer(ST_Point(-74.237, 42.037), 46800.0))\nORDER BY distance\n\"\"\").show(3)", "execution_count": 34, "outputs": [{"output_type": "stream", "text": "+--------------------+------------------+\n|                name|          distance|\n+--------------------+------------------+\n|Greene County Mem...| 36634.88406671428|\n|      Adventist Home|39014.090792071736|\n|Hudson River Stat...| 43362.54508885234|\n+--------------------+------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "A key difference to be noted here is that the `ST_Buffer` in this package supports buffering of arbitrary geometries and can be used to compute in that manner. Note that:\n- The `ST_Buffer` query on large geometries can be expensive.\n- For a large number of geometries, the user is advised to calculate the buffers separately, store the buffers in columns, and operate on the stored buffers."}, {"metadata": {}, "cell_type": "code", "source": "spark.sql(\"\"\"\nSELECT name, ST_Distance(location, ST_WKTToSQL(\n 'LINESTRING (-74.0 42.0, -73.0 42.0)'))\nFROM hospitals\nWHERE ST_Intersects(location, ST_Buffer(ST_WKTToSQL(\n 'LINESTRING (-74.0 42.0, -73.0 42.0)'), 46800.0))\n\"\"\").show(3)", "execution_count": 35, "outputs": [{"output_type": "stream", "text": "+--------------------+-------------------------------------------------------------------------------+\n|                name|UDF:ST_Distance(location, UDF:ST_WKTToSQL(LINESTRING (-74.0 42.0, -73.0 42.0)))|\n+--------------------+-------------------------------------------------------------------------------+\n|      Avery Hospital|                                                              38777.96495714722|\n|   Hartford Hospital|                                                               38204.0148377887|\n|Springfield Munic...|                                                              39761.18673983218|\n+--------------------+-------------------------------------------------------------------------------+\nonly showing top 3 rows\n\n", "name": "stdout"}]}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n##  Summary\n\nIn this notebook, you learned how to query spatial data you downloaded from the IBM Watson Studio Gallery. You registered each data frame (one with data on hospitals and another with county information) as a table to run your queries on. The sample queries showed you how to determine the hospitals within a certain distance or in a polygon, to find the name of the county in which a hospital is located, or to identify the counties where the population is underserved by hospitals. The sample queries showed you how to use and combine the most common Spark SQL spatial functions in queries. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Author\n\n**Linsong Chu**, Research Engineer at IBM Research"}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2019 IBM. This notebook and its source code are released under the terms of the MIT License."}, {"metadata": {}, "cell_type": "markdown", "source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>"}], "metadata": {"kernelspec": {"name": "python36", "display_name": "Python 3.6 with Spark", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "name": "python", "pygments_lexer": "ipython3", "version": "3.6.8", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4, "nbformat_minor": 1}
