{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Spatial Queries in PySpark</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook shows you show to use use spatial queries in Spark environments. The notebook uses the spatio-temporal library that is pre-installed on all Spark environments in Watson Studio. You will learn how to perform common spatial queries in Spark. \n",
    "\n",
    "The types of spatial queries you will learn to use are:\n",
    "- In a set of points, find all the points that are within a certain distance to a particular point. For example, find all the hospitals that are within a certain distance to a given location.\n",
    "- In a set of polygons, find all the polygons that contain a particular point. For example, find all the risk areas for fires, floods, or hurricanes that contain a particular location.\n",
    "- In a set of points, find all the points that are contained within a particular polygon. For example, find all the retail outlets in a particular region.\n",
    "\n",
    "Often, a spatial function has one parameter that refers to a spatial column in one table and a second parameter that refers to a spatial constant or to a spatial column in another table. This notebook shows you how to use functions to access and combine data of different types to perform spatial queries.\n",
    "\n",
    "This notebook runs on Python and Spark.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "\n",
    "1. [Register the Spark SQL spatial functions](#register)\n",
    "2. [Get sample data](#getData)\n",
    "3. [Create a geometry column](#createColumn)\n",
    "4. [Register the data frames](#registerDataframe)\n",
    "5. [Run spatial queries](#runQueries)  \n",
    "6. [Summary](#summary)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"register\"></a>\n",
    "## 1. Register the Spark SQL spatial functions\n",
    "\n",
    "Register the Spark SQL spatial functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark._jvm.org.apache.spark.sql.types.SqlGeometry.registerAll(spark._jsparkSession)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"getData\"></a>\n",
    "## 2. Get sample data\n",
    "\n",
    "This notebook uses a sample data set that is available in the IBM Watson Studio Gallery. Direct links are used by default to make sure this notebook is publicly runnable.\n",
    "\n",
    "In your own cases, you should use your preferred way of loading data into a Spark dataframe, depending on where your data source sits.\n",
    "\n",
    "Here are some hints if you are using IBM Cloud Object Storage:\n",
    "- If your data is uploaded directly into the current project, you can simply click `Code snippets` button in the menu bar, find your dataset, and then, under Insert as, select `SparkSession DataFrame`. Code that adds a Spark data frame will be generated automatically.\n",
    "- If your data is hosted in a designated bucket, you can use `ibmos2spark` to read the data into a Spark data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the hospital data where each hospital's location is a latitude-longitude point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen\n",
    "    \n",
    "req = Request('https://api.dataplatform.cloud.ibm.com/v2/gallery-assets/entries/5562ced564e776edc5f91e13d48d8309/data?accessKey=466875ad0187d4ea757478e5c1130b59')\n",
    "req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0')\n",
    "content = urlopen(req)\n",
    "\n",
    "hospital_pdf = pd.read_csv(content)\n",
    "print(hospital_pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you run into an error running the above code due to link not found, please download the `hospitals.csv` data set in the Watson Studio gallery and insert it manually using the method given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_schema = StructType([StructField('id', IntegerType()),\n",
    "                              StructField('name', StringType()),\n",
    "                              StructField('city', StringType()),\n",
    "                              StructField('state', StringType()),\n",
    "                              StructField('lon', DoubleType()),\n",
    "                              StructField('lat', DoubleType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df = spark.createDataFrame(hospital_pdf, hospital_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the county data where each county is a polygon/multipolygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import Request, urlopen  # Python 3\n",
    "    \n",
    "req = Request('https://api.dataplatform.cloud.ibm.com/v2/gallery-assets/entries/c8cc28f4c30dc4d8c0b13f18c50c3244/data?accessKey=c8cc28f4c30dc4d8c0b13f18c50fa2d5')\n",
    "req.add_header('User-Agent', 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:77.0) Gecko/20100101 Firefox/77.0')\n",
    "content = urlopen(req)\n",
    "\n",
    "counties_pdf = pd.read_csv(content)[['NAME', 'STATE_NAME', 'POP2000', 'shape_WKT']]\n",
    "print(counties_pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_schema = StructType([StructField('NAME', StringType()),\n",
    "                              StructField('STATE_NAME', StringType()),\n",
    "                              StructField('POP2000', IntegerType()),\n",
    "                              StructField('shape_WKT', StringType())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_df = spark.createDataFrame(counties_pdf, counties_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"createColumn\"></a>\n",
    "## 3. Create a geometry column for hospital and county data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw spatial data in the data frame can be of various types, for example **columns indicating latitude and longitude** or **column indicating wkt string for the geometry**, and so on.\n",
    "\n",
    "Therefore, the first step is to use a spatial query to generate a new spatial column that combines the data in these columns.  \n",
    "For example, use the function:\n",
    "- `ST_Point(lon_col, lat_col)` if the raw spatial data is in a latitude column and a longitude column  \n",
    "- `ST_WKTTOSQL(wkt_col)` if the raw spatial data is in a column containing the wkt string form of the geometry  \n",
    "\n",
    "For the full list of possible query functions, see [Geospatial Toolkit functions](https://www.ibm.com/support/knowledgecenter/en/SSCJDQ/com.ibm.swg.im.dashdb.analytics.doc/doc/geo_functions.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a geometry column for the hospital data using `ST_Point(lon, lat)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df.createOrReplaceTempView(\"hospitals\")\n",
    "hospital_df = spark.sql(\"SELECT *, ST_Point(lon, lat) as location from hospitals\")\n",
    "hospital_df.show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a geometry column for the county data using `ST_WKTToSQL(wkt_string)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_df.createOrReplaceTempView('counties')\n",
    "counties_df = spark.sql(\"SELECT NAME, STATE_NAME, POP2000, ST_WKTToSQL(shape_WKT) as shape from counties\")\n",
    "counties_df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"registerDataframe\"></a>\n",
    "## 4. Register the hospital and county data frames as a temporary view\n",
    "\n",
    "A data frame can also be used to create a temporary view. Registering a data frame as a table allows you to run SQL queries over its data. Register the hospital and county data frames as a temporary view: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql.legacy.storeAnalyzedPlanForView = True\n",
    "\n",
    "#SparkSession.sql(\"set spark.sql.legacy.storeAnalyzedPlanForView = true\")\n",
    "\n",
    "\n",
    "spark.conf.set(\"spark.sql.legacy.storeAnalyzedPlanForView\",\"False\")\n",
    "\n",
    "print(spark.conf.get(\"spark.sql.legacy.storeAnalyzedPlanForView\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_df.createOrReplaceTempView('hospitals_temp')\n",
    "counties_df.createOrReplaceTempView('counties_temp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"runQueries\"></a>\n",
    "## 5. Run spatial queries\n",
    "\n",
    "1. [Example 1: Query to determine points closest to another point](#ex1)\n",
    "1. [Example 2: Queries to determine which polygon contains a point](#ex2)\n",
    "1. [Example 3: Queries to determine the points in a polygon](#ex3)\n",
    "1. [Example 4: Spatial join queries to determine points in a polygon](#ex4)\n",
    "1. [Example 5: Spatial join queries with additional predicates and aggregation](#ex5)\n",
    "1. [Example 6: Window queries](#ex6)\n",
    "1. [Example 7: Distance queries](#ex7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex1\"></a>\n",
    "### Example 1: Query to determine points closest to another point\n",
    "\n",
    "This sample query shows you how to find the hospitals that are within a certain distance of a given location (which is constructed using the `ST_Point` constructor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name, city, state\n",
    "FROM hospitals_temp\n",
    "WHERE ST_Distance(location, ST_Point(-77.574722, 43.146732)) < 10000.0\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex2\"></a>\n",
    "### Example 2: Queries to determine which polygon contains a point\n",
    "\n",
    "The following sample queries show you how to use spatial functions to determine which polygon contains a given point. The examples use the following functions:\n",
    "\n",
    "1. `ST_Contains(geom1, geom2)`: returns TRUE if the `geom2` values are completely contained by the polygons identified by `geom1`.\n",
    "2. `ST_Within(geom1, geom2)`: returns TRUE if the `geom1` values are within the polygons identified by `geom2`.\n",
    "3. `ST_Intersects(geom1, geom2)`: returns TRUE if `geom1` and `geom2` intersect spatially in any way. This can be that they  touch, cross, or contain one other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT NAME \n",
    "FROM counties_temp \n",
    "WHERE ST_Contains(shape, ST_Point(-74.237, 42.037))\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT NAME\n",
    "FROM counties_temp\n",
    "WHERE\n",
    "ST_Within(ST_Point(-74.237, 42.037), shape)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT NAME\n",
    "FROM counties_temp\n",
    "WHERE\n",
    "ST_Intersects(shape, ST_Point(-74.237, 42.037))\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex3\"></a>\n",
    "### Example 3: Queries to determine the points in a polygon\n",
    "\n",
    "Each of the following queries determine which hospitals are located within the specified polygon, which is defined as a constant using the  well-known text (WKT) representation. The polygon definition consists of the character string POLYGON followed by a pair of $x$ and $y$ coordinates for each vertex, separated by a comma. The individual $x$ and $y$ values are separated by a space. The entire list of coordinate pairs must be in parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name\n",
    "FROM hospitals_temp\n",
    "WHERE\n",
    "ST_Contains(ST_WKTToSQL('POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'), location)\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name \n",
    "FROM hospitals_temp\n",
    "WHERE ST_Within(location, ST_WKTToSQL('POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name \n",
    "FROM hospitals_temp\n",
    "WHERE ST_Intersects(location, ST_WKTToSQL('POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex4\"></a>\n",
    "### Example 4: Spatial join queries to determine points in a polygon\n",
    "\n",
    "Just as a regular join function can join two tables based on the values in columns that contain character or numeric data, spatial join functions can be used to join tables based on the values in the columns that contain spatial data. The following examples use the **counties** and **hospitals** tables.\n",
    "\n",
    "You can use the spatial join function to find the hospitals located within a specific county. For example, the following query returns a list of all the hospitals in the Dutchess county:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.NAME, h.name \n",
    "FROM counties_temp AS c, hospitals_temp AS h \n",
    "WHERE c.NAME = 'Dutchess' \n",
    "AND ST_Intersects(c.shape, h.location)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the SQL `JOIN ... ON ...` notation, which is equivalent to a spatial predicate in the `WHERE` clause. For example, the following query produces the same result set as the previous query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT h.name, c.NAME\n",
    "FROM counties_temp AS c\n",
    "JOIN hospitals_temp AS h\n",
    "ON c.NAME = 'Dutchess'\n",
    "AND ST_Intersects(h.location, c.shape)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query returns the name of the county in which a particular hospital is located:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.NAME, h.name\n",
    "FROM hospitals_temp AS h, counties_temp AS c\n",
    "WHERE ST_Intersects(h.location, c.shape)\n",
    "AND h.name = 'Vassar Brothers Hospital'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex5\"></a>\n",
    "### Example 5: Spatial join queries with additional predicates and aggregation\n",
    "\n",
    "This example shows you how to use spatial joins in conjunction with additional predicates and aggregation, which can address business problems. These examples continue to use the hospitals and counties tables, but the same principles could be applied to any other type of data.\n",
    "\n",
    "The following example queries the hospitals within each county in New York state, qualifying by the state name in the counties table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.NAME, h.name\n",
    "FROM counties_temp AS c, hospitals_temp AS h\n",
    "WHERE ST_Intersects(h.location, c.shape)\n",
    "AND c.STATE_NAME='New York'\n",
    "ORDER BY c.NAME, h.name\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same results can be obtained by rewriting the above query and using the fields from the hospitals table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.NAME, h.name\n",
    "FROM hospitals_temp AS h, counties_temp AS c\n",
    "WHERE ST_Intersects(h.location, c.shape)\n",
    "AND h.state='NY'\n",
    "ORDER BY c.NAME, h.name\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example lists the number of hospitals per county in New York:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.NAME, COUNT(h.name) AS hospital_count\n",
    "FROM counties_temp AS c, hospitals_temp AS h\n",
    "WHERE ST_Intersects(h.location, c.shape)\n",
    "AND c.STATE_NAME='New York'\n",
    "GROUP BY c.NAME\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify counties where the population is underserved by hospitals, an interesting metric might be the number of people per hospital in each county. Using the population of each county in the year 2000, you can calculate this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT c.NAME, \n",
    "COUNT(h.name) AS hospital_count, \n",
    "c.POP2000 AS Population, \n",
    "c.POP2000/COUNT(h.name) AS people_per_hospital\n",
    "FROM counties_temp AS c, hospitals_temp AS h\n",
    "WHERE c.STATE_NAME='New York'\n",
    "AND ST_Intersects(h.location, c.shape)\n",
    "GROUP BY c.NAME, c.POP2000\n",
    "ORDER BY people_per_hospital DESC\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With additional detail, such as number of beds, number of doctors per hospital, you could determine a better measure for health care coverage per state and population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex6\"></a>\n",
    "### Example 6: Window queries\n",
    "\n",
    "A common use case for mapping applications, and in particular for web mapping, is to select objects that fall within a specific rectangular region. This can be done by creating a polygon to represent the rectangle and using the `ST_Intersects` spatial predicate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name\n",
    "FROM hospitals_temp\n",
    "WHERE ST_Intersects(location, ST_WKTToSQL(\n",
    " 'POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"ex7\"></a>\n",
    "### Example 7: Distance queries\n",
    "\n",
    "Another common spatial query is to find things within a specified distance of a particular location. You have probably used web-mapping applications to get this kind of information. You can issue SQL queries from your application for questions like:\n",
    "\n",
    "- Find customers within 10 miles of a store\n",
    "- Find ATMs within 500 meters of the current location\n",
    "- Find competitive stores within 10 kilometers of a proposed store location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spatial function used for these queries is `ST_Distance`, which computes the distance between the spatial values and returns a result in meters. \n",
    "\n",
    "The following query generates eight results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name\n",
    "FROM hospitals_temp\n",
    "WHERE ST_Intersects(location, ST_WKTToSQL(\n",
    " 'POLYGON ((-74.0 42.0, -73.0 42.0, -73.0 43.0, -74.0 43.0, -74.0 42.0))'))\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A different way of querying the same location above is to use the `ST_Buffer` function, where a circular buffer is created around the given geometry and the desired geometries within that buffer are determined. The `ST_Buffer` function takes as parameters a spatial geometry and a distance in meters to the buffer around this spatial value. The results are the same as when you us `ST_Intersects`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name\n",
    "FROM hospitals_temp\n",
    "WHERE\n",
    "ST_Intersects(location,\n",
    "  ST_Buffer(ST_Point(-74.237, 42.037), 46800.0))\n",
    "ORDER BY name\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following query returns the distance from a specified point to each object within a 30 mile (or approximately 46800m) radius:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name, ST_Distance(location, ST_Point(-74.237, 42.037)) AS distance\n",
    "FROM hospitals_temp\n",
    "WHERE ST_Distance(location, ST_Point(-74.237, 42.037)) < 46800.0\n",
    "ORDER BY distance\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could also use `ST_Buffer` to compute the spatial relation and then determine the distance as is shown in the following query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name, ST_Distance(location, ST_Point(-74.237, 42.037)) AS distance\n",
    "FROM hospitals_temp\n",
    "WHERE\n",
    "  ST_Intersects(location,\n",
    "  ST_Buffer(ST_Point(-74.237, 42.037), 46800.0))\n",
    "ORDER BY distance\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A key difference to be noted here is that the `ST_Buffer` in this package supports buffering of arbitrary geometries and can be used to compute in that manner. Note that:\n",
    "- The `ST_Buffer` query on large geometries can be expensive.\n",
    "- For a large number of geometries, the user is advised to calculate the buffers separately, store the buffers in columns, and operate on the stored buffers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT name, ST_Distance(location, ST_WKTToSQL(\n",
    " 'LINESTRING (-74.0 42.0, -73.0 42.0)'))\n",
    "FROM hospitals_temp\n",
    "WHERE ST_Intersects(location, ST_Buffer(ST_WKTToSQL(\n",
    " 'LINESTRING (-74.0 42.0, -73.0 42.0)'), 46800.0))\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "##  Summary\n",
    "\n",
    "In this notebook, you learned how to query spatial data you downloaded from the IBM Watson Studio Gallery. You registered each data frame (one with data on hospitals and another with county information) as a table to run your queries on. The sample queries showed you how to determine the hospitals within a certain distance or in a polygon, to find the name of the county in which a hospital is located, or to identify the counties where the population is underserved by hospitals. The sample queries showed you how to use and combine the most common Spark SQL spatial functions in queries. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "\n",
    "**Linsong Chu**, Research Engineer at IBM Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2019 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 with Spark",
   "language": "python3",
   "name": "python310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
