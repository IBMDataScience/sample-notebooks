{"nbformat_minor": 2, "cells": [{"source": "# Predict Loan Applicant Behavior with TensorFlow Neural Networking\n\nThis notebook guides you through the basic concepts to construct a neural net\nmodel with the\nTensorFlow library in Watson Studio. It includes instructions on how to import the predictive\ndata, train the model to predict\nclient behaviors, and save the model to use for future inference.\n\nSome familiarity with Python is recommended. This notebook runs on Python and Spark.\n\nThis TensorFlow neural network tutorial has several aspects that are unique or\nnot evident in other TensorFlow tutorials, such as the MNIST handwritten digits\ntutorial. The focus is on business, both in terms of the use case and data and\nin terms of extra steps that are needed to help take your data science results to\nproduction.\n\n## Describe the business problem\n\nThis notebook describes how to use the TensorFlow library for a Neural Network\nmodel to predict whether a loan applicant is likely to \u2018default\u2019 on a bank loan\nbased on specific client characteristics. The model must be trained over a\nserial of data batches to increase prediction accuracy.\n\nThe following characteristics are considered:\n\n   * Predictor variables such as age, education, income.\n   * Differing business objectives, for example, curbing the typical defaulters,\nor filtering the best applicants only.\n\nTo train the machine learning model, a sample data set is provided and run\nthrough in this notebook that specifies the data and\nvariables.\n\n## Table of Contents\n\n1. [Import the Data](#import)<br>\n2. [Download the TensorFlow library](#download)<br>\n3. [Tuning the Hyperparameters](#tuning)<br>\n4. [Save the Model](#save-model)<br>\n5. [Derive Confidence Values](#derive)<br>\n6. [Summary](#summary)<br>\n    a. [Related Links](#rel_links)<br>\n    b. [Author information](#author)<br>\n\n<div class=\"alert alert-block alert-info\">\nYou can follow along if you already installed\nJupyter, Python and TensorFlow, or you can take a few minutes to sign up for a free\ntrial of IBM Watson Studio on IBM Cloud to run the notebook on a data science\nplatform <a href=\"https://dataplatform.cloud.ibm.com/\" target=\"_blank\">here</a>.\n</div>\n\n<a id=\"import\"></a>\n## Import the Data\n\nAs a prerequisite, you must prepare a CSV file that you would like to use the\nTensorFlow neural net with.\n\nTo provide an example, a sample data file that is called `bankloanData.csv` from the IBM SPSS Statistics package is extracted and is available for download here: <a href=\"https://github.com/john-boyer-phd/TensorFlow-Samples/tree/master/Neural%20Net\" target=\"_blank\">John Boyer's GitHub repo</a>. The advantages are that theresulting TensorFlow code can be easily adapted to build bigger neural nets that can learn from much larger data sets.\n\nThis particular sample is selected because it is easy to use SPSS to double\ncheck whether the TensorFlow code is behaving as expected (for more\ninformation on IBM SPSS, see [Related Links](#rel-links)).\n\nThe dependent variable is called the \u2018label\u2019, and the data in the column is\ncalled the \u2018labeled data\u2019. In this case, the dependent variable that is\npredicted is the column that is named \u2018default\u2019 in the CSV file.\n\nTo read the CSV file from cloud Object Storage into a Pandas Dataframe, open the\n**Files** window by clicking the binary icon in the upper right corner and\nupload the file. Then, select the empty cell below and click Insert to code, and\nthen Insert Pandas DataFrame. After the code is\nloaded, the cell can be run to read the CSV file.\n\nFor larger data sets, you can prefer to use a SparkSession Dataframe instead, but\nin that case, you\u2019ll need to slightly adjust the numpy extraction code in the\nnext notebook cell.", "cell_type": "markdown", "metadata": {}}, {"source": "# @hidden_cell\n# USE 'INSERT CODE' HERE\n# This function accesses a file in your Object Storage. The definition contains your credentials.\n# You might want to remove those credentials before you share your notebook.\n\n# Please change last two lines to 'df_data_1...' rather than 'df_data_2...' etc.\n", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "1"}}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190629182743-0000\nKERNEL_ID = 19e295ce-1756-4993-b90c-e1c34a51c7d2\n"}, {"output_type": "execute_result", "data": {"text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>age</th>\n      <th>ed</th>\n      <th>employ</th>\n      <th>address</th>\n      <th>income</th>\n      <th>debtinc</th>\n      <th>creddebt</th>\n      <th>othdebt</th>\n      <th>default</th>\n      <th>preddef1</th>\n      <th>preddef2</th>\n      <th>preddef3</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>41</td>\n      <td>3</td>\n      <td>17</td>\n      <td>12</td>\n      <td>176</td>\n      <td>9.3</td>\n      <td>11.359392</td>\n      <td>5.008608</td>\n      <td>1</td>\n      <td>0.808394</td>\n      <td>0.788640</td>\n      <td>0.213043</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>27</td>\n      <td>1</td>\n      <td>10</td>\n      <td>6</td>\n      <td>31</td>\n      <td>17.3</td>\n      <td>1.362202</td>\n      <td>4.000798</td>\n      <td>0</td>\n      <td>0.198297</td>\n      <td>0.128445</td>\n      <td>0.436903</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>40</td>\n      <td>1</td>\n      <td>15</td>\n      <td>14</td>\n      <td>55</td>\n      <td>5.5</td>\n      <td>0.856075</td>\n      <td>2.168925</td>\n      <td>0</td>\n      <td>0.010036</td>\n      <td>0.002987</td>\n      <td>0.141023</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>41</td>\n      <td>1</td>\n      <td>15</td>\n      <td>14</td>\n      <td>120</td>\n      <td>2.9</td>\n      <td>2.658720</td>\n      <td>0.821280</td>\n      <td>0</td>\n      <td>0.022138</td>\n      <td>0.010273</td>\n      <td>0.104422</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>24</td>\n      <td>2</td>\n      <td>2</td>\n      <td>0</td>\n      <td>28</td>\n      <td>17.3</td>\n      <td>1.787436</td>\n      <td>3.056564</td>\n      <td>1</td>\n      <td>0.781588</td>\n      <td>0.737885</td>\n      <td>0.436903</td>\n    </tr>\n  </tbody>\n</table>\n</div>", "text/plain": "   age  ed  employ  address  income  debtinc   creddebt   othdebt default  \\\n0   41   3      17       12     176      9.3  11.359392  5.008608       1   \n1   27   1      10        6      31     17.3   1.362202  4.000798       0   \n2   40   1      15       14      55      5.5   0.856075  2.168925       0   \n3   41   1      15       14     120      2.9   2.658720  0.821280       0   \n4   24   2       2        0      28     17.3   1.787436  3.056564       1   \n\n   preddef1  preddef2  preddef3  \n0  0.808394  0.788640  0.213043  \n1  0.198297  0.128445  0.436903  \n2  0.010036  0.002987  0.141023  \n3  0.022138  0.010273  0.104422  \n4  0.781588  0.737885  0.436903  "}, "execution_count": 1, "metadata": {}}], "execution_count": 1}, {"source": "As you can see from the chart that is generated, the predictor variables are as follows:\n\n- *age*\n- *ed* (level of education)\n- *employ*\n- *address* (years at current address)\n- *income* (household income in thousands)\n- *debtinc*\n- *creddebt*\n- *othdebt* (other debt in thousands)\n\nThe predictor variables are also called \u2018features\u2019. The remaining columns of\ndata are unnecessary for this particular model.\n\nNow, look at the details of the data columns.\n\nIf the following code cells generate an error message, please recitify the last two lines of the previous import code to \n<br><br>\n*df_data_1 = pd.read_csv(body)<br>\ndf_data_1.head()*", "cell_type": "markdown", "metadata": {}}, {"source": "df_data_1.dtypes", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "2"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "age           int64\ned            int64\nemploy        int64\naddress       int64\nincome        int64\ndebtinc     float64\ncreddebt    float64\nothdebt     float64\ndefault      object\npreddef1    float64\npreddef2    float64\npreddef3    float64\ndtype: object"}, "execution_count": 2, "metadata": {}}], "execution_count": 2}, {"source": "df_data_1.values", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "3"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([[41, 3, 17, ..., 0.808394327359702, 0.7886404318214371,\n        0.21304337612811897],\n       [27, 1, 10, ..., 0.19829747615910395, 0.128445387038174,\n        0.43690300550604605],\n       [40, 1, 15, ..., 0.0100361080990023, 0.00298677834821412,\n        0.141022623460993],\n       ...,\n       [48, 1, 13, ..., 0.0301374981044824, 0.0325702625943738,\n        0.24801041775523303],\n       [35, 2, 1, ..., 0.26900345101699397, 0.37854649636973203,\n        0.181814378077261],\n       [37, 1, 20, ..., 0.006397812918809229, 0.0111731232851226,\n        0.30304155578236497]], dtype=object)"}, "execution_count": 3, "metadata": {}}], "execution_count": 3}, {"source": "The next couple of cells prunes the data. The code assumes that a\nPandas dataframe that are named `df_data_1` exists and uses it to extract the data into\nnumpy arrays that are needed as input to the TensorFlow API. The comprehension in the\nfirst np.array() removes instances (rows) that are missing a label.", "cell_type": "markdown", "metadata": {}}, {"source": "import numpy as np\n\n# Make a numpy array from the dataframe, except remove rows with no value for 'default'\ni = list(df_data_1.columns.values).index('default')\ndata = np.array([x for x in df_data_1.values if x[i] in ['0', '1']])\n\n# Remove the columns for preddef1, predef2 and preddef3\ndata = np.delete(data, slice(9,12), axis=1)\n\n# Separate the 'predictors' (aka 'features') from the dependent variable (aka 'label') \n# that we will learn how to predict\npredictors = np.delete(data, 8, axis=1)\ndependent = np.delete(data, slice(0, 8), axis=1)", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "4"}}, "outputs": [], "execution_count": 4}, {"source": "The next cell reshapes the data slightly, with the following steps:\n\n1. Separate predictors from the dependent variable.\n2. Convert labeled data from type string to type integer.\n3. Flatten the dependent array to one dimension to match the shape of the data\nthat comes from the neural network output layer.\n4. Convert the predictor type to Float to facilitate matrix multiplication with\nweights and biases within the neural network.", "cell_type": "markdown", "metadata": {}}, {"source": "# Convert the label type to numeric categorical representing the classes to predict (binary classfier)\ndependent = dependent.astype(int)\n\n# And flatten it to one dimensional for use as the expected output label vector in TensorFlow\ndependent = dependent.flatten()\ndependent\n\n# Convert all the predictors to float to simplify this demo TensorFlow code\npredictors = predictors.astype(float)\n\n# Get the shape of the predictors\nm, n = predictors.shape\nm, n", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "5"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "(700, 8)"}, "execution_count": 5, "metadata": {}}], "execution_count": 5}, {"source": "# Take a peak at the first sample of the predictors\npredictors[0]", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "6"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([ 41.      ,   3.      ,  17.      ,  12.      , 176.      ,\n         9.3     ,  11.359392,   5.008608])"}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "# Take a peak at the dependent variable values\ndependent", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "7"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n       1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n       0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n       0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1,\n       0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,\n       0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0,\n       0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n       1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n       0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0,\n       1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n       1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0,\n       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n       1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n       0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1,\n       0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"}, "execution_count": 7, "metadata": {}}], "execution_count": 7}, {"source": "When you train the model, the feature values of the instances (rows) of data are\ninput to the neural net, and the weights and biases of the neural network are\nadjusted to minimize \u2018loss\u2019, which coarsely maps to maximizing accuracy of the\nneural network\u2019s output layer predictions of the labeled data.\n\n<a id=\"random\"></a>\n### Randomize the training data\n\nThe next cell takes the first 500 instances as training data, and leaves the\nremaining 200 instances as a test set. This particular sample is already\nrandomly enumerated, so there is no need to randomly select the training and\ntest sets from the data. The code also chooses about a 70:30 percent split\nfor training and testing. Though, it does not round the split sample to a size\ndivisible by the training batch size, which is defined later in the notebook.\n\nThis cell also defines a method that returns batch-sized slices of the training\ndata. If the training data is too large to fit in memory, then this method can\ninstead load data one batch at a time, such as with an SQL query.", "cell_type": "markdown", "metadata": {}}, {"source": "# Partition the input data into a training set and a test set\n\nm_train = 500\nm_test = m - m_train\n\npredictors_train = predictors[:m_train]\ndependent_train = dependent[:m_train]\n\npredictors_test = predictors[m_train:]\ndependent_test = dependent[m_train:]\n\n# Gets a batch of the training data. \n# NOTE: Rather than loading a whole large data set as above and then taking array slices as done here, \n#       This method can connect to a data source and select just the batch needed.\ndef get_training_batch(batch_num, batch_size):\n    lower = batch_num * (m_train // batch_size)\n    upper = lower + batch_size\n    return predictors_train[lower:upper], dependent_train[lower:upper]", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "8"}}, "outputs": [], "execution_count": 8}, {"source": "<a id=\"download\"></a>\n## Download the TensorFlow library\n\nNow you are set to start the training with some actual TensorFlow code. The next\ncell imports the TensorFlow library and makes a few initializations. After, it\ndefines a method that will build a neural network layer of any size, connect it\nto a preceding layer, and set the output activation function.", "cell_type": "markdown", "metadata": {}}, {"source": "import tensorflow as tf\n\n# Make this notebook's output stable across runs\ntf.reset_default_graph()\ntf.set_random_seed(42)\nnp.random.seed(42)\n\n# A method to build a new neural net layer of a given size,  \n# fully connect it to a given preceding layer X, and \n# compute its output Z either with or without (default) an activation function\n# Call with activation=tf.nn.relu or tf.nn.sigmoid or tf.nn.tanh, for examples\n\ndef make_nn_layer(layer_name, layer_size, X, activation=None):\n    with tf.name_scope(layer_name):\n        X_size = int(X.get_shape()[1])\n        SD = 2 / np.sqrt(X_size)\n        weights = tf.truncated_normal((X_size, layer_size), dtype=tf.float64, stddev=SD)\n        W = tf.Variable(weights, name='weights')\n        b = tf.Variable(tf.zeros([layer_size], dtype=tf.float64), name='biases')\n        Z = tf.matmul(X, W) + b\n        if activation is not None:\n            return activation(Z)\n        else:\n            return Z", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "9"}}, "outputs": [], "execution_count": 9}, {"source": "The next section is where most of the tuning of hyperparameters occurs.\n\n<a id=\"tuning\"></a>\n## Tuning network structure and activation function\n\nNow you can add the code cell that builds the neural network structure. In this\ncase, you have one input layer `X`, one hidden layer `hidden1`, and one output\nlayer `outputs`. The comments in the following code cell explain how to add more\nhidden layers, but with this sample data, it is possible to learn everything\nwith only one layer. The output layer has two nodes, one for outputting class 0\n(the loan applicant will not default) and the other for class 1 (the loan applicant\nwill default). The `y` variable will be used during tis usedtore the\nlabeled data that is expected to match with the output layer.\n\nOne line of code that helps makes this tutorial unique is the one that creates a\ntf.identity() node that gives the name `nn_output`. This allows you to save a\nname for the output layer so that you can recover and use the output layer after\na restore.", "cell_type": "markdown", "metadata": {}}, {"source": "# Make the neural net structure\n\nn_inputs = n\nn_hidden1 = n \n### n_hidden2 = n // 2\nn_outputs = 2   # Two output classes: defaulting or non-defaulting on loan\n\nX = tf.placeholder(tf.float64, shape=(None, n_inputs), name='X')\n\nwith tf.name_scope('nn'):\n    hidden1 = make_nn_layer('hidden1', n_hidden1, X, activation=tf.nn.relu)\n    hidden2 = hidden1\n    ### hidden2 = make_nn_layer('hidden2', n_hidden2, hidden1, activation=tf.nn.relu)\n    outputs = make_nn_layer('outputs', n_outputs, hidden2) \n    outputs = tf.identity(outputs, \"nn_output\")\n    \ny = tf.placeholder(tf.int64, shape=(None), name='y')", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "10"}}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "WARNING:tensorflow:From /opt/ibm/conda/miniconda36/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n"}], "execution_count": 10}, {"source": "The input parameters that are passed to a neural network during inference are the feature\nvalues. During training, the input parameters are feature values and the\nexpected output are labeled data. But the neural network is adaptable beyond\npreviously mentioned input parameters, and these configurable parts are called\nhyperparameters. The hyperparameters contain the number and size of the hidden\nlayers and the activation function.  You can try other numbers and sizes of\nhidden layers. \u2018tanh\u2019 and \u2018sigmoid\u2019 are other activation functions that you can try.\nHowever, the default configuration works best on this data.\n\nWhat you accomplished so far in this notebook is create the main part of a\nTensorFlow compute graph that has the shape for a neural network. Next,\nyou must attach two different root nodes to the output layer, one that adds\nfunctions for training and the other for testing, like in the following\ncell. The \u2018training_op\u2019 uses the gradient descent method for minimizing loss (of\nperfect confidence in the correct answers and zero confidence in incorrect\nanswers, where the correct answers are provided by the labeled data that are\nin \u2018y\u2019).", "cell_type": "markdown", "metadata": {}}, {"source": "# Define how the neural net will learn\n\nwith tf.name_scope('loss'):\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n    loss = tf.reduce_mean(xentropy, name='l')\n    \nlearning_rate = 0.01\nwith tf.name_scope(\"train\"):\n    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n    training_op = optimizer.minimize(loss)\n    \nwith tf.name_scope(\"test\"):\n    correct = tf.nn.in_top_k(tf.cast(outputs, tf.float32), y, 1)\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    ", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "11"}}, "outputs": [], "execution_count": 11}, {"source": "<a id=\"save-dir\"></a>\n### Create save directories\n\nNow you must run this cell that sets up your ability to save the model when it\nis trained. You need to do these mkdir commands the first time that you run\nthis notebook only, so you might want to put them in a separate cell to make it\neasier to skip them. ", "cell_type": "markdown", "metadata": {}}, {"source": "# Set up the ability to save and restore the trained neural net...\n\ninit = tf.global_variables_initializer()\nsaver = tf.train.Saver()", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "12"}}, "outputs": [], "execution_count": 12}, {"source": "<a id=\"save\"></a>\n## Train the Data\n\nNow you can run the magic notebook cell that trains and saves the trained model.\nEach epoch of training exposes the neural net to the entire set of training\ndata. When you run the code, you can see the accuracy of results increase over\nthe many epochs of runs, just as biological neural networks learn through\nrepetition. For each epoch, you run through the training data in batches to\nsimulate how you\u2019d handle a larger training set. Each batch of features and\ncorresponding labeled data is fed to the `training_op` root node in the compute\ngraph, which is run by *training_session.run()*.\n\nOne aspect of this tutorial that is unique (relative to other tutorials) is the\nrandomization of the training data that takes place at the beginning of each\nepoch.  This essentially drives different data into the batches in each epoch,\nwhich dramatically improves accuracy over a larger number of epochs (though it\nis much easier programmatically to do this randomization when all data fits into\nmemory).", "cell_type": "markdown", "metadata": {}}, {"source": "# TRAINING TIME\n\n# This is how many times to use the full set of training data\nn_epochs = 3000\n\n# For a larger training set, it's typically necessary to break training into\n# batches so only the memory needed to store one batch of training data is used\nbatch_size = 50\n\nwith tf.Session() as training_session:\n    init.run()\n    \n    for epoch in range(n_epochs):\n        \n        # Shuffling (across batches) is easier to do for small data sets and\n        # helps increase accuracy\n        training_set = [[pt_elem, dependent_train[i]] for i, pt_elem in enumerate(predictors_train)]\n        np.random.shuffle(training_set)\n        predictors_train = [ts_elem[0] for ts_elem in training_set]\n        dependent_train = [ts_elem[1] for ts_elem in training_set]\n        \n        # Loop through the whole training set in batches\n        for batch_num in range(m_train // batch_size):\n            X_batch, y_batch = get_training_batch(batch_num, batch_size)\n            training_session.run(training_op, feed_dict={X: X_batch, y: y_batch})\n\n        if epoch % 100 == 99:\n            acc_train = accuracy.eval(feed_dict={X: predictors_train, y: dependent_train})\n            acc_test = accuracy.eval(feed_dict={X: predictors_test, y: dependent_test})\n            print(epoch+1, \"Training accuracy:\", acc_train, \"Testing accuracy:\", acc_test)\n\n    save_path = saver.save(training_session, \"../Neural Net.ckpt\")\n    \n    # A quick test with the trained model \n    Z = outputs.eval(feed_dict={X: predictors_test[:20]})\n    dependent_pred = np.argmax(Z, axis=1)\n    print(\"\")\n    print(\"Actual classes:   \", dependent_test[:20])  \n    print(\"Predicted classes:\", dependent_pred)", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "14"}}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "100 Training accuracy: 0.76 Testing accuracy: 0.755\n200 Training accuracy: 0.774 Testing accuracy: 0.79\n300 Training accuracy: 0.798 Testing accuracy: 0.82\n400 Training accuracy: 0.798 Testing accuracy: 0.835\n500 Training accuracy: 0.808 Testing accuracy: 0.825\n600 Training accuracy: 0.796 Testing accuracy: 0.815\n700 Training accuracy: 0.802 Testing accuracy: 0.81\n800 Training accuracy: 0.788 Testing accuracy: 0.815\n900 Training accuracy: 0.81 Testing accuracy: 0.815\n1000 Training accuracy: 0.82 Testing accuracy: 0.81\n1100 Training accuracy: 0.818 Testing accuracy: 0.81\n1200 Training accuracy: 0.806 Testing accuracy: 0.805\n1300 Training accuracy: 0.802 Testing accuracy: 0.815\n1400 Training accuracy: 0.812 Testing accuracy: 0.82\n1500 Training accuracy: 0.806 Testing accuracy: 0.8\n1600 Training accuracy: 0.816 Testing accuracy: 0.81\n1700 Training accuracy: 0.806 Testing accuracy: 0.81\n1800 Training accuracy: 0.822 Testing accuracy: 0.825\n1900 Training accuracy: 0.802 Testing accuracy: 0.805\n2000 Training accuracy: 0.83 Testing accuracy: 0.82\n2100 Training accuracy: 0.812 Testing accuracy: 0.81\n2200 Training accuracy: 0.824 Testing accuracy: 0.825\n2300 Training accuracy: 0.804 Testing accuracy: 0.815\n2400 Training accuracy: 0.818 Testing accuracy: 0.815\n2500 Training accuracy: 0.828 Testing accuracy: 0.825\n2600 Training accuracy: 0.83 Testing accuracy: 0.815\n2700 Training accuracy: 0.822 Testing accuracy: 0.815\n2800 Training accuracy: 0.82 Testing accuracy: 0.805\n2900 Training accuracy: 0.814 Testing accuracy: 0.83\n3000 Training accuracy: 0.826 Testing accuracy: 0.825\n\nActual classes:    [0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\nPredicted classes: [1 1 1 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n"}], "execution_count": 13}, {"source": "<div class=\"alert alert-block alert-warning\"><b>Author's Sidebar:</b>\nWhen you do business with a real stakeholder customer, you need to have a second\ntest set, often called a validation set or a blind set. Why do you need a second\ntest set? The typical reply might be akin to, \u201cI don\u2019t know, to\ndouble-check accuracy?\u201d  Well, that is not incorrect. But if you look at the structure of\ntraining, the weights and biases are affected not just by the training data.\nIndirectly, they are also affected by the test data because you choose\n<i>n_epochs</i> to keep running training epochs until you get the best accuracy\non the test set. In other words, you're teaching to the test. The validation set\nor blind set has no such indirect effect on the weights and biases that are computed for\nthe neural network. It is another test set that to ensure construct\nvalidity, must be randomly from the same pool of data that the training set\nand test set are randomly selected from. In this way, the validation set is not\njust the \u2018final exam\u2019, it\u2019s the first experience of the real world.\n</div>\n\n<a id=\"save-model\"></a>\n## Save and Restore models for a Production Environment\n\nWhen all training is done, the model is saved into the data sets subdirectory that is created previously. For larger training data sets, you can call on\nTensorFlow to save on checkpoints throughout the epoch-based training so that\nyou can stop and resume training if needed. ", "cell_type": "markdown", "metadata": {}}, {"source": "The data files that TensorFlow created during that save operation can be\ntransported to a production environment. The neural network can then be restored\nby using the code in the next cell, and the input and output layers can be obtained\nand used for inference, by using *get_tensor_by_name()*.\n\nThe next cell also shows how to reference into the hierarchy of a name scope to get the output layer from within the neural network structure. It mocks\nup by having a REST API receive a batch of feature instances and converting them\nto a numpy array by taking a slice of the test data.  With the inference\nTensorFlow session, the compute graph and the values it contained are restored.\nAfter that, you obtain the tensors corresponding to the neural network input and output\nlayers by using the names that you previously assigned.  Then, you run the output\nlayer, and give the batch of feature instances to the input layer `X`\n(*inference_session.run()*). The predictions of the dependent variable are then\nobtained by choosing whichever of the two output layer nodes that have the higher\nvalue (by using *np.argmax()*).", "cell_type": "markdown", "metadata": {}}, {"source": "# Restore the saved model and use it to perform inference on a \"received\" new set of data\n\n# We will simulate \"receiving\" the new data by taking some slice of the test set (all in this case).\npredictors_received = predictors_test[:]\n\nimport tensorflow as tf_inference\n\nwith tf_inference.Session() as inference_session:\n    inf_saver = tf_inference.train.import_meta_graph('../Neural Net.ckpt.meta')\n    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../'))\n    \n    graph = tf_inference.get_default_graph()\n    X = graph.get_tensor_by_name(\"X:0\")\n    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n\n    Z = inference_session.run(nn_output, feed_dict={X: predictors_received})\n    dependent_pred = np.argmax(Z, axis=1)\n    \nprint(\"Actual classes:   \", dependent_test[20:40])\nprint(\"Predicted classes:\", dependent_pred[20:40])", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "16"}}, "outputs": [{"output_type": "stream", "name": "stderr", "text": "WARNING:tensorflow:From /opt/ibm/conda/miniconda36/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse standard file APIs to check for files with this prefix.\n"}, {"output_type": "stream", "name": "stdout", "text": "Actual classes:    [0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1]\nPredicted classes: [0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 1]\n"}], "execution_count": 14}, {"source": "### Transfer learning\n\nAs another unique aspect of this tutorial, you can also use this\nmethod of naming with <i>tf.identity</i> and then getting the tensor from a\nrestored graph to transfer learning between neural nets. Specifically, when you\ncreate a hidden layer with <i>make_nn_layer()</i>, you can name it with\n<i>tf.identity</i>. Then, you train and save as shown above. To transfer to a\nsecond neural net, you restore the trained and saved one, get the hidden layer\nby name, attach alternative fully connected hidden layers as needed, and an\nalternative output layer, and then train the new second neural network by using the method above.\n\n<a id=\"derive\"></a>\n## Derive confidence values for neural net output\n\nFinally, to showcase more feature that makes this tutorial unique, you can look\nat how to get the actual confidence values for the predictions. This might not\nseem as important when you do the MNIST hand-written digit tutorial, but\nin a business context it\u2019s important to know how much confidence we have in an\nanswer.", "cell_type": "markdown", "metadata": {}}, {"source": "with tf_inference.Session() as inference_session:\n    inf_saver = tf_inference.train.import_meta_graph('../Neural Net.ckpt.meta')\n    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../'))\n    \n    graph = tf_inference.get_default_graph()\n    X = graph.get_tensor_by_name(\"X:0\")\n    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n\n    dependent_prob = inference_session.run(tf_inference.nn.softmax(nn_output), feed_dict={X: predictors_received})\n\nconfidences = [p[dependent_pred[i]] for i, p in enumerate(dependent_prob)]\n    \nprint(\"Confidences: \", confidences[20:40])\nprint(\"\")\nprint(\"Probabilities: \", dependent_prob[20:40])", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "17"}}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Confidences:  [0.7211578884122729, 0.8060704757830571, 0.8393404197391471, 0.9952225189537691, 0.6358370722403351, 0.9230137006102419, 0.9246727235093638, 0.7843975950250139, 0.9999999999958342, 0.6456186876399689, 0.7273640893160148, 0.6456186876399689, 0.8395932856637522, 0.9429180151471286, 0.6817096039466525, 0.99981345698851, 0.967785956412393, 0.5924861126052802, 0.9929383179955898, 0.5220483545876665]\n\nProbabilities:  [[7.21157888e-01 2.78842112e-01]\n [8.06070476e-01 1.93929524e-01]\n [8.39340420e-01 1.60659580e-01]\n [9.95222519e-01 4.77748105e-03]\n [6.35837072e-01 3.64162928e-01]\n [9.23013701e-01 7.69862994e-02]\n [9.24672724e-01 7.53272765e-02]\n [7.84397595e-01 2.15602405e-01]\n [1.00000000e+00 4.16586334e-12]\n [3.54381312e-01 6.45618688e-01]\n [7.27364089e-01 2.72635911e-01]\n [3.54381312e-01 6.45618688e-01]\n [8.39593286e-01 1.60406714e-01]\n [9.42918015e-01 5.70819849e-02]\n [6.81709604e-01 3.18290396e-01]\n [9.99813457e-01 1.86543011e-04]\n [9.67785956e-01 3.22140436e-02]\n [4.07513887e-01 5.92486113e-01]\n [9.92938318e-01 7.06168200e-03]\n [4.77951645e-01 5.22048355e-01]]\n"}], "execution_count": 15}, {"source": "To get the confidence values, a new root node was inserted onto the output layer\nto apply the *softmax* function. Now the compute graph produces the probability\nof occurrence for each output value. Then, one final comprehension was done to\nferret out the confidences of the predicted labels for each feature instance.", "cell_type": "markdown", "metadata": {}}, {"source": "# Now we're going to assess the quality of the neural net using ROC curve and AUC\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# send the actual dependent variable classifications for param 1, \n# and the confidences of the true classification for param 2.\nFPR, TPR, _ = roc_curve(dependent_test, dependent_prob[:, 1])\n\n# Calculate the area under the confidence ROC curve.\n# This area is equated with the probability that the classifier will rank \n# a randomly selected defaulter higher than a randomly selected non-defaulter.\nAUC = auc(FPR, TPR)\n\n# What is \"good\" can dependm but an AUC of 0.7+ is generally regarded as good, \n# and 0.8+ is generally regarded as being excellent \nAUC", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "18"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0.8527397260273972"}, "execution_count": 16, "metadata": {}}], "execution_count": 16}, {"source": "# Now we'll plot the confidence ROC curve \nplt.figure()\nplt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % AUC)\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.02])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend(loc=\"lower right\")\nplt.show()", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "19"}}, "outputs": [{"output_type": "display_data", "data": {"image/png": "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\n", "text/plain": "<Figure size 432x288 with 1 Axes>"}, "metadata": {"needs_background": "light"}}], "execution_count": 17}, {"source": "You can also run a classification report to see how the model is behaving within\neach class.", "cell_type": "markdown", "metadata": {}}, {"source": "from sklearn.metrics import classification_report\nprint(classification_report(dependent_test, dependent_pred))", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "20"}}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "              precision    recall  f1-score   support\n\n           0       0.86      0.91      0.88       146\n           1       0.71      0.59      0.65        54\n\n   micro avg       0.82      0.82      0.82       200\n   macro avg       0.78      0.75      0.77       200\nweighted avg       0.82      0.82      0.82       200\n\n"}], "execution_count": 18}, {"source": "As you can see, the recall on non-defaulters is 91%, which means the model\nallows most non-defaulters proceed to getting a loan. The recall for defaulters\nis only 59%, so many defaulters are flagged, but a substantial number are not.\n\nIf the business objective of the model is to curb many but not all of the deficient\napplications while not perturbing business for better customers, then this is a good model.  But suppose that the objective is to have a model that lets through\nonly the best applications and flags the rest for review. It might be costly to\nreview many applications, but that cost might be less than the cost of making\nconsiderable loans that don't get repaid.\n\nTo accommodate for the different objectives, this notebook calculates the\nconfidence threshold that it would take to get a perfect true positive rate (that is, to\ndetect all of the defaulters in the test set) even if that means you also get many false positives (that is, non-defaulters that the model classifies as\ndefaulters).  If you look up at the ROC curve, you can see that a perfect 100%\nTPR happens just above 60% FPR, so the overall accuracy will be lower due to the number of false positives, but the\nconfidence threshold is better from the standpoint of achieving the alternative desired business objective.", "cell_type": "markdown", "metadata": {}}, {"source": "# Say you want a model that is very accurate at recalling true positives (defaulters), \n# even if it gets a lot a false positives (non-defaulters). You might be automatically\n# accepting the false classifications and, for true classifications, you may send them\n# for human review rather than rejecting their loan applications.\n\n# The lowest confidence that can give 100% TPR on the test set is equal to the \n# true class with the lowest confidence, so we'll find that now\ndefaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 1]\nmin_conf = np.min(defaulter_probs)\nmin_conf", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "21"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0.043730491447763704"}, "execution_count": 19, "metadata": {}}], "execution_count": 19}, {"source": "# Each non-defaulter with a confidence at or above min_conf would be predicted \n# to be a defaulter (which would be a false positve prediction for a non-defaulter)\n\nnon_defaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 0]\nfalse_positives = [x for x in non_defaulter_probs if x >= min_conf]\n\ntotal = len(defaulter_probs) + len(non_defaulter_probs)\ntotal_correct = total - len(false_positives)\naccuracy = float(total_correct) / total\n\n# Overall accuracy would suffer quite a bit, but this achieves \n# the desired high accuracy on true positive identification (defaulters)  \naccuracy", "cell_type": "code", "metadata": {"attributes": {"classes": [], "id": "", "n": "22"}}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0.54"}, "execution_count": 20, "metadata": {}}], "execution_count": 20}, {"source": "<a id=\"summary\"></a>\n## Summary\n\nYou learned how to import the TensorFlow library from IBM Watson Studio, create\na neural network with layers, train the model to accurately predict business\nbehavior to achieve your business objective.\n\n<a id=\"rel_links\"></a>\n## Related Links\n\n1. <a href=\"https://datascience.ibm.com/\" target=\"_blank\">See Watson Studio</a>\n2. <a href=\"https://www.ibm.com/developerworks/community/profiles/html/profileView.do?userid=060000VMNY&lang=en\" target=\"_blank\">Author's Blog on IBM Developer Works</a>\n3. <a href=\"https://www.ibm.com/developerworks/community/blogs/JohnBoyer/entry/Measuring_the_Quality_of_a_TensorFlow_Neural_Network_An_IBM_Data_Science_Experience?lang=en\" target=\"_blank\">Author Blog: Measuring the Quality of a TensorFlow Neural Network</a>\n4. <a href=\"https://www.ibm.com/ca-en/marketplace/spss-statistics\" target=\"_blank\">IBM SPSS Statistics</a>\n\n\n<a id=\"author\"></a>\n### Author\n\nJohn M. Boyer, IBM Global Chief Data Office\n\n<hr>\nCopyright \u00a9 IBM Corp. 2018, 2019. This notebook and its source code are released under\nthe terms of the MIT License.", "cell_type": "markdown", "metadata": {}}, {"source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>", "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.6 with Spark", "name": "python36", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.8", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}
