{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning optimization using the cognitive assistant function\n",
    "\n",
    "This notebook demonstrates the use of the cognitive assistant that is part of Jupyter Notebook running a Python kernel. In this sample, you will use cognitive assistant function to optimize machine learning piplelines for a notebook user's dataframe.\n",
    "\n",
    "Cognitive assistant currently employs Apache® Spark (PySpark) and SKLearn (Python scikit-learn) analytics on Spark 1.6 and Python 2.7, optionally with feature selection, for classification and regression. \n",
    "\n",
    "The notebook remains interactive during the optimizations. The execution counter and the kernel activity indicator reflect the activity of both the notebook user and the cognitive assistant.\n",
    "\n",
    "## Contents\n",
    "\n",
    "This notebook has the following main sections:\n",
    "1. [Download the data](#download)\n",
    "1. [Format the data](#format)\n",
    "1. [Run the cognitive assistant optimization](#cads)\n",
    "\n",
    "<a id=\"download\"></a>\n",
    "## 1. Download the data\n",
    "\n",
    "Because cognitive assistant is especially suited to large data sets, in the following step, you'll be retrieving a large (2.6 GB) .csv file that contains data on HIGGS boson particles. \n",
    "\n",
    "From the description of the data set on the UCI Web site: \"The data has been produced using Monte Carlo simulations. The first 21 features (columns 2-22) are kinematic properties measured by the particle detectors in the accelerator. The last seven features are functions of the first 21 features; these are high-level features derived by physicists to help discriminate between the two classes. There is an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks are presented in the original paper. The last 500,000 examples are used as a test set.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "#Download a portion of a popular compressed csv dataset.  Target kilobytes of data are specified after '-ge' nine or so lines below\n",
    "cat << 'EOF' > limited.sh\n",
    "# optionally you can limit the download rate to wget via --limit-rate=2m\n",
    "wget -nv  -O limited.csv.gz https://archive.ics.uci.edu/ml/machine-learning-databases/00280/HIGGS.csv.gz &\n",
    "WGET_PID=$!\n",
    "while [ `ps $WGET_PID | wc -l ` -eq 2 ] ; do\n",
    "    du -k --apparent-size limited.csv.gz\n",
    "    sleep 1\n",
    "    #modify the target kilobyte download size after -ge below, or comment out the line below to get the whole file\n",
    "    if [ -e limited.csv.gz ] ; then if [ `du -k --apparent-size limited.csv.gz | awk '{print \\$1}'` -ge 10000  ] ; then kill -15 $WGET_PID; fi; fi\n",
    "done\n",
    "EOF\n",
    "cat limited.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!echo \"expect an unexpected-end-of-file-message, that is ok if you specified a limited download size\"\n",
    "!bash limited.sh\n",
    "#sed will swallow incomplete last line which will likely occur in the case of a partial download\n",
    "!gunzip -c -q limited.csv.gz | sed -e 's/rarelyseen/rarelyseen/'> limited.csv\n",
    "!echo `wc -l limited.csv` ' complete lines of csv retrieved'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -f limited.csv.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"format\"></a>\n",
    "## 2. Format the data\n",
    "\n",
    "Format the data into a Pyspark SQL dataframe consisting of a numeric `label` column and a vector `features` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "  .options(header='false', inferschema='true')\\\n",
    "  .load(\"limited.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Remove compressed file\n",
    "\n",
    "You can clear up space on your filesystem by removing the compressed *limited.csv.gz* file. Because of the deferred computation in Spark, the uncompressed file is read later and cannot be erased at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!rm -f limited.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "allNames = [f.name for f in df.schema.fields]\n",
    "#FIRST column is the label (unlike many csv where it is the last)\n",
    "labelName = allNames[0]\n",
    "print 'labelName ' + labelName\n",
    "featureNames = [ f.name for f in df.schema.fields if f.name != labelName ]\n",
    "print 'featureNames=' + str(featureNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "\n",
    "featureAssembler = VectorAssembler(\n",
    "    inputCols=featureNames,\n",
    "    outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create features (vector) column then select label and features column, with renaming.\n",
    "newDF=featureAssembler.transform(df).selectExpr(labelName + \" as label\",\"features as features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print newDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"cads\"></a>\n",
    "## 3. Run the cognitive assistant optimization\n",
    "\n",
    "Before you can use the cognitive assistant, you must import the cognitive_assistant package and then start the assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cognitive_assistant as cognitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cognitive.assistant.startAssistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Invoke the cognitive assistant function\n",
    "\n",
    "You invoke the cognitive assistant function by providing it the following information:\n",
    "\n",
    "-  the name of the data frame\n",
    "-  the prediction type\n",
    "    -  For a classification algorithm, you specify predictionType='CLASSIFICATION' or goalTags=\"CADS_FS_JY_CL\"\n",
    "    -  For a regression algorithm, you specify predictionType='REGRESSION' or goalTags=\"CADS_FS_JY_RG\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cognitive.assistant.startOptimization(newDF, goalTags=\"CADS_FS_JY_CL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Check on the progress\n",
    "\n",
    "You can check on the progress of optimization by running the following code. Re-invoke this as many times as you want to view current results. An error message can be expected while the optimization is starting up and you may need to wait until the process is fulling running before checking the progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cognitive.assistant.visualizeProgress()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Do a quick check of CPU - Optional\n",
    "\n",
    "To further check on the progress of your optimization, you can can run the following command to display CPU activity and the number of processes that are running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!top -u $USER -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.4 Stop the cognitive assistant process\n",
    "\n",
    "When optimization has completed or progressed to an acceptable point, the cognitive assistant can be stopped by running the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cognitive.assistant.stopAssistant()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You downloaded and formatted a publicly available data set and then used the cognitive assistant to optimize the pipeline. You're probably feeling pretty good about yourself right now. You ROCK!\n",
    "\n",
    "## Authors\n",
    "\n",
    "**Peter D. Kirchner**, PhD (Electrical Engineering), is a Research Scientist persuing computer science research in machine learning and cloud computing at the IBM Thomas J. Watson Research Center. He is presently engaged in cognitive automation of data science workflow to assist data scientists, focused on cloud-based deployments and scalability.\n",
    "\n",
    "**Mike Sochka** is a content designer focusing on IBM Data Science Experience and Watson Machine Learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "### References\n",
    "\n",
    "Baldi, P., P. Sadowski, and D. Whiteson. “Searching for Exotic Particles in High-energy Physics with Deep Learning.” Nature Communications 5 (July 2, 2014).\n",
    "\n",
    "Lichman, M. (2013). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2017 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 1.6",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}