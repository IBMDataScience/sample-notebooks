{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Use XGBoost to classify tumors with `ibm-watson-machine-learning`"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook contains steps and code to get data from the IBM Watson Studio Community, create a predictive model, and start scoring new data. It introduces commands for getting data and for basic data cleaning and exploration, model training, model persistance to Watson Machine Learning repository, model deployment, and scoring.\n\nSome familiarity with Python is helpful. This notebook runs Python. It uses XGBoost and scikit-learn.\n\nYou will use a publicly available data set, the Breast Cancer Wisconsin (Diagnostic) Data Set, to train an XGBoost Model to classify breast cancer tumors (as benign or malignant) from 569 diagnostic images based on measurements such as radius, texture, perimeter and area. XGBoost is short for \u201cE**x**treme **G**radient **Boost**ing\u201d.\n\nThe XGBoost classifier makes its predictions based on the majority vote from collection of models which are a set of classification trees. It uses the combination of weak learners to create a single strong learner. It\u2019s a sequential training process, whereby new learners focus on the misclassified examples of previous learners.\n\n\n## Learning goals\n\nYou will learn how to:\n\n-  Load a CSV file into numpy array\n-  Explore data\n-  Prepare data for training and evaluation\n-  Create an XGBoost machine learning model\n-  Train and evaluate a model\n-  Use cross-validation to optimize model's hyperparameters\n-  Persist a model in Watson Machine Learning repository\n-  Deploy a model for online scoring\n-  Score sample data\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1. [Setup](#setup)\n2. [Load and explore the data](#load)\n3. [Create the XGBoost model](#model)\n4. [Persist model](#persistence)\n5. [Deployment](#deployment)\n6. [Score the model](#score)\n7. [Clean up](#cleanup)\n8. [Summary and next steps](#summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/admin/create-services.html?context=cpdaas&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\n\nAuthenticate the Watson Machine Learning service on IBM Cloud. You need to provide platform `api_key` and instance `location`.\n\nYou can use [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to retrieve platform API Key and instance location.\n\nAPI Key can be generated in the following way:\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n\nIn result, get the value of `api_key` from the output.\n\n\nLocation of your WML instance can be retrieved in the following way:\n```\nibmcloud login --apikey API_KEY -a https://cloud.ibm.com\nibmcloud resource service-instance WML_INSTANCE_NAME\n```\n\nIn result, get the value of `location` from the output."}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: Your `Cloud API key` can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below. You can also get a service specific url by going to the [**Endpoint URLs** section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning).  You can check your instance location in your  <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance details.\n\nYou can also get service specific apikey by going to the [**Service IDs** section of the Cloud Console](https://cloud.ibm.com/iam/serviceids).  From that page, click **Create**, then copy the created key and paste it below.\n\n**Action**: Enter your `api_key` and `location` in the following cell."}, {"metadata": {}, "cell_type": "code", "source": "api_key = 'PASTE YOUR PLATFORM API KEY HERE'\nlocation = 'PASTE YOUR INSTANCE LOCATION HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Install and import the `ibm-watson-machine-learning` package\n**Note:** `ibm-watson-machine-learning` documentation can be found <a href=\"http://ibm-wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Working with spaces\n\nFirst of all, you need to create a space that will be used for your work. If you do not have space already created, you can use [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces?context=cpdaas) to create one.\n\n- Click New Deployment Space\n- Create an empty space\n- Select Cloud Object Storage\n- Select Watson Machine Learning instance and press Create\n- Copy `space_id` and paste it below\n\n**Tip**: You can also use SDK to prepare the space for your work. More information can be found [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n\n**Action**: Assign space ID below"}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'PASTE YOUR SPACE ID HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can use `list` method to print all existing spaces."}, {"metadata": {}, "cell_type": "code", "source": "client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "To be able to interact with all resources available in Watson Machine Learning, you need to set **space** which you will be using."}, {"metadata": {}, "cell_type": "code", "source": "client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load\"></a>\n## 2. Load and explore the data"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will load the data as a numpy array and perform a basic exploration.\n\nTo load the data as a numpy array, user `wget` to download the data, then use the `genfromtxt` method to read the data."}, {"metadata": {}, "cell_type": "code", "source": "WisconsinDataSet = 'BreastCancerWisconsinDataSet.csv' ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!wget 'https://api.dataplatform.cloud.ibm.com/v2/gallery-assets/entries/c173693bf48aeb22e41bbe2b41d79c1f/data?accessKey=e20607d75c8473daaade1e77c210873f' \\\n    -O $WisconsinDataSet", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The csv file **BreastCancerWisconsinDataSet.csv** is downloaded. Run the code in the next cells to load the file to the numpy array."}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** Update `numpy` to ensure you have the latest version."}, {"metadata": {}, "cell_type": "code", "source": "# Run this code to upgrade numpy.\n!pip install numpy --upgrade", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\n\nnp_data = np.genfromtxt(WisconsinDataSet, delimiter=',', names=True, dtype=None, encoding='utf-8')\nprint(np_data[0])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Run the code in the next cell to view the feature names and data storage types."}, {"metadata": {}, "cell_type": "code", "source": "# Display the feature names and data storage types.\nprint(np_data.dtype)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Display the number of records and features.\nprint('Number of rows: {}'.format(np_data.size))\nprint('Number of columns: {}'.format(len(np_data[0])))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can see that the data set has 569 records and 32 features."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"model\"></a>\n## 3. Create an XGBoost model\n\nIn this section you will learn how to train and test an XGBoost model.\n\n- [3.1. Prepare the data](#prepare)\n- [3.2. Create the XGBoost model](#create)"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1. Prepare data<a id=\"prepare\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Now, you can prepare your data for model building. You will use the `diagnosis` column as your target variable so you must remove it from the set of predictors. You must also remove the `id` variable."}, {"metadata": {}, "cell_type": "code", "source": "y = 1.0*(np_data['diagnosis'] == 'M')\nX = np.array([list(r)[2:] for r in np_data])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Split the data set into: \n- Train data set\n- Test data set"}, {"metadata": {}, "cell_type": "code", "source": "# Split the data set and create two data sets.\n# from sklearn.cross_validation import train_test_split \n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=143)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# List the number of records in each data set.\nprint(\"Number of training records: \" + str(X_train.shape[0]))\nprint(\"Number of testing records : \" + str(X_test.shape[0]))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The data has been successfully split into two data sets:\n- The train data set, which is the largest group, will be used for training\n- The test data set will be used for model evaluation and is used to test the assumptions of the model"}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2. Create the XGBoost model<a id=\"create\"></a>"}, {"metadata": {}, "cell_type": "markdown", "source": "Start by importing the necessary libraries."}, {"metadata": {}, "cell_type": "code", "source": "# Import the libraries you need to create the XGBoost model.\nfrom xgboost.sklearn import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "#### 3.2.1. Create an XGBoost classifier"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you create an XGBoost classifier with default hyperparameter values and you will call it *xgb_model*. \n\n**Note** The next sections show you how to improve this base model."}, {"metadata": {}, "cell_type": "code", "source": "# Create the XGB classifier, xgb_model.\nxgb_model = XGBClassifier()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Display the default parameters for *xgb_model*."}, {"metadata": {}, "cell_type": "code", "source": "# List the default parameters.\nprint(xgb_model.get_xgb_params())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now that your XGBoost classifier, *xgb_model*, is set up, you can train it by invoking the fit method. You will also evaluate *xgb_model* while the train and test data are being trained."}, {"metadata": {}, "cell_type": "code", "source": "# Train and evaluate.\nxgb_model.fit(X_train, y_train, eval_metric=['error'], eval_set=[((X_train, y_train)),(X_test, y_test)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** You can also use a pandas dataFrame instead of the numpy array."}, {"metadata": {}, "cell_type": "markdown", "source": "Plot the model performance evaluated during the training process to assess model overfitting."}, {"metadata": {}, "cell_type": "code", "source": "# Import the library\nfrom matplotlib import pyplot\n%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Plot and display the performance evaluation\nxgb_eval = xgb_model.evals_result()\neval_steps = range(len(xgb_eval['validation_0']['error']))\n\nfig, ax = pyplot.subplots(1, 1, sharex=True, figsize=(8, 6))\n\nax.plot(eval_steps, [1-x for x in xgb_eval['validation_0']['error']], label='Train')\nax.plot(eval_steps, [1-x for x in xgb_eval['validation_1']['error']], label='Test')\nax.legend()\nax.set_title('Accuracy')\nax.set_xlabel('Number of iterations')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can see that there is model overfitting, and there is a decrease in model accuracy after about 60 iterations \n\nSelect the trained model obtained after 30 iterations."}, {"metadata": {}, "cell_type": "code", "source": "# Select trained model.\nn_trees = 30\ny_pred = xgb_model.predict(X_test, ntree_limit= n_trees)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Check the accuracy of the trained model.\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100.0))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** You will use the accuracy value obtained on the test data to compare the accuracy of the model with default parameters to the accuracy of the model with tuned parameters."}, {"metadata": {}, "cell_type": "markdown", "source": "#### 3.2.2. Use grid search and cross-validation to tune the model "}, {"metadata": {}, "cell_type": "markdown", "source": "You can use grid search and cross-validation to tune your model to achieve better accuracy.\n\nXGBoost has an extensive catalog of hyperparameters which provides great flexibility to shape an algorithm\u2019s desired behavior. Here you will the optimize the model tuning which adds an L1 penalty (`reg_alpha`).\n\nUse a 5-fold cross-validation because your training data set is small."}, {"metadata": {}, "cell_type": "markdown", "source": "In the cell below, create the XGBoost pipeline and set up the parameter grid for the search."}, {"metadata": {}, "cell_type": "code", "source": "# Create XGBoost pipeline, set up parameter grid.\nxgb_model_gs = XGBClassifier()\nparameters = {'reg_alpha': [0.0, 1.0], 'reg_lambda': [0.0, 1.0], 'n_estimators': [n_trees], 'seed': [1337]}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use ``GridSearchCV`` to search for the best parameters over the parameters values that were specified in the previous section."}, {"metadata": {}, "cell_type": "code", "source": "# Search for the best parameters.\nclf = GridSearchCV(xgb_model_gs, parameters, scoring='accuracy', cv=5, verbose=1, n_jobs=-1, refit=True)\nclf.fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "From the grid scores, you can see the performance result of all parameter combinations including the best parameter combination based on model performance."}, {"metadata": {}, "cell_type": "markdown", "source": "Display the accuracy estimated using cross-validation and the hyperparameter values for the best model."}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "print(\"Best score: %.1f%%\" % (clf.best_score_*100))\nprint(\"Best parameter set: %s\" % (clf.best_params_))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Display the accuracy of best parameter combination on the test set."}, {"metadata": {}, "cell_type": "code", "source": "y_pred = clf.best_estimator_.predict(X_test, ntree_limit= n_trees)\n\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100.0))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The accuracy on test set is about the same for tuned model as it is for the trained model that has default hyperparameters values, even though the selected hyperparameters are different to the default parameters."}, {"metadata": {}, "cell_type": "markdown", "source": "#### 3.2.3. Model with pipeline data preprocessing"}, {"metadata": {}, "cell_type": "markdown", "source": "Here you learn how to use the XGBoost model within the scikit-learn pipeline. \n\nLet's start by importing the required objects."}, {"metadata": {}, "cell_type": "code", "source": "from sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pca = PCA(n_components=10)\nxgb_model_pca = XGBClassifier(n_estimators=n_trees)\npipeline = Pipeline(steps=[('pca', pca), ('xgb', xgb_model_pca)])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "pipeline.fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now you are ready to evaluate accuracy of the model trained on the reduced set of features."}, {"metadata": {}, "cell_type": "code", "source": "y_pred = pipeline.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy: %.1f%%\" % (accuracy * 100.0))\npipeline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can see that this model has a similar accuracy to the model trained using default hyperparameter values."}, {"metadata": {}, "cell_type": "markdown", "source": "Let's see how you can save your XGBoost pipeline using the WML service instance and deploy it for online scoring."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"persistence\"></a>\n## 4. Persist model"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you learn how to use the Python client libraries to store your XGBoost model in the WML repository."}, {"metadata": {}, "cell_type": "markdown", "source": "### Save the XGBoost model to the WML Repository\n\nSave the model artifact as *XGBoost model for breast cancer* to your WML instance.\n\nGet software specification for XGBoost."}, {"metadata": {}, "cell_type": "code", "source": "software_spec_uid = client.software_specifications.get_uid_by_name(\"runtime-23.1-py3.10\")\nsoftware_spec_uid", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "metadata = {\n    client.repository.ModelMetaNames.NAME: \"XGBoost model for breast cancer\",\n    client.repository.ModelMetaNames.TYPE: \"scikit-learn_1.1\",\n    client.repository.ModelMetaNames.SOFTWARE_SPEC_UID : software_spec_uid,\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_details = client.repository.store_model(pipeline, metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Get the saved model metadata from WML."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"deployment\"></a>\n## 5. Deployment\nIn this section you will learn how to create batch deployment to create job using the Watson Machine Learning Client.\n\nYou can use commands bellow to create batch deployment for stored model (web service).\n\n### 5.1: Create model deployment"}, {"metadata": {}, "cell_type": "markdown", "source": "You need the model uid to create the deployment. You can extract the model uid from the saved model details."}, {"metadata": {}, "cell_type": "code", "source": "# Extract the uid.\nmodel_uid = client.repository.get_model_id(model_details)\nprint(model_uid)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Use this modul_uid in the next section to create the deployment."}, {"metadata": {}, "cell_type": "markdown", "source": "Now you can create a deployment, *Predict breast cancer*."}, {"metadata": {}, "cell_type": "code", "source": "# Create the deployment.\nmeta_props = {\n    client.deployments.ConfigurationMetaNames.NAME: \"Predict breast cancer'\",\n    client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n\ndeployment_details = client.deployments.create(model_uid,meta_props)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Get a list of all deployments."}, {"metadata": {}, "cell_type": "code", "source": "# List the deployments.\nclient.deployments.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "The *Predict breast cancer model* has been successfully deployed."}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.2 Get deployment details\n"}, {"metadata": {}, "cell_type": "markdown", "source": "To show deployments details, you need get deployment_uid."}, {"metadata": {}, "cell_type": "code", "source": "import json\n\ndeployment_uid = client.deployments.get_uid(deployment_details)\nprint(json.dumps(client.deployments.get_details(deployment_uid), indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "\n## 6. Score the model\nLet's see if our deployment works.\n"}, {"metadata": {}, "cell_type": "markdown", "source": "Now, extract the url endpoint, *scoring_url*, which will be used to send scoring requests."}, {"metadata": {}, "cell_type": "code", "source": "deployment_id = client.deployments.get_id(deployment_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Prepare the scoring payload with the values to score."}, {"metadata": {}, "cell_type": "code", "source": "# Prepare scoring payload.\npayload_scoring = {client.deployments.ScoringMetaNames.INPUT_DATA:\n    [\n        {\n        'values': [X_test[0].tolist()]\n        }\n   ]\n}\nprint(json.dumps(payload_scoring, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Perform prediction and display the result.\nresponse_scoring = client.deployments.score(deployment_id, payload_scoring)\nprint(json.dumps(response_scoring, indent=2))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Result**: The patient record is classified as a benign tumor."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"cleanup\"></a>\n## 7. Clean up"}, {"metadata": {}, "cell_type": "markdown", "source": "If you want to clean up all created assets:\n- experiments\n- trainings\n- pipelines\n- model definitions\n- models\n- functions\n- deployments\n\nplease follow up this sample [notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Machine%20Learning%20artifacts%20management.ipynb)."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## 8. Summary and next steps\n\nYou successfully completed this notebook! You learned how to use Keras machine learning library as well as Watson Machine Learning for model creation and deployment. Check out our _[Online Documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/getting-started/welcome-main.html?context=analytics?pos=2)_ for more samples, tutorials, documentation, how-tos, and blog posts. \n\n### Authors\n\n**Wojciech Jargielo**, Software Engineer\n\nCopyright \u00a9 2020 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.14", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "pycharm": {"stem_cell": {"cell_type": "raw", "metadata": {"collapsed": false}, "source": []}}}, "nbformat": 4, "nbformat_minor": 4}
