{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Calculate rolling averages on streaming data using Python and IBM Streams\n",
    "\n",
    "This sample demonstrates creating a Streams Python application to perform some analytics, and viewing the results.\n",
    "\n",
    "Familiarity with [Python](https://www.python.org/about/gettingstarted/) is recommended.\n",
    "\n",
    "\n",
    "In this notebook, you'll see examples of how to :\n",
    " 1. [Setup a connection to the Streams instance](#setup)\n",
    " 2. [Create the application](#create)\n",
    " 3. [Submit the application](#launch)\n",
    " 4. [Connect to the running application to view data](#view)\n",
    " 5. [Stop the application](#cancel)\n",
    "\n",
    "# Overview\n",
    "\n",
    "**About the sample**\n",
    "\n",
    "This application simulates a data hub that receives readings from sensors. It computes the 30 second rolling average of the reported readings using [Pandas](https://pandas.pydata.org/).  \n",
    "\n",
    "**How it works**\n",
    "   \n",
    "A Streams Python application processes a continuous and potentially infinite stream of data. The data is processed in memory and is not stored in a database first.\n",
    "\n",
    "The Python application created in this notebook is submitted to the IBM Streams service for execution. Once the application is running in the service, you can connect to it from the notebook to continuously retrieve the results.\n",
    "\n",
    "<img src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2019/04/how-it-works.jpg\" alt=\"How it works\">\n",
    "\n",
    "\n",
    "### Documentation\n",
    "\n",
    "- [Streams Python development guide](https://ibmstreams.github.io/streamsx.documentation/docs/latest/python/)\n",
    "- [Streams Python API](https://streamsxtopology.readthedocs.io/)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "This notebook can be used as-is from within an IBM Cloud Pak for Data project. \n",
    "\n",
    "If you are not running this notebook from within IBM Cloud Pak for Data, [follow these steps to make sure you have installed all the prerequisites](https://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-2/).\n",
    "\n",
    "<a name=\"setup\"></a>\n",
    "# 1. Set up a connection to the Streams instance\n",
    "\n",
    "\n",
    "To submit the application for execution, you have to connect to the Streams instance. The information required to connect to the instance depends on the target installation of Streams. \n",
    "\n",
    "Choose the option that matches your development environment.\n",
    "\n",
    "- **Option 1**: [I'm running the notebook from an IBM Cloud for Data project](#cpd)\n",
    "- **Option 2**: [I'm using IBM Watson Studio, Jupyter Notebooks, or any other development environment](#notcpd)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cpd\"></a>\n",
    "### Option 1: Connect to a Streams instance from an IBM Cloud Pak for Data  project\n",
    "\n",
    "In order to submit a Streams application you need to provide the name of the Streams instance.\n",
    "\n",
    "1. From the navigation menu, click **My instances**.\n",
    "2. Click the **Provisioned Instances** tab.\n",
    "3. Update the value of `streams_instance_name` in the cell below according to your Streams instance name\n",
    "4. Run the cell and skip to section 1.2\n",
    "\n",
    "The cell below defines a function called `submit_topology` that will be used later on to submit the `Topology` once it is defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from icpd_core import icpd_util\n",
    "from streamsx.topology import context\n",
    "\n",
    "streams_instance_name = \"sample-streams\" ## Change this to Streams instance\n",
    "\n",
    "try:\n",
    "    cfg=icpd_util.get_service_instance_details(name=streams_instance_name, instance_type=\"streams\")\n",
    "except TypeError:\n",
    "    cfg=icpd_util.get_service_instance_details(name=streams_instance_name)\n",
    "\n",
    "def submit_topology(topo):\n",
    "    global cfg\n",
    "    # Disable SSL certificate verification if necessary\n",
    "    cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "    # Topology wil be deployed as a distributed app\n",
    "    contextType = context.ContextTypes.DISTRIBUTED\n",
    "    return context.submit (contextType, topo, config = cfg)\n",
    "\n",
    "print(\"Setup complete, continue to section 1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"notcpd\"></a>\n",
    "### Option 2: Connect to a Streams instance from IBM Watson Studio and other environments\n",
    "\n",
    "*Skip this section if you are running the notebook from a Cloud Pak for Data project.*\n",
    "\n",
    "The code for each scenario is available in the development guide.  \n",
    "Each snippet will define a function called `submit_topology` that will be used later on to submit the `Topology` once it is defined.\n",
    "\n",
    "- Choose the tab that best matches your environment. \n",
    "- Copy the code under the heading **Copy this code snippet**.\n",
    "- Paste it in the cell below.\n",
    "\n",
    "    <a target=\"_blank\" href=\"https://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-2/#connect\">Connection instructions from the development guide</a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Analytics credentials:········\n",
      "Saved credentials, continue to section 1.2\n"
     ]
    }
   ],
   "source": [
    "## This code snippet is applicable only to Streaming Analytics service on Watson Studio\n",
    "from streamsx.topology.context import ConfigParams\n",
    "from streamsx.topology import context\n",
    "import json\n",
    "import getpass\n",
    "\n",
    "\n",
    "cfg  = {}\n",
    "SA_credentials=getpass.getpass('Streaming Analytics credentials:')\n",
    "cfg[ConfigParams.SERVICE_DEFINITION] = json.loads(SA_credentials)\n",
    "cfg[context.ConfigParams.SSL_VERIFY] = False\n",
    "print(\"Saved credentials, continue to section 1.2\")\n",
    "def submit_topology(topo):\n",
    "    global cfg\n",
    "    # This specifies how the application will be deployed\n",
    "    contextType = context.ContextTypes.STREAMING_ANALYTICS_SERVICE\n",
    "\n",
    "    return context.submit (contextType, topo, config = cfg) \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Verify `streamsx` package version\n",
    "\n",
    "Run the cell below to check which version of the `streamsx` package is installed.  \n",
    "\n",
    "If you need to upgrade, use\n",
    "\n",
    "- `import sys`\n",
    "- `!{sys.executable} -m pip install --user --upgrade streamsx` to upgrade the package.\n",
    "- Or, use  `!{sys.executable} -m pip install --user streamsx==somever` to install a specific version of the package. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: streamsx package version: 1.13.14\n",
      "Name: streamsx\n",
      "Version: 1.13.14\n",
      "Summary: IBM Streams Python Support\n",
      "Home-page: https://github.com/IBMStreams/pypi.streamsx\n",
      "Author: IBM Streams @ github.com\n",
      "Author-email: debrunne@us.ibm.com\n",
      "License: Apache License - Version 2.0\n",
      "Location: /opt/conda/envs/Python36/lib/python3.6/site-packages\n",
      "Requires: future, dill, requests\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "import streamsx.topology.context\n",
    "print(\"INFO: streamsx package version: \" + streamsx.topology.context.__version__)\n",
    "\n",
    "#For more details uncomment line below.\n",
    "#!pip show streamsx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"create\"></a>\n",
    "# 2. Create the application\n",
    " \n",
    "\n",
    "All Streams applications start with  a `Topology` object, so start by creating one:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from streamsx.topology.topology import Topology\n",
    "\n",
    "topo = Topology(name=\"SensorAverages\",namespace=\"demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define sources\n",
    "Your application needs some data to analyze, so the first step is to define a data source that produces the data to be analyzed. \n",
    "\n",
    "Next, use the data source to create a `Stream` object. A `Stream` is a potentially infinite sequence of tuples containing the data to be analyzed.\n",
    "\n",
    "Tuples are Python objects by default. Other supported formats include JSON. [See the doc for all supported formats](https://streamsxtopology.readthedocs.io/en/stable/streamsx.topology.topology.html#stream)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Define a source class\n",
    "\n",
    "Define a callable class that will produce the data to be analyzed.\n",
    "\n",
    "This example class simulates readings from sensors. Each reading is a Python `dict` containing the sensor id, the reported value, and the timestamp of the reading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define a callable source \n",
    "class SensorReadingsSource(object):\n",
    "    def __call__(self):\n",
    "        # This is just an example of using generated data, \n",
    "        # Here you could connect to db\n",
    "        # generate data\n",
    "        # connect to data set\n",
    "        # open file\n",
    "        while True:\n",
    "            time.sleep(0.001)\n",
    "            sensor_id = random.randint(1,100)\n",
    "            reading = {}\n",
    "            reading [\"sensor_id\"] = \"sensor_\" + str(sensor_id)\n",
    "            reading [\"value\"] =  random.random() * 3000\n",
    "            reading[\"ts\"] = int((datetime.now().timestamp())) \n",
    "            yield reading "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2  Create the `Stream `\n",
    "\n",
    "Create a `Stream` called  `readings` that will contain the simulated data that `SensorReadingsSource` produces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a stream from the data using Topology.source\n",
    "readings = topo.source(SensorReadingsSource(), name=\"Readings\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Analyze data\n",
    "\n",
    "Use a variety of methods in the `Stream` class to analyze your in-flight data, including applying machine learning models.\n",
    " \n",
    "This section will:\n",
    "- Filter out tuples based on a condition,\n",
    "- Compute the rolling average, and\n",
    "- Enrich the rolling average with data from another source.\n",
    "\n",
    "Built-in methods exist for common operations, such as <code>Stream.filter</code> and <code>Stream.split</code>, which filter or split a stream of data respectively.\n",
    "\n",
    "See the <a href=\"/streamsx.documentation/docs/python/1.6/python-appapi-devguide-4/\"> common operations section</a> for other common examples. Check out the <a href=\"https://ibmstreams.github.io/streamsx.topology/doc/pythondoc/streamsx.topology.topology.html#streamsx.topology.topology.Stream\">documentation </a> of the <code>Stream</code> class for full list of functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Filter data from the  `Stream`  \n",
    "\n",
    "Use `Stream.filter()` to remove data that doesn't match a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept only values greater than 100\n",
    "\n",
    "valid_readings = readings.filter(lambda x : x[\"value\"] > 100,\n",
    "                                 name=\"ValidReadings\")\n",
    "\n",
    "# You could create another stream of the invalid data:\n",
    "# invalid_readings = readings.filter(lambda x : x[\"value\"] <= 100,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2  Compute averages on the  `Stream`  \n",
    "\n",
    "Define a function to compute the 30 second rolling average for the readings.\n",
    "\n",
    "Steps are outlined in the code below.\n",
    "See the [Window class documentation](https://streamsxtopology.readthedocs.io/en/stable/streamsx.topology.topology.html#streamsx.topology.topology.Window)  for details.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define aggregation function\n",
    "    \n",
    "def average_reading(items_in_window):\n",
    "    df = pd.DataFrame(items_in_window)\n",
    "    readings_by_id = df.groupby(\"sensor_id\")\n",
    "    \n",
    "    averages = readings_by_id[\"value\"].mean()\n",
    "    period_end = df[\"ts\"].max()\n",
    "\n",
    "    result = []\n",
    "    for id, avg in averages.iteritems():\n",
    "        result.append({\"average\": avg,\n",
    "                \"sensor_id\": id,\n",
    "                \"period_end\": time.ctime(period_end)})\n",
    "               \n",
    "    return result\n",
    "\n",
    "# 2. Define window: e.g. a 30 second rolling average, updated every second\n",
    "\n",
    "interval = timedelta(seconds=30)\n",
    "window = valid_readings.last(size=interval).trigger(when=timedelta(seconds=1))\n",
    "\n",
    "\n",
    "# 3. Pass aggregation function to Window.aggregate\n",
    "# average_reading returns a list of the averages for each sensor,\n",
    "# use flat map to convert it to individual tuples, one per sensor\n",
    "rolling_average = window.aggregate(average_reading).flat_map()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2.2.3 Enrich the data on the `Stream`\n",
    "\n",
    "Each tuple on the `rolling_average Stream` will have the following format: \n",
    "\n",
    "`{'average': 1655.1009278955999, 'sensor_id': 'sensor_17', 'period_end': 'Tue Nov 19 22:07:02 2019'}\n",
    "`. (See the `average_reading` function above).\n",
    "\n",
    "Imagine that you want to add the geographical coordinates of the sensor to each tuple. This information might come from a different data source, such as a database.\n",
    "\n",
    "Use `Stream.map()`. The `map` transfrom uses a function you provide to convert each tuple on the `Stream` into a new tuple. \n",
    "\n",
    "In our case, for each tuple on the `rolling_average Stream`,  we will update it to include the geographical coordinates of the sensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify this tuple with the coordinates of the sensor\n",
    "# Returns the original tuple with a new `coords` attribute\n",
    "# representing the latitude and longitude of the sensor\n",
    "def enrich(tpl):\n",
    "    # use simulated data, but you could make a database call, \n",
    "    lat = round(random.random() + 39.8338515, 4)\n",
    "    lon = round(-74.871826 + random.random(), 4)\n",
    "    # update the tuple with new data\n",
    "    tpl[\"coords\"] = (lat, lon)\n",
    "    return tpl\n",
    "\n",
    "# Update the data on the rolling_average stream with the map transform\n",
    "enriched_average = rolling_average.map(enrich)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Create a `View` to preview the tuples on the `Stream` \n",
    "\n",
    "\n",
    "A `View` is a connection to a `Stream` that becomes activated when the application is running. The connection allows you to access the data on the `Stream` as it is being processed.\n",
    "\n",
    "\n",
    "After submitting the `Topology`, we use a `View`  to examine the from within the notebook [in section 4](#view).\n",
    "\n",
    "To view the data on the `enriched_average Stream`, define a `View` using `Stream.view()`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages_view = enriched_average.view(name=\"RollingAverage\", \n",
    "                                      description=\"Sample of rolling averages for each sensor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can <a href=\"http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide-6/#accessing-the-tuples-of-a-view\">connect to a view in <em>any</em> running Streams job using the REST API</a> , regardless of what language was used to create the application.\n",
    "\n",
    "# 2.4 Define output\n",
    "\n",
    "You could also enable a microservices based architecture by publishing the results so that other Streams applications can connect to it.\n",
    "\n",
    "Use `Stream.publish()` to make the `enriched_average Stream` available to other applications. \n",
    "\n",
    "To send the stream to another database or system, use a sink function (similar to the source function) and invoke it using `Stream.for_each`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<streamsx.topology.topology.Sink at 0x7f2be124cdd8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "# publish results as JSON\n",
    "enriched_average.publish(topic=\"AverageReadings\",\n",
    "                        schema=json, \n",
    "                        name=\"PublishAverage\")\n",
    "\n",
    "# Other options include:\n",
    "# invoke another sink function:\n",
    "# rolling_average.for_each(func=send_to_db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"launch\"></a>\n",
    "\n",
    "# 3. Submit the application\n",
    "A running Streams application is called a *job*. Use this cell to submit the `Topology` for execution, using the `submit_topology` function [defined in step 1](#setup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitting Topology to Streams for execution..\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "773a5e8d64d3425b82bbe258e1b726a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntProgress(value=0, bar_style='info', description='Initializing', max=10, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobId:  0 \n",
      "Job name:  demo::SensorAverages_0\n"
     ]
    }
   ],
   "source": [
    "# The submission_result object contains information about the running application, or job\n",
    "print(\"Submitting Topology to Streams for execution..\")\n",
    "submission_result = submit_topology(topo)\n",
    "\n",
    "if submission_result.job:\n",
    "  streams_job = submission_result.job\n",
    "  print (\"JobId: \", streams_job.id , \"\\nJob name: \", streams_job.name)\n",
    "else:\n",
    "  print(\"Submission failed: \"   + str(submission_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"view\"></a>\n",
    "\n",
    "# 4. Use a `View` to access data from the job\n",
    "\n",
    "Now that the job is started, use the `averages_view` object you created in step 2.3 to start retrieving data from the `enriched_average Stream`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching view data ...\n",
      "{'average': 1506.6545288633777, 'sensor_id': 'sensor_1', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.3664, -74.8488]}\n",
      "{'average': 1448.4884753156052, 'sensor_id': 'sensor_10', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.7354, -74.0566]}\n",
      "{'average': 1371.947140819208, 'sensor_id': 'sensor_100', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.6778, -74.4536]}\n",
      "{'average': 1687.6960889456923, 'sensor_id': 'sensor_11', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.4391, -74.4675]}\n",
      "{'average': 1615.8352964381731, 'sensor_id': 'sensor_12', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.2493, -73.9043]}\n",
      "{'average': 1470.34668409349, 'sensor_id': 'sensor_13', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.5989, -74.2169]}\n",
      "{'average': 1564.1658840745379, 'sensor_id': 'sensor_14', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.3127, -74.5819]}\n",
      "{'average': 1431.769733733516, 'sensor_id': 'sensor_15', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [40.7591, -74.404]}\n",
      "{'average': 1623.0778062436768, 'sensor_id': 'sensor_16', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [39.8671, -74.1024]}\n",
      "{'average': 1625.3272862193953, 'sensor_id': 'sensor_17', 'period_end': 'Fri May 29 18:38:37 2020', 'coords': [39.9366, -74.1596]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Fetching view data ...\")\n",
    "# Connect to the view and display the data\n",
    "queue = averages_view.start_data_fetch()\n",
    "try:\n",
    "    for val in range(10):\n",
    "        print(queue.get())    \n",
    "finally:\n",
    "    averages_view.stop_data_fetch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Display the results in real time\n",
    "Calling `View.display()` from the notebook displays the results of the view in a table that is updated in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9663ab1444b14537b0108b38f274ff3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Text(value='Sample of rolling averages for each sensor', description='RollingAve…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the results for 30 seconds\n",
    "averages_view.display(duration=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4.2 See job status \n",
    "\n",
    "In IBM Cloud Pak for Data, you can view job status and logs with the Job Graph.\n",
    "\n",
    "To view job status and logs:\n",
    "<ol>\n",
    "<li>From the main menu, go to <b>My Instances &gt; Jobs</b>. </li>\n",
    "<li>Find your job based on the <code>JobId</code> printed when you submitted the topology.</li>\n",
    "<li>Select <b>View graph</b> from the context menu action for the running job.</li>\n",
    "</ol>\n",
    "\n",
    "For all other development environments, use the Streams Console.\n",
    "\n",
    "[See instructions and an example](http://ibm.biz/Bdz6yD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"cancel\"></a>\n",
    "\n",
    "# 5. Cancel the job\n",
    "Streams jobs run indefinitely, so make sure you cancel the job once you are finished running the sample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell generates a widget you can use to cancel the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cancel the job in the IBM Streams service\n",
    "submission_result.cancel_job_button()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also interact with the job through the [Job](https://streamsxtopology.readthedocs.io/en/stable/streamsx.rest_primitives.html#streamsx.rest_primitives.Job) object returned from `submission_result.job`\n",
    "\n",
    "For example, use `job.cancel()` to cancel the running job directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "We started with a `Stream` called `readings`, which contained the data we wanted to analyze. Next, we used functions in the `Stream` object to perform simple analysis and produced the `enriched_average` stream.  This stream is `published` for other applications running within our Streams instance to access.\n",
    "\n",
    "After submitting the application to the Streams service, we used the `enriched_average` view to see the results.\n",
    "\n",
    "\n",
    "## Authors\n",
    "\n",
    "**Dan Debrunner** is a software architect at IBM focusing on streaming analytics.\n",
    "\n",
    "**Natasha D'Silva**  is a software developer at IBM working with streaming analytics.\n",
    "\n",
    "\n",
    "# Learn more \n",
    "\n",
    "- **Find more samples**: This notebook is one of several sample notebooks available in the [starter notebooks repository on GitHub](https://github.com/IBMStreams/sample.starter_notebooks). Visit the repository for examples of how to connect to common data sources, including Apache Kafka, IBM, and Db2 Warehouse. \n",
    "\n",
    "\n",
    "- Learn more about how to use the API from the [development guide](http://ibmstreams.github.io/streamsx.documentation/docs/python/1.6/python-appapi-devguide/).\n",
    "<br><br>\n",
    "Copyright © 2019 IBM. This notebook and its source code are released under the terms of the MIT License.\n",
    "<br><br>\n",
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
