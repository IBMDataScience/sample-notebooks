{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"4\" color=\"black\"><b>Use Spark ML and Scala to detect network intrusions</b></font></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr> \n",
    "   <tr style=\"border: none\">\n",
    "       <td style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/raw/master/tensorflow/hand-written-digit-recognition/images/experiment_banner.png\" width=\"600\" height = \"200\" alt=\"Icon\"></td>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows you how to easily build two classification models using the Spark Machine Learning (ML) library to detect network intrusions. It uses the Random Forest (RF) classifier and the Multilayer Perceptron (MLP) classifier to build the required algorithms.\n",
    "\n",
    "<a href=\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\" target=\"_blank\" rel=\"noopener noreferrer\">UCI kddcup data</a> (743MB) is used in this notebook. This data set can be audited and provides intrusions simulated in a military network environment. It was originally used for the **The Third International Knowledge Discovery and Data Mining Tools Competition** organized for **KDD-99**. \n",
    "\n",
    "\n",
    "This notebook runs on Scala and Spark and it was tested with Watson Studio Spark Environments.\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Download data](#download)<br>\n",
    "2. [Load and prepare data](#load)<br>\n",
    "3. [Build the models](#build)<br>\n",
    "    3.1 [Set up the Random Forest model](#rf)<br>\n",
    "    3.2 [Set up the Multilayer Perceptron model](#mlp)<br>\n",
    "4.  [Summary and next steps](#summary)  \n",
    "\n",
    "\n",
    "<a id=\"download\"></a>\n",
    "## 1. Download data\n",
    "\n",
    "First, download the prerequisite data set from Watson Studio using the following url: <a href=\"https://dataplatform.ibm.com/exchange-api/v1/entries/1438a61212a64ac435c837ba046efc19/data?accessKey=903188bb984a30f38bb889102a7db39f\" target=\"_blank\" rel=\"noopener noreferrer\">https://dataplatform.ibm.com/exchange-api/v1/entries/1438a61212a64ac435c837ba046efc19/data?accessKey=903188bb984a30f38bb889102a7db39f</a> \n",
    "\n",
    "Assign this URL to the variable `url`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys.process._\n",
    "\n",
    "val url = \"LINK-TO-DATA-SET-URL\"\n",
    "val filename = \"./kddcup.zip\"\n",
    "s\"wget $url -O $filename\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ```kddcup``` directory and **unzip** the file that you downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"mkdir ./kddcup\".!\n",
    "\"unzip ./kddcup.zip -d ./kddcup/\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the content of the unzipped file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"ls ./kddcup/\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the entire data set ```kddcup.data``` (743 MB) run **gunzip** to unzip the file to the same directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"gunzip ./kddcup/kddcup.data.gz -d ./kddcup/kddcup.data\".!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load and prepare data\n",
    "You can use the ```SparkSession``` to read the data directly into a dataframe because the data is provided in CSV (comma-separated values) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|_c0|_c1| _c2|_c3|_c4|  _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|   _c41|\n",
      "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|  0|tcp|http| SF|215|45076|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   0|   0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|162| 4528|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   1|   1| 1.0| 0.0| 1.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|236| 1228|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   2|   2| 1.0| 0.0| 0.5| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|233| 2032|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   3|   3| 1.0| 0.0|0.33| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|239|  486|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   3|   3| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   4|   4| 1.0| 0.0|0.25| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@2510b661\n",
       "df = [_c0: int, _c1: string ... 40 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, _c1: string ... 40 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "val spark = SparkSession.\n",
    "    builder().\n",
    "    getOrCreate()\n",
    "val df = spark.\n",
    "    read.format(\"csv\").\n",
    "    option(\"inferSchema\", \"true\").\n",
    "    load(\"./kddcup/kddcup.data\")\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the schema and labels of the last column ```_c41```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: double (nullable = true)\n",
      " |-- _c26: double (nullable = true)\n",
      " |-- _c27: double (nullable = true)\n",
      " |-- _c28: double (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: double (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: double (nullable = true)\n",
      " |-- _c34: double (nullable = true)\n",
      " |-- _c35: double (nullable = true)\n",
      " |-- _c36: double (nullable = true)\n",
      " |-- _c37: double (nullable = true)\n",
      " |-- _c38: double (nullable = true)\n",
      " |-- _c39: double (nullable = true)\n",
      " |-- _c40: double (nullable = true)\n",
      " |-- _c41: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|            _c41|  count|\n",
      "+----------------+-------+\n",
      "|    warezmaster.|     20|\n",
      "|          smurf.|2807886|\n",
      "|            pod.|    264|\n",
      "|           imap.|     12|\n",
      "|           nmap.|   2316|\n",
      "|   guess_passwd.|     53|\n",
      "|        ipsweep.|  12481|\n",
      "|      portsweep.|  10413|\n",
      "|          satan.|  15892|\n",
      "|           land.|     21|\n",
      "|     loadmodule.|      9|\n",
      "|      ftp_write.|      8|\n",
      "|buffer_overflow.|     30|\n",
      "|        rootkit.|     10|\n",
      "|    warezclient.|   1020|\n",
      "|       teardrop.|    979|\n",
      "|           perl.|      3|\n",
      "|            phf.|      4|\n",
      "|       multihop.|      7|\n",
      "|        neptune.|1072017|\n",
      "+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"_c41\").groupBy(\"_c41\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the <a href=\"http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types\" target=\"_blank\" rel=\"noopener noreferrer\">description</a>, the labels should be recoded into five categories using an SQL query. The new column name ```label_s``` stands for *label in string*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|label_s|  count|\n",
      "+-------+-------+\n",
      "|    u2r|     52|\n",
      "| normal| 972781|\n",
      "|    r2l|   1126|\n",
      "|  probe|  41102|\n",
      "|    dos|3883370|\n",
      "+-------+-------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "query = \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "        WHEN 'warez...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SELECT *, \n",
       "    CASE _c41 \n",
       "        WHEN 'back.' THEN 'dos'\n",
       "        WHEN 'buffer_overflow.' THEN 'u2r'\n",
       "        WHEN 'ftp_write.' THEN 'r2l'\n",
       "        WHEN 'guess_passwd.' THEN 'r2l'\n",
       "        WHEN 'imap.' THEN 'r2l'\n",
       "        WHEN 'ipsweep.' THEN 'probe'\n",
       "        WHEN 'land.' THEN 'dos'\n",
       "        WHEN 'loadmodule.' THEN 'u2r'\n",
       "        WHEN 'multihop.' THEN 'r2l'\n",
       "        WHEN 'neptune.' THEN 'dos'\n",
       "        WHEN 'nmap.' THEN 'probe'\n",
       "        WHEN 'perl.' THEN 'u2r'\n",
       "        WHEN 'phf.' THEN 'r2l'\n",
       "        WHEN 'pod.' THEN 'dos'\n",
       "        WHEN 'portsweep.' THEN 'probe'\n",
       "        WHEN 'rootkit.' THEN 'u2r'\n",
       "        WHEN 'satan.' THEN 'probe'\n",
       "        WHEN 'smurf.' THEN 'dos'\n",
       "        WHEN 'spy.' THEN 'r2l'\n",
       "        WHEN 'teardrop.' THEN 'dos'\n",
       "        WHEN 'warezclient.' THEN 'r2l'\n",
       "        WHEN 'warezmaster.' THEN 'r2l'\n",
       "        ELSE 'normal'\n",
       "END AS label_s \n",
       "FROM attack"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"attack\")\n",
    "\n",
    "val query = \"\"\"SELECT *, \n",
    "    CASE _c41 \n",
    "        WHEN 'back.' THEN 'dos'\n",
    "        WHEN 'buffer_overflow.' THEN 'u2r'\n",
    "        WHEN 'ftp_write.' THEN 'r2l'\n",
    "        WHEN 'guess_passwd.' THEN 'r2l'\n",
    "        WHEN 'imap.' THEN 'r2l'\n",
    "        WHEN 'ipsweep.' THEN 'probe'\n",
    "        WHEN 'land.' THEN 'dos'\n",
    "        WHEN 'loadmodule.' THEN 'u2r'\n",
    "        WHEN 'multihop.' THEN 'r2l'\n",
    "        WHEN 'neptune.' THEN 'dos'\n",
    "        WHEN 'nmap.' THEN 'probe'\n",
    "        WHEN 'perl.' THEN 'u2r'\n",
    "        WHEN 'phf.' THEN 'r2l'\n",
    "        WHEN 'pod.' THEN 'dos'\n",
    "        WHEN 'portsweep.' THEN 'probe'\n",
    "        WHEN 'rootkit.' THEN 'u2r'\n",
    "        WHEN 'satan.' THEN 'probe'\n",
    "        WHEN 'smurf.' THEN 'dos'\n",
    "        WHEN 'spy.' THEN 'r2l'\n",
    "        WHEN 'teardrop.' THEN 'dos'\n",
    "        WHEN 'warezclient.' THEN 'r2l'\n",
    "        WHEN 'warezmaster.' THEN 'r2l'\n",
    "        ELSE 'normal'\n",
    "END AS label_s \n",
    "FROM attack\"\"\"\n",
    "val labeled = spark.sql(query)\n",
    "labeled.select(\"label_s\").groupBy(\"label_s\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a pipeline to prepare the data before building the models.\n",
    "\n",
    "**Data preparation pipeline:**\n",
    "\n",
    "*    StringIndexers: c1, c2, and c3 are categorical strings. They must be indexed first.\n",
    "*    OneHotEncoders: When the categorical strings have been indexed, you can use one-hot encoding to the indexed columns.\n",
    "*    VectorAssembler: Include the wanted columns and assemble them as a feature vector.\n",
    "*    labelIndexer: Another StringIndexer is used to index the label_s column to output it as label column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "indexer1 = strIdx_a7eb7d249c8b\n",
       "indexer2 = strIdx_bae7a99525b3\n",
       "indexer3 = strIdx_d2290e2b7896\n",
       "encoder1 = oneHot_0ba66b85978c\n",
       "encoder2 = oneHot_2cc465e87014\n",
       "encoder3 = oneHot_7587f352e24a\n",
       "featurenames = Array(_c0, v_c1, v_c2, v_c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: there were three deprecation warnings; re-run with -deprecation for details\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0, v_c1, v_c2, v_c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33, _c34, _c35, _c36, _c37, _c38, _c39, _c40]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, OneHotEncoder}\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val indexer1 = new StringIndexer()\n",
    "  .setInputCol(\"_c1\")\n",
    "  .setOutputCol(\"i_c1\")\n",
    "//   .setHandleInvalid(\"skip\")\n",
    "\n",
    "val indexer2 = new StringIndexer()\n",
    "  .setInputCol(\"_c2\")\n",
    "  .setOutputCol(\"i_c2\")\n",
    "//   .setHandleInvalid(\"skip\")\n",
    "\n",
    "val indexer3 = new StringIndexer()\n",
    "  .setInputCol(\"_c3\")\n",
    "  .setOutputCol(\"i_c3\")\n",
    "//   .setHandleInvalid(\"skip\")\n",
    "\n",
    "val encoder1 = new OneHotEncoder()\n",
    "  .setInputCol(\"i_c1\")\n",
    "  .setOutputCol(\"v_c1\")\n",
    "\n",
    "val encoder2 = new OneHotEncoder()\n",
    "  .setInputCol(\"i_c2\")\n",
    "  .setOutputCol(\"v_c2\")\n",
    "\n",
    "val encoder3 = new OneHotEncoder()\n",
    "  .setInputCol(\"i_c3\")\n",
    "  .setOutputCol(\"v_c3\")\n",
    "\n",
    "val featurenames = Array(\"_c0\", \"v_c1\", \"v_c2\", \"v_c3\", \"_c4\", \"_c5\", \"_c6\", \n",
    "                         \"_c7\", \"_c8\", \"_c9\", \"_c10\", \"_c11\", \"_c12\", \"_c13\", \n",
    "                         \"_c14\", \"_c15\", \"_c16\", \"_c17\", \"_c18\", \"_c19\",\n",
    "                         \"_c22\", \"_c23\", \"_c24\", \"_c25\", \"_c26\", \"_c27\", \n",
    "                         \"_c28\", \"_c29\", \"_c30\", \"_c31\", \"_c32\", \"_c33\", \"_c34\", \n",
    "                         \"_c35\", \"_c36\", \"_c37\", \"_c38\", \"_c39\", \"_c40\")\n",
    "val assembler = new VectorAssembler()\n",
    "  .setInputCols(featurenames)\n",
    "  .setOutputCol(\"features\")\n",
    "\n",
    "val labelIndexer = new StringIndexer()\n",
    "  .setInputCol(\"label_s\")\n",
    "  .setOutputCol(\"label\")\n",
    "\n",
    "val pipeline_prepare = new Pipeline()\n",
    "  .setStages(Array(indexer1,indexer2,indexer3,encoder1,encoder2,encoder3,assembler,labelIndexer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now fit and transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [_c0: int, _c1: string ... 49 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, _c1: string ... 49 more fields]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = pipeline_prepare.fit(labeled).transform(labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "## 3. Build the models\n",
    "This section describes how to build the models.\n",
    "Because of the large amount of data, we can use 60/40 split to mitigate overfitting:\n",
    "* 60% for the ```training``` set\n",
    "* 40% for the ```testing``` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train = [_c0: int, _c1: string ... 49 more fields]\n",
       "test = [_c0: int, _c1: string ... 49 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[_c0: int, _c1: string ... 49 more fields]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val Array(train, test) = data.randomSplit(Array(0.6, 0.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rf\"></a>\n",
    "### 3.1 Set up the Random Forest model\n",
    "\n",
    "As the Random Forest (RF) algorithm is provided by Spark ML, you only have to set it up. Train and fit the model to the training data.\n",
    "\n",
    "**Note:** There are 70 categories in the ```c2``` column. This is larger than the default of ```_MaxBins_```. To avoid errors ```_MaxBins_``` is set to 72, because it has to be larger than the biggest number of categories of all categorical variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training process takes 124.641922243 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t = 4439303327041089\n",
       "rf = rfc_a37b39f56d8b\n",
       "rf_model = RandomForestClassificationModel (uid=rfc_a37b39f56d8b) with 5 trees\n",
       "duration = 124.641922243\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "124.641922243"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\n",
    "val t = System.nanoTime\n",
    "val rf = new RandomForestClassifier()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setNumTrees(5)\n",
    "  .setMaxBins(72)\n",
    "\n",
    "val rf_model = rf.fit(train)\n",
    "val duration = (System.nanoTime - t) / 1e9d\n",
    "println(s\"Training process takes $duration secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about the random forest classification model can be printed and will look something like the following:\n",
    "```\n",
    "RandomForestClassificationModel (uid=rfc_a37b39f56d8b) with 5 trees\n",
    "  Tree 0 (weight 1.0):\n",
    "    If (feature 1 in {0.0})\n",
    "     If (feature 104 <= 0.325)\n",
    "      If (feature 116 <= 0.005)\n",
    "       If (feature 100 <= 0.375)\n",
    "        If (feature 113 <= 0.025)\n",
    "         Predict: 1.0\n",
    "        Else (feature 113 > 0.025)\n",
    "         Predict: 2.0\n",
    "       Else (feature 100 > 0.375)\n",
    "        If (feature 110 <= 0.14500000000000002)\n",
    "         Predict: 0.0\n",
    "        Else (feature 110 > 0.14500000000000002)\n",
    "         Predict: 0.0\n",
    "      Else (feature 116 > 0.005)\n",
    "       If (feature 0 <= 0.5)\n",
    "        If (feature 105 <= 0.41500000000000004)\n",
    "         Predict: 0.0\n",
    "        Else (feature 105 > 0.41500000000000004)\n",
    "         Predict: 2.0\n",
    "       Else (feature 0 > 0.5)\n",
    "        If (feature 110 <= 0.025)\n",
    "         Predict: 1.0\n",
    "        Else (feature 110 > 0.025)\n",
    "         Predict: 2.0\n",
    "     Else (feature 104 > 0.325)\n",
    "      If (feature 73 in {0.0})\n",
    "       If (feature 90 <= 0.5)\n",
    "        If (feature 83 <= 17.0)\n",
    "         Predict: 1.0\n",
    "        Else (feature 83 > 17.0)\n",
    "         Predict: 1.0\n",
    "       Else (feature 90 > 0.5)\n",
    "        If (feature 87 <= 1.5)\n",
    "         Predict: 1.0\n",
    "        Else (feature 87 > 1.5)\n",
    "         Predict: 0.0\n",
    "      Else (feature 73 not in {0.0})\n",
    "       If (feature 108 <= 143.5)\n",
    "        If (feature 111 <= 0.095)\n",
    "         Predict: 0.0\n",
    "        Else (feature 111 > 0.095)\n",
    "         Predict: 0.0\n",
    "       Else (feature 108 > 143.5)\n",
    "        If (feature 110 <= 0.045)\n",
    "         Predict: 1.0\n",
    "        Else (feature 110 > 0.045)\n",
    "         Predict: 0.0\n",
    "    Else (feature 1 not in {0.0})\n",
    "     If (feature 100 <= 0.01)\n",
    "      If (feature 111 <= 0.245)\n",
    "       If (feature 110 <= 0.015)\n",
    "        Predict: 1.0\n",
    "       Else (feature 110 > 0.015)\n",
    "        If (feature 108 <= 18.5)\n",
    "         Predict: 1.0\n",
    "        Else (feature 108 > 18.5)\n",
    "         Predict: 0.0\n",
    "      Else (feature 111 > 0.245)\n",
    "       If (feature 99 <= 48.5)\n",
    "        If (feature 115 <= 0.005)\n",
    "         Predict: 2.0\n",
    "        Else (feature 115 > 0.005)\n",
    "         Predict: 0.0\n",
    "       Else (feature 99 > 48.5)\n",
    "        If (feature 108 <= 24.5)\n",
    "         Predict: 2.0\n",
    "        Else (feature 108 > 24.5)\n",
    "         Predict: 0.0\n",
    "     Else (feature 100 > 0.01)\n",
    "      Predict: 1.0\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, check the error and accuracy. Notice that the model is very good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of RF = 0.006327153168961042\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "evaluator = mcEval_4e127ff5dadb\n",
       "accuracy = 0.993672846831039\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.993672846831039"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val rf_predictions = rf_model.transform(test)\n",
    "\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val evaluator = new MulticlassClassificationEvaluator()\n",
    "  .setLabelCol(\"label\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "  .setMetricName(\"accuracy\")\n",
    "val accuracy = evaluator.evaluate(rf_predictions)\n",
    "println(\"Test Error of RF = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mlp\"></a>\n",
    "### 3.2 Set up the Multilayer Perceptron model\n",
    "Before building the Multilayer Perceptron (MLP) model, you need to know how many nodes are required for the input layer. \n",
    "\n",
    "Check the length of feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|(117,[1,10,72,82,...|\n",
      "|(117,[1,10,72,82,...|\n",
      "|(117,[1,10,72,82,...|\n",
      "|(117,[1,10,72,82,...|\n",
      "|(117,[1,10,72,82,...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **input** layer should have 117 nodes and the **output** layer should have ```5``` (5 label categories). This definition also contains an additional hidden layer with ```10``` nodes to build the model. You can change the definition of hidden layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "layers = Array(117, 8, 5)\n",
       "mlp = mlpc_ce78101a560d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "mlpc_ce78101a560d"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\n",
    "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n",
    "val layers = Array[Int](117, 10, 5)\n",
    "val mlp = new MultilayerPerceptronClassifier()\n",
    "  .setLayers(layers)\n",
    "  .setBlockSize(128)\n",
    "  .setSeed(1234L)\n",
    "  .setMaxIter(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training process takes 89.397066077 secs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "t = 4439491555134199\n",
       "mlp_model = mlpc_ce78101a560d\n",
       "duration = 89.397066077\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "89.397066077"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = System.nanoTime\n",
    "val mlp_model = mlp.fit(train)\n",
    "val duration = (System.nanoTime - t) / 1e9d\n",
    "println(s\"Training process takes $duration secs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the performance of the MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error of MLP = 0.015756382401562186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "mlp_predictions = [_c0: int, _c1: string ... 52 more fields]\n",
       "accuracy = 0.9842436175984378\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9842436175984378"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mlp_predictions = mlp_model.transform(test)\n",
    "val accuracy = evaluator.evaluate(mlp_predictions)\n",
    "println(\"Test Error of MLP = \" + (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 4. Summary and next steps     \n",
    "This notebook shows how to build two well-performing models using the Spark environment in Watson Studio. It is easy to build models using the Spark API and Watson Studio. Just provision the Spark environment, create the notebook, and you are ready to write your code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "Dua, D. and Karra Taniskidou, E. (2017). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "\n",
    "**Bufan Zeng** is a Data Scientist in IBM and a member of the Watson Studio offering management team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.11 with Spark",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
