{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"border: none\" align=\"left\">\n",
    "   <tr style=\"border: none\">\n",
    "      <th style=\"border: none\"><font face=\"verdana\" size=\"4\" color=\"black\"><b>Use Spark ML and Python to detect network intrusions</b></font></th>\n",
    "      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n",
    "   </tr> \n",
    "   <tr style=\"border: none\">\n",
    "       <td style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/raw/master/tensorflow/hand-written-digit-recognition/images/experiment_banner.png\" width=\"600\" height = \"200\" alt=\"Icon\"></td>\n",
    "   </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This notebook shows you how to easily build two classification models using the Spark Machine Learning (ML) library to detect network intrusions. It uses the Random Forest (RF) classifier and the Multilayer Perceptron (MLP) classifier to build the required algorithms.\n",
    "\n",
    "<a href=\"http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html\" target=\"_blank\" rel=\"noopener noreferrer\">UCI kddcup data</a> (743MB) is used in this notebook. This data set can be audited and provides intrusions simulated in a military network environment. It was originally used for the **The Third International Knowledge Discovery and Data Mining Tools Competition** organized for **KDD-99**. \n",
    "\n",
    "\n",
    "This notebook runs on Python and Spark in Watson Studio Spark Environments.\n",
    "\n",
    "\n",
    "## Table of contents\n",
    "\n",
    "1. [Download data](#download)<br>\n",
    "2. [Load and prepare data](#load)<br>\n",
    "3. [Build the models](#build)<br>\n",
    "    3.1 [Set up the Random Forest model](#rf)<br>\n",
    "    3.2 [Set up the Multilayer Perceptron model](#mlp)<br>\n",
    "4. [Saving models](#save)<br>\n",
    "5. [Summary and next steps](#summary)  \n",
    "\n",
    "  \n",
    "<a id=\"download\"></a>\n",
    " \n",
    "## 1. Download data <a id=\"download\"></a>\n",
    "\n",
    "First, download the prerequisite data set from Watson Studio using the following url: <a href=\"https://dataplatform.ibm.com/exchange-api/v1/entries/1438a61212a64ac435c837ba046efc19/data?accessKey=903188bb984a30f38bb889102a7db39f\" target=\"_blank\" rel=\"noopener noreferrer\">https://dataplatform.ibm.com/exchange-api/v1/entries/1438a61212a64ac435c837ba046efc19/data?accessKey=903188bb984a30f38bb889102a7db39f</a> \n",
    "\n",
    "Assign this URL to the variable `url`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"LINK-TO-DATA-SET-URL\"\n",
    "filename = \"./kddcup.zip\"\n",
    "!wget $url -O $filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a ```kddcup``` directory and **unzip** the file that you downloaded to the directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !rm -rf kddcup\n",
    "!mkdir kddcup\n",
    "!unzip kddcup.zip -d ./kddcup/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the content of the unzipped file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./kddcup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the entire data set ```kddcup.data``` (743 MB), run **gunzip** to unzip the file to the same directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gunzip ./kddcup/kddcup.data.gz -d ./kddcup/kddcup.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "## 2. Load and prepare data\n",
    "You can use the ```SparkSession``` to read the data directly into a dataframe because the data is provided in CSV (comma-separated values) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|_c0|_c1| _c2|_c3|_c4|  _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|   _c41|\n",
      "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "|  0|tcp|http| SF|215|45076|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   0|   0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|162| 4528|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   1|   1| 1.0| 0.0| 1.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|236| 1228|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   2|   2| 1.0| 0.0| 0.5| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|233| 2032|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   3|   3| 1.0| 0.0|0.33| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "|  0|tcp|http| SF|239|  486|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   3|   3| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   4|   4| 1.0| 0.0|0.25| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n",
      "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"./kddcup/kddcup.data\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take a look at the schema and labels of the last column ```_c41```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: integer (nullable = true)\n",
      " |-- _c5: integer (nullable = true)\n",
      " |-- _c6: integer (nullable = true)\n",
      " |-- _c7: integer (nullable = true)\n",
      " |-- _c8: integer (nullable = true)\n",
      " |-- _c9: integer (nullable = true)\n",
      " |-- _c10: integer (nullable = true)\n",
      " |-- _c11: integer (nullable = true)\n",
      " |-- _c12: integer (nullable = true)\n",
      " |-- _c13: integer (nullable = true)\n",
      " |-- _c14: integer (nullable = true)\n",
      " |-- _c15: integer (nullable = true)\n",
      " |-- _c16: integer (nullable = true)\n",
      " |-- _c17: integer (nullable = true)\n",
      " |-- _c18: integer (nullable = true)\n",
      " |-- _c19: integer (nullable = true)\n",
      " |-- _c20: integer (nullable = true)\n",
      " |-- _c21: integer (nullable = true)\n",
      " |-- _c22: integer (nullable = true)\n",
      " |-- _c23: integer (nullable = true)\n",
      " |-- _c24: double (nullable = true)\n",
      " |-- _c25: double (nullable = true)\n",
      " |-- _c26: double (nullable = true)\n",
      " |-- _c27: double (nullable = true)\n",
      " |-- _c28: double (nullable = true)\n",
      " |-- _c29: double (nullable = true)\n",
      " |-- _c30: double (nullable = true)\n",
      " |-- _c31: integer (nullable = true)\n",
      " |-- _c32: integer (nullable = true)\n",
      " |-- _c33: double (nullable = true)\n",
      " |-- _c34: double (nullable = true)\n",
      " |-- _c35: double (nullable = true)\n",
      " |-- _c36: double (nullable = true)\n",
      " |-- _c37: double (nullable = true)\n",
      " |-- _c38: double (nullable = true)\n",
      " |-- _c39: double (nullable = true)\n",
      " |-- _c40: double (nullable = true)\n",
      " |-- _c41: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------+\n",
      "|            _c41|  count|\n",
      "+----------------+-------+\n",
      "|    warezmaster.|     20|\n",
      "|          smurf.|2807886|\n",
      "|            pod.|    264|\n",
      "|           nmap.|   2316|\n",
      "|           imap.|     12|\n",
      "|   guess_passwd.|     53|\n",
      "|        ipsweep.|  12481|\n",
      "|      portsweep.|  10413|\n",
      "|          satan.|  15892|\n",
      "|           land.|     21|\n",
      "|     loadmodule.|      9|\n",
      "|      ftp_write.|      8|\n",
      "|buffer_overflow.|     30|\n",
      "|        rootkit.|     10|\n",
      "|    warezclient.|   1020|\n",
      "|       teardrop.|    979|\n",
      "|           perl.|      3|\n",
      "|            phf.|      4|\n",
      "|       multihop.|      7|\n",
      "|        neptune.|1072017|\n",
      "+----------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"_c41\").groupBy(\"_c41\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the <a href=\"http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types\" target=\"_blank\" rel=\"noopener noreferrer\">description</a>, the labels should be recoded into five categories using an SQL query. The new column name ```label_s``` stands for *label in string*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|label_s|  count|\n",
      "+-------+-------+\n",
      "|    u2r|     52|\n",
      "| normal| 972781|\n",
      "|    r2l|   1126|\n",
      "|  probe|  41102|\n",
      "|    dos|3883370|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"attack\")\n",
    "query = \"\"\"SELECT *, \n",
    "    CASE _c41 \n",
    "        WHEN 'back.' THEN 'dos'\n",
    "        WHEN 'buffer_overflow.' THEN 'u2r'\n",
    "        WHEN 'ftp_write.' THEN 'r2l'\n",
    "        WHEN 'guess_passwd.' THEN 'r2l'\n",
    "        WHEN 'imap.' THEN 'r2l'\n",
    "        WHEN 'ipsweep.' THEN 'probe'\n",
    "        WHEN 'land.' THEN 'dos'\n",
    "        WHEN 'loadmodule.' THEN 'u2r'\n",
    "        WHEN 'multihop.' THEN 'r2l'\n",
    "        WHEN 'neptune.' THEN 'dos'\n",
    "        WHEN 'nmap.' THEN 'probe'\n",
    "        WHEN 'perl.' THEN 'u2r'\n",
    "        WHEN 'phf.' THEN 'r2l'\n",
    "        WHEN 'pod.' THEN 'dos'\n",
    "        WHEN 'portsweep.' THEN 'probe'\n",
    "        WHEN 'rootkit.' THEN 'u2r'\n",
    "        WHEN 'satan.' THEN 'probe'\n",
    "        WHEN 'smurf.' THEN 'dos'\n",
    "        WHEN 'spy.' THEN 'r2l'\n",
    "        WHEN 'teardrop.' THEN 'dos'\n",
    "        WHEN 'warezclient.' THEN 'r2l'\n",
    "        WHEN 'warezmaster.' THEN 'r2l'\n",
    "        ELSE 'normal'\n",
    "END AS label_s \n",
    "FROM attack\"\"\"\n",
    "\n",
    "labeled = spark.sql(query)\n",
    "labeled.select(\"label_s\").groupBy(\"label_s\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build a pipeline to prepare data before building models.\n",
    "\n",
    "**Data preparation pipeline:**\n",
    "* StringIndexers: ```c1```, ```c2```, and ```c3``` are categorical strings. They must be indexed first.\n",
    "* OneHotEncoders: When the categorical strings have been indexed, you can use one-hot encoding to the indexed columns.\n",
    "* VectorAssembler: Include the wanted columns and assemble them as a feature vector.\n",
    "* labelIndexer: Another ```StringIndexer``` is used to index the ```label_s``` column to output it as ```label``` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "indexer1 = StringIndexer(inputCol=\"_c1\", outputCol=\"i_c1\")\n",
    "indexer2 = StringIndexer(inputCol=\"_c2\", outputCol=\"i_c2\")\n",
    "indexer3 = StringIndexer(inputCol=\"_c3\", outputCol=\"i_c3\")\n",
    "\n",
    "encoder1 = OneHotEncoder(inputCol=\"i_c1\", outputCol=\"v_c1\")\n",
    "encoder2 = OneHotEncoder(inputCol=\"i_c2\", outputCol=\"v_c2\")\n",
    "encoder3 = OneHotEncoder(inputCol=\"i_c3\", outputCol=\"v_c3\")\n",
    "\n",
    "featurenames = [\"_c0\", \"v_c1\", \"v_c2\", \"v_c3\", \"_c4\", \"_c5\", \"_c6\", \n",
    "                         \"_c7\", \"_c8\", \"_c9\", \"_c10\", \"_c11\", \"_c12\", \"_c13\", \n",
    "                         \"_c14\", \"_c15\", \"_c16\", \"_c17\", \"_c18\", \"_c19\",\n",
    "                         \"_c22\", \"_c23\", \"_c24\", \"_c25\", \"_c26\", \"_c27\", \n",
    "                         \"_c28\", \"_c29\", \"_c30\", \"_c31\", \"_c32\", \"_c33\", \"_c34\", \n",
    "                         \"_c35\", \"_c36\", \"_c37\", \"_c38\", \"_c39\", \"_c40\"]\n",
    "assembler = VectorAssembler(inputCols=featurenames, outputCol=\"features\")\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"label_s\", outputCol=\"label\")\n",
    "\n",
    "pipeline_prepare = Pipeline(stages=[indexer1,indexer2,indexer3,encoder1,encoder2,encoder3,assembler,labelIndexer])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now fit and transform the data to train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pipeline_prepare.fit(labeled).transform(labeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "## 3. Build the models\n",
    "This section describes how to build the models. Because of the large amount of data, we can use 60/40 split to mitigate overfitting:\n",
    "* 60% for the ```training``` set\n",
    "* 40% for the ```testing``` set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = data.randomSplit([0.6, 0.4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"rf\"></a>\n",
    "### 3.1 Set up the Random Forest model\n",
    "\n",
    "As the Random Forest (RF) algorithm is provided by Spark ML, you only have to set it up. \n",
    "\n",
    "In order to use the Watson Machine Learning (WML) service to store and deploy model, you need to put the classifier into a pipeline object.\n",
    "\n",
    "**Note:** There are 70 categories in the ```c2``` column. This is larger than the default of ```_MaxBins_```. To avoid errors ```_MaxBins_``` is set to 72, because it has to be larger than the biggest number of categories of all categorical variables. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=5, maxBins=72)\n",
    "\n",
    "rf_pipeline = Pipeline(stages=[rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training process takes 128.3435480594635 secs\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "rf_model = rf_pipeline.fit(train)\n",
    "print(\"Training process takes %s secs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details about the random forest classification model can be printed and will look something like the following:\n",
    "```\n",
    "RandomForestClassificationModel (uid=RandomForestClassifier_45c1962e4b7f0c44ea25) with 5 trees\n",
    "  Tree 0 (weight 1.0):\n",
    "    If (feature 83 <= 2.0)\n",
    "     If (feature 102 <= 0.04)\n",
    "      If (feature 105 <= 0.32)\n",
    "       If (feature 108 <= 254.5)\n",
    "        If (feature 98 <= 16.5)\n",
    "         Predict: 1.0\n",
    "        Else (feature 98 > 16.5)\n",
    "         Predict: 0.0\n",
    "       Else (feature 108 > 254.5)\n",
    "        If (feature 82 <= 299.5)\n",
    "         Predict: 2.0\n",
    "        Else (feature 82 > 299.5)\n",
    "         Predict: 0.0\n",
    "      Else (feature 105 > 0.32)\n",
    "       If (feature 98 <= 5.5)\n",
    "        If (feature 114 <= 0.05)\n",
    "         Predict: 1.0\n",
    "        Else (feature 114 > 0.05)\n",
    "         Predict: 0.0\n",
    "       Else (feature 98 > 5.5)\n",
    "        If (feature 94 <= 0.5)\n",
    "         Predict: 0.0\n",
    "        Else (feature 94 > 0.5)\n",
    "         Predict: 1.0\n",
    "     Else (feature 102 > 0.04)\n",
    "      If (feature 4 in {0.0})\n",
    "       If (feature 75 in {0.0})\n",
    "        If (feature 74 in {0.0})\n",
    "         Predict: 0.0\n",
    "        Else (feature 74 not in {0.0})\n",
    "         Predict: 1.0\n",
    "       Else (feature 75 not in {0.0})\n",
    "        If (feature 6 in {0.0})\n",
    "         Predict: 2.0\n",
    "        Else (feature 6 not in {0.0})\n",
    "         Predict: 1.0\n",
    "      Else (feature 4 not in {0.0})\n",
    "       If (feature 110 <= 0.10500000000000001)\n",
    "        If (feature 105 <= 0.365)\n",
    "         Predict: 0.0\n",
    "        Else (feature 105 > 0.365)\n",
    "         Predict: 2.0\n",
    "       Else (feature 110 > 0.10500000000000001)\n",
    "        Predict: 2.0\n",
    "    Else ...\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = rf_model.stages[0]\n",
    "print(rf.toDebugString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the error and accuracy. Notice that the model is very good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating process takes 43.46675395965576 secs\n",
      "Test Error of RF = 0.00214254 \n"
     ]
    }
   ],
   "source": [
    "rf_prediction = rf_model.transform(test)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "start_time = time.time()\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_accuracy = evaluator.evaluate(rf_prediction)\n",
    "print(\"Evaluating process takes %s secs\" % (time.time() - start_time))\n",
    "print(\"Test Error of RF = %g \" % (1.0 - rf_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mlp\"></a>\n",
    "### 3.2 Set up the Multilayer Perceptron model\n",
    "Before building the Multilayer Perceptron (MLP) model, you need to know how many nodes are required for the input layer. \n",
    "\n",
    "Check the length of feature vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputlayer = len(train.select(\"features\").take(1)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **output** layer should have ```5``` nodes (5 label categories). This definition also contains an additional hidden layer with ```10``` nodes to build the model. You can change the definition of hidden layer(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = [inputlayer, 10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the MLP classifier into a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "\n",
    "mlp = MultilayerPerceptronClassifier(maxIter=25, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "mlp_pipeline = Pipeline(stages=[mlp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train and fit the model to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training process takes 78.19903659820557 secs\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "mlp_model = mlp_pipeline.fit(train)\n",
    "print(\"Training process takes %s secs\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check the performance of the MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating process takes 42.84950494766235 secs\n",
      "Test Error of MLP = 0.010878 \n"
     ]
    }
   ],
   "source": [
    "mlp_prediction = mlp_model.transform(test)\n",
    "\n",
    "start_time = time.time()\n",
    "mlp_accuracy = evaluator.evaluate(mlp_prediction)\n",
    "print(\"Evaluating process takes %s secs\" % (time.time() - start_time))\n",
    "print(\"Test Error of MLP = %g \" % (1.0 - mlp_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## 4. Saving models\n",
    "\n",
    "You can use the Watson Machine Learning (WML) service to save and deploy your models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the WML client and credentials. Credentials can be generated and copied from bluemix.net. Then connect to WML using credentials.\n",
    "\n",
    "<b>Tip:</b> Authentication information (your credentials) can be found in the Service Credentials tab of the service instance that you created on IBM Cloud.\n",
    "If you cannot see the instance_id field in Service Credentials, click New credential (+) to generate new authentication information.\n",
    "\n",
    "<b>Action:</b> Enter your Watson Machine Learning service instance credentials here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!rm -rf $PIP_BUILD/watson-machine-learning-client\n",
    "!pip install watson-machine-learning-client --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watson_machine_learning_client import WatsonMachineLearningAPIClient\n",
    "import json\n",
    "wml_credentials = {\n",
    "  \"username\": \"...\",\n",
    "  \"password\": \"...\",\n",
    "  \"instance_id\": \"...\",\n",
    "  \"url\": \"https://ibm-watson-ml.mybluemix.net\"\n",
    "}\n",
    "client = WatsonMachineLearningAPIClient(wml_credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After connecting to the WML client, you can manage the models you have. API details can be found <a href=\"https://wml-api-pyclient.mybluemix.net/\" target=\"_blank\" rel=\"noopener noreferrer\">here</a>.\n",
    "\n",
    "List the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  --------------------------  ------------------------  --------------\n",
      "GUID                                  NAME                        CREATED                   FRAMEWORK\n",
      "4d2d79e9-88fe-477f-ae36-49e60467debd  RF_AttackDetection_PySpark  2018-07-17T15:46:30.155Z  mllib-2.3\n",
      "53c1a878-94e9-402e-88a6-110df59a363e  My linear svc model         2018-07-13T21:48:56.344Z  mllib-2.3\n",
      "48570f35-e9d3-434d-9ba5-030b84c5b9f4  test1                       2018-07-06T18:03:12.847Z  tensorflow-1.5\n",
      "c0abf3ad-af7c-4be4-ba22-3c39dca28c12  MyDLModel                   2018-06-26T20:38:21.425Z  tensorflow-1.5\n",
      "e614cddc-cb0a-440c-8e95-f4cfc4a29509  Kddcuprf                    2018-06-26T18:20:13.068Z  pmml-4.3\n",
      "f9aace7f-c3f8-4ab6-afd2-e7876eaccad1  My linear svc model         2018-06-19T14:48:05.469Z  mllib-2.2\n",
      "4a22405a-4464-40e0-a8a5-0ebc53537e3b  Best Heart Drug Selection   2018-05-31T20:02:18.644Z  mllib-2.1\n",
      "------------------------------------  --------------------------  ------------------------  --------------\n"
     ]
    }
   ],
   "source": [
    "client.repository.list_models()\n",
    "\n",
    "## To delete the model(s):\n",
    "#client.repository.delete(GUID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Meta data can be added to the model. You then pass the fitted pipeline object and the training data to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf_props = {client.repository.ModelMetaNames.AUTHOR_NAME: \"Bufan\", \n",
    "               client.repository.ModelMetaNames.NAME: \"RF_AttackDetection_PySpark\"}\n",
    "\n",
    "rf_saved_model = client.repository.store_model(model=rf_model, pipeline = rf_pipeline, meta_props=rf_props, training_data=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Details of the saved model can be printed and will look something like the following: \n",
    "```\n",
    "{\n",
    "  \"entity\": {\n",
    "    \"deployments\": {\n",
    "      \"count\": 0,\n",
    "      \"url\": \"https://us-south.ml.cloud.ibm.com/v3/wml_instances/40ac6090-70f5-40d9-836c-30dff9242a25/published_models/b365e08c-a480-4195-a920-94d8da0d0001/deployments\"\n",
    "    },\n",
    "    \"label_col\": \"label\",\n",
    "    \"model_type\": \"mllib-2.3\",\n",
    "    \"evaluation_metrics_url\": \"https://us-south.ml.cloud.ibm.com/v3/wml_instances/40ac6090-70f5-40d9-836c-30dff9242a25/published_models/b365e08c-a480-4195-a920-94d8da0d0001/evaluation_metrics\",\n",
    "    \"author\": {\n",
    "      \"name\": \"Bufan\"\n",
    "    },\n",
    "    \"name\": \"RF_AttackDetection_PySpark\",\n",
    "    \"learning_iterations_url\": \"https://us-south.ml.cloud.ibm.com/v3/wml_instances/40ac6090-70f5-40d9-836c-30dff9242a25/published_models/b365e08c-a480-4195-a920-94d8da0d0001/learning_iterations\",\n",
    "    \"input_data_schema\": {\n",
    "      \"type\": \"struct\",\n",
    "      \"fields\": [\n",
    "        {\n",
    "          \"nullable\": true,\n",
    "          \"name\": \"_c0\",\n",
    "          \"type\": \"integer\",\n",
    "          \"metadata\": {}\n",
    "        },\n",
    "        {\n",
    "          \"nullable\": true,\n",
    "          \"name\": \"_c1\",\n",
    "          \"type\": \"string\",\n",
    "          \"metadata\": {}\n",
    "        },\n",
    "        {\n",
    "          \"nullable\": true,\n",
    "          \"name\": \"_c2\",\n",
    "          \"type\": \"string\",\n",
    "          \"metadata\": {}\n",
    "        },\n",
    "        ....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_uid = client.repository.get_model_uid(rf_saved_model)\n",
    "model_details = client.repository.get_details(rf_model_uid)\n",
    "print(json.dumps(model_details, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the model should be in the WML service model list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------  --------------------------  ------------------------  --------------\n",
      "GUID                                  NAME                        CREATED                   FRAMEWORK\n",
      "b365e08c-a480-4195-a920-94d8da0d0001  RF_AttackDetection_PySpark  2018-07-20T19:56:24.554Z  mllib-2.3\n",
      "4d2d79e9-88fe-477f-ae36-49e60467debd  RF_AttackDetection_PySpark  2018-07-17T15:46:30.155Z  mllib-2.3\n",
      "53c1a878-94e9-402e-88a6-110df59a363e  My linear svc model         2018-07-13T21:48:56.344Z  mllib-2.3\n",
      "48570f35-e9d3-434d-9ba5-030b84c5b9f4  test1                       2018-07-06T18:03:12.847Z  tensorflow-1.5\n",
      "c0abf3ad-af7c-4be4-ba22-3c39dca28c12  MyDLModel                   2018-06-26T20:38:21.425Z  tensorflow-1.5\n",
      "e614cddc-cb0a-440c-8e95-f4cfc4a29509  Kddcuprf                    2018-06-26T18:20:13.068Z  pmml-4.3\n",
      "f9aace7f-c3f8-4ab6-afd2-e7876eaccad1  My linear svc model         2018-06-19T14:48:05.469Z  mllib-2.2\n",
      "4a22405a-4464-40e0-a8a5-0ebc53537e3b  Best Heart Drug Selection   2018-05-31T20:02:18.644Z  mllib-2.1\n",
      "------------------------------------  --------------------------  ------------------------  --------------\n"
     ]
    }
   ],
   "source": [
    "client.repository.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is in saved to the WML service, it can be loaded by connecting to WML client using the GUID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_loaded_model = client.repository.load(rf_model_uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print the details of the loaded_model, looks like this:\n",
    "```\n",
    "RandomForestClassificationModel (uid=RandomForestClassifier_45c1962e4b7f0c44ea25) with 5 trees\n",
    "  Tree 0 (weight 1.0):\n",
    "    If (feature 83 <= 2.0)\n",
    "     If (feature 102 <= 0.04)\n",
    "      If (feature 105 <= 0.32)\n",
    "       If (feature 108 <= 254.5)\n",
    "        If (feature 98 <= 16.5)\n",
    "         Predict: 1.0\n",
    "        Else (feature 98 > 16.5)\n",
    "         Predict: 0.0\n",
    "       Else (feature 108 > 254.5)\n",
    "        If (feature 82 <= 299.5)\n",
    "         Predict: 2.0\n",
    "        Else (feature 82 > 299.5)\n",
    "         Predict: 0.0\n",
    "      Else (feature 105 > 0.32)\n",
    "       If (feature 98 <= 5.5)\n",
    "        If (feature 114 <= 0.05)\n",
    "         Predict: 1.0\n",
    "        Else (feature 114 > 0.05)\n",
    "         Predict: 0.0\n",
    "       Else (feature 98 > 5.5)\n",
    "        If (feature 94 <= 0.5)\n",
    "         Predict: 0.0\n",
    "        Else (feature 94 > 0.5)\n",
    "         Predict: 1.0\n",
    "     Else (feature 102 > 0.04)\n",
    "      If (feature 4 in {0.0})\n",
    "       If (feature 75 in {0.0})\n",
    "        If (feature 74 in {0.0})\n",
    "         Predict: 0.0\n",
    "        Else (feature 74 not in {0.0})\n",
    "         Predict: 1.0\n",
    "       Else (feature 75 not in {0.0})\n",
    "        If (feature 6 in {0.0})\n",
    "         Predict: 2.0\n",
    "        Else (feature 6 not in {0.0})\n",
    "         Predict: 1.0\n",
    "      Else (feature 4 not in {0.0})\n",
    "       If (feature 110 <= 0.10500000000000001)\n",
    "        If (feature 105 <= 0.365)\n",
    "         Predict: 0.0\n",
    "        Else (feature 105 > 0.365)\n",
    "         Predict: 2.0\n",
    "       Else (feature 110 > 0.10500000000000001)\n",
    "        Predict: 2.0\n",
    "        ....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_debug_string = rf_loaded_model.stages[0].toDebugString\n",
    "print(loaded_debug_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_debug_string == rf.toDebugString"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## 5. Summary and next steps     \n",
    "This notebook shows how to build two well-performing models using the Spark environment in Watson Studio. It is easy to build models using the Spark API and Watson Studio. Just provision the Spark environment, create the notebook, and you are ready to write your code!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citations\n",
    "\n",
    "Dua, D. and Karra Taniskidou, E. (2017). [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml). Irvine, CA: University of California, School of Information and Computer Science."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author\n",
    "\n",
    "**Bufan Zeng**: a Data Scientist with the Watson Studio offering management team at IBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© IBM Corp. 2018. This notebook and its source code are released under the terms of the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
