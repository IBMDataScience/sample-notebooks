{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Run Spark use cases for watsonx.data\n","## Introduction\n","\n","This notebook demonstrates how IBM® watsonx.data integrates with IBM Analytics Engine and also helps to understand how to run Spark use cases for watsonx.data by using Python samples.\n","\n","IBM Cloud Pak for Data provides sample Spark usecase notebook, which can be used to understand the following functionalities in watsonx.data:\n","\n","* Accessing tables \n","\n","* Ingesting data\n","\n","* Modifying schema \n","\n","* Performing table maintenance activities\n","\n","\n","This notebook uses data that is publicly available at : https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page"]},{"cell_type":"markdown","metadata":{},"source":["## Table of Contents\n","\n","* [Before you begin](#byb)\n","* [Configuring IBM Analytics Engine](#conf)\n","* [Listing watsonx.data database](#lb)\n","* [Creating watsonx.data database](#cdb)\n","* [Table operations](#to)\n","* [Summary](#summ)\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"byb\"></a>\n","## Before you begin\n","\n","\n","* You can go through the notebook execution cell by cell, by selecting Shift-Enter or you can execute the entire notebook by selecting **Cell -> Run All** from the menu.\n","* Get the following information from watsonx.data:\n","\n","    * <wxd_hms_endpoint> : Thrift endpoint. For example, thrift://81823aaf-8a88-4bee-a0a1-6e76a42dc833.cfjag3sf0s5o87astjo0.databases.appdomain.cloud:32683.\n","    * <wxd_hms_username> : Username for watsonx.data instance. For IBM Cloud, the username is by default `ibmlhapikey`. For IBM Cloud Pak for Data, username of the user with `Metastore admin` role. For more information, see [Managing access to the Hive Metastore](https://www.ibm.com/docs/en/watsonxdata/1.1.x?topic=users-managing-access-hive-metastore).\n","    * <wxd_hms_password> : Hive Metastore (HMS) password.\n","    * Source bucket details:\n","        - <source_bucket_endpoint> : Endpoint of the source bucket. For example, for a source bucket in Dallas region, the endpoint is `s3.direct.us-south.cloud-object-storage.appdomain.cloud`.\n","        - <source_bucket_access_key>: Access key of the source bucket.\n","        - <source_bucket_secret_key> : Secret key of the source bucket.\n","    * Catalog bucket details:\n","        - <wxd_bucket_endpoint> : Endpoint of the catalog bucket. You can get the endpoint details from the watsonx.data instance administrator.\n","        - <wxd_bucket_access_key> : The access key of the catalog bucket. You can get the endpoint details from the watsonx.data instance administrator.\n","        - <wxd_bucket_secret_key> : The secret key of the catalog bucket. You can get the endpoint details from the watsonx.data instance administrator."]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"conf\"></a>\n","## Configuring IBM Analytics Engine\n"]},{"cell_type":"markdown","metadata":{},"source":["To connect to watsonx.data from IBM Analytics Engine, configure the following watsonx.data and Spark details in the Analytics Engine instance."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Configure Spark for wxd\n","conf=spark.sparkContext.getConf()\n","spark.stop()\n","\n","from pyspark import SparkConf,SparkContext\n","from pyspark.sql import SparkSession\n","\n","#conf=SparkConf()\n","conf.setAll([(\"spark.sql.catalogImplementation\", \"hive\"), \\\n","    (\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\"), \\\n","    (\"spark.sql.iceberg.vectorization.enabled\", \"false\"), \\\n","    (\"spark.sql.catalog.lakehouse\", \"org.apache.iceberg.spark.SparkCatalog\"), \\\n","    (\"spark.sql.catalog.lakehouse.type\", \"hive\"), \\\n","    (\"spark.sql.catalog.lakehouse.uri\", wxd_hms_endpoint), \\\n","    (\"spark.hive.metastore.client.auth.mode\", \"PLAIN\"), \\\n","    (\"spark.hive.metastore.client.plain.username\", wxd_hms_username), \\\n","    (\"spark.hive.metastore.client.plain.password\", wxd_hms_password), \\\n","    (\"spark.hive.metastore.use.SSL\", \"true\"), \\\n","    (\"spark.hive.metastore.truststore.type\", \"JKS\"), \\\n","    (\"spark.hive.metastore.truststore.path\", \"file:///opt/ibm/jdk/lib/security/cacerts\"), \\\n","    (\"spark.hive.metastore.truststore.password\", \"changeit\"), \\\n","    (\"spark.hadoop.fs.s3a.bucket.lakehouse-iae.endpoint\", source_bucket_endpoint), \\\n","    (\"spark.hadoop.fs.s3a.bucket.lakehouse-iae.access.key\", source_bucket_access_key), \\\n","    (\"spark.hadoop.fs.s3a.bucket.lakehouse-iae.secret.key\", source_bucket_secret_key), \\\n","    (\"spark.hadoop.fs.s3a.bucket.serverless-demo-bucket.endpoint\", wxd_bucket_endpoint), \\\n","    (\"spark.hadoop.fs.s3a.bucket.serverless-demo-bucket.access.key\", wxd_bucket_access_key), \\\n","    (\"spark.hadoop.fs.s3a.bucket.serverless-demo-bucket.secret.key\", wxd_bucket_secret_key) \\\n","])\n","\n","spark=SparkSession.builder.config(conf=conf).getOrCreate()"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"lb\"></a>\n","## Listing watsonx.data database\n","\n","\n","Use the `list_databases` function to list the existing databases in watsonx.data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def list_databases(spark):\n","    # list the database under lakehouse catalog\n","    spark.sql(\"show databases from lakehouse\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list_databases(spark)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"cdb\"></a>\n","## Creating watsonx.data database\n","\n","\n","Use the `create_database` function to create a new database named, `demodb` inside the watsonx.data catalog, `lakehouse`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_database(spark):\n","    # Create a database in the lakehouse catalog\n","    spark.sql(\"create database if not exists lakehouse.demodb LOCATION 's3a://lakehouse-iae/'\")\n","\n","create_database(spark)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list_databases(spark)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"to\"></a>\n","## Table operations\n","\n","watsonx.data supports creating tables by using the `basic_iceberg_table_operations` function. You can also create tables by importing data files that are in parquet and .csv formats by using `create_table_from_parquet_data` and `ingest_from_csv_temp_table`."]},{"cell_type":"markdown","metadata":{},"source":["### Performing basic table operations in Iceberg\n","\n","The database, `demodb` is configured to store all the data and metadata under the Cloud Object Storage (COS) bucket. It also creates an Iceberg table named, `testTable` and accesses the table to insert data and run a basic query."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def basic_iceberg_table_operations(spark):\n","    # demonstration: Create a basic Iceberg table, insert some data and then query table\n","    spark.sql(\"create table if not exists lakehouse.demodb.testTable(id INTEGER, name VARCHAR(10), age INTEGER, salary DECIMAL(10, 2)) using iceberg\").show()\n","    spark.sql(\"insert into lakehouse.demodb.testTable values(1,'Alan',23,3400.00),(2,'Ben',30,5500.00),(3,'Chen',35,6500.00)\")\n","    spark.sql(\"select * from lakehouse.demodb.testTable\").show()\n","\n","basic_iceberg_table_operations(spark)"]},{"cell_type":"markdown","metadata":{},"source":["### Ingesting data in parquet format to watsonx.data\n","\n","To run the usecase, download sample parquet data (for example, six months taxi data for the year 2022) from the following link.\n","* [Sample parquet file](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page)\n","\n","The `create_table_from_parquet_data` function allows you to ingest data in parquet format from a source COS bucket into a watsonx.data table within the `demodb` database. The sample data, `yellow_tripdata_2022-01.parquet` is inserted from the source COS bucket into the watsonx.data table, `table yellow_taxi_2022`. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def create_table_from_parquet_data(spark):\n","    # load parquet data into dataframce\n","    df = spark.read.option(\"header\",True).parquet(\"s3a://serverless-demo-bucket/nyc-taxi/yellow_tripdata_2022-01.parquet\")\n","    # write the dataframe into an Iceberg table\n","    df.writeTo(\"lakehouse.demodb.yellow_taxi_2022\").create()\n","    # describe the table created\n","    spark.sql('describe table lakehouse.demodb.yellow_taxi_2022').show(25)\n","    # query the table\n","    spark.sql('select * from lakehouse.demodb.yellow_taxi_2022').count()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["create_table_from_parquet_data(spark)"]},{"cell_type":"markdown","metadata":{},"source":["Run the following code to display the data from the watsonx.data table, `yellow_taxi_2022`. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark.sql('select * from lakehouse.demodb.yellow_taxi_2022').show()"]},{"cell_type":"markdown","metadata":{},"source":["### Ingesting data in CSV format to watsonx.data\n","\n","To run the usecase, download sample csv file (for example, zipcodes.csv) from the following link.\n","\n","* [Sample CSV file](https://raw.githubusercontent.com/spark-examples/spark-scala-examples/3ea16e4c6c1614609c2bd7ebdffcee01c0fe6017/src/main/resources/zipcodes.csv)\n","\n","The `ingest_from_csv_temp_table` function allows you to ingest data in CSV format from a source COS bucket into a watsonx.data table within the `demodb` database. The sample data, `zipcodes.csv` is inserted from source COS bucket into the watsonx.data table, `zipcodes`. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ingest_from_csv_temp_table(spark):\n","    # load csv data into a dataframe\n","    csvDF = spark.read.option(\"header\",True).csv(\"s3a://serverless-demo-bucket/zipcodes.csv\")\n","    csvDF.createOrReplaceTempView(\"tempCSVTable\")\n","    # load temporary table into an Iceberg table\n","    spark.sql('create or replace table lakehouse.demodb.zipcodes using iceberg as select * from tempCSVTable')\n","    # describe the table created\n","    spark.sql('describe table lakehouse.demodb.zipcodes').show(25)\n","    # query the table\n","    spark.sql('select * from lakehouse.demodb.zipcodes').show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ingest_from_csv_temp_table(spark)"]},{"cell_type":"markdown","metadata":{},"source":["### Analyzing monthly data\n","\n","Use the `ingest_monthly_data` function to stack monthly data (both CSV and parquet) into the watsonx.data table. You can update the watsonx.data table to include new data from the source data bucket on a monthly basis. Here, parquet sample is used as an example."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def ingest_monthly_data(spark):\n","    df_feb = spark.read.option(\"header\",True).parquet(\"s3a://serverless-demo-bucket//nyc-taxi/yellow_tripdata_2022-02.parquet\")\n","    df_march = spark.read.option(\"header\",True).parquet(\"s3a://serverless-demo-bucket//nyc-taxi/yellow_tripdata_2022-03.parquet\")\n","    df_april = spark.read.option(\"header\",True).parquet(\"s3a://serverless-demo-bucket//nyc-taxi/yellow_tripdata_2022-04.parquet\")\n","    df_may = spark.read.option(\"header\",True).parquet(\"s3a://serverless-demo-bucket//nyc-taxi/yellow_tripdata_2022-05.parquet\")\n","    df_june = spark.read.option(\"header\",True).parquet(\"s3a://serverless-demo-bucket//nyc-taxi/yellow_tripdata_2022-06.parquet\")\n","\n","    df_q1_q2 = df_feb.union(df_march).union(df_april).union(df_may).union(df_june)\n","    df_q1_q2.write.insertInto(\"lakehouse.demodb.yellow_taxi_2022\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ingest_monthly_data(spark)"]},{"cell_type":"markdown","metadata":{},"source":["Run the following code to check for the total number of records in the watsonx.data table, `table yellow_taxi_2022`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["spark.sql('select * from lakehouse.demodb.yellow_taxi_2022').count()"]},{"cell_type":"markdown","metadata":{},"source":["### Table maintenence   \n","\n","Table maintenance helps in maintaining the performance of tables in watsonx.data. Iceberg provides table maintenance procedures out of the box that allows performing powerful table optimizations in a declarative way. The following sample demonstrates how to do some table maintenance operations by using Spark. For more information about the Iceberg Spark table maintenance operations, see [Table Operations](https://iceberg.apache.org/docs/latest/spark-procedures/).\n","\n","The following are some of the table maintenance operations:\n","\n","* Querying the table and listing the underlying data files.\n","* Adjusting the file size to make data compact\n","* Removing unused data files\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def perform_table_maintenance_operations(spark):\n","    # Query the metadata files table to list underlying data files\n","    spark.sql(\"SELECT file_path, file_size_in_bytes FROM lakehouse.demodb.yellow_taxi_2022.files\").show()\n","\n","    # There are many smaller files compact them into files of 200MB each using the\n","    # `rewrite_data_files` Iceberg Spark procedure\n","    spark.sql(f\"CALL lakehouse.system.rewrite_data_files(table => 'demodb.yellow_taxi_2022', options => map('target-file-size-bytes','209715200'))\").show()\n","\n","    # Again, query the metadata files table to list underlying data files; 6 files are compacted\n","    # to 3 files\n","    spark.sql(\"SELECT file_path, file_size_in_bytes FROM lakehouse.demodb.yellow_taxi_2022.files\").show()\n","\n","    # List all the snapshots\n","    # Expire earlier snapshots. Only latest one with comacted data is required\n","    # Again, List all the snapshots to see only 1 left\n","    spark.sql(\"SELECT committed_at, snapshot_id, operation FROM lakehouse.demodb.yellow_taxi_2022.snapshots\").show()\n","    #retain only the latest one\n","    latest_snapshot_committed_at = spark.sql(\"SELECT committed_at, snapshot_id, operation FROM lakehouse.demodb.yellow_taxi_2022.snapshots\").tail(1)[0].committed_at\n","    print (latest_snapshot_committed_at)\n","    spark.sql(f\"CALL lakehouse.system.expire_snapshots(table => 'demodb.yellow_taxi_2022',older_than => TIMESTAMP '{latest_snapshot_committed_at}',retain_last => 1)\").show()\n","    spark.sql(\"SELECT committed_at, snapshot_id, operation FROM lakehouse.demodb.yellow_taxi_2022.snapshots\").show()\n","\n","    # Removing Orphan data files\n","    spark.sql(f\"CALL lakehouse.system.remove_orphan_files(table => 'demodb.yellow_taxi_2022')\").show(truncate=False)\n","\n","    # Rewriting Manifest Files\n","    spark.sql(f\"CALL lakehouse.system.rewrite_manifests('demodb.yellow_taxi_2022')\").show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Query the metadata files table to list underlying data files\n","spark.sql(\"SELECT file_path, file_size_in_bytes FROM lakehouse.demodb.yellow_taxi_2022.files\").show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# There are many smaller files compact them into files of 200MB each using the\n","# `rewrite_data_files` Iceberg Spark procedure\n","spark.sql(f\"CALL lakehouse.system.rewrite_data_files(table => 'demodb.yellow_taxi_2022', options => map('target-file-size-bytes','209715200'))\").show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["perform_table_maintenance_operations(spark)"]},{"cell_type":"markdown","metadata":{},"source":["### Modifying schema in watsonx.data\n","\n","Use the `evolve_schema` function to modify the data present in the watsonx.data table, `table yellow_taxi_2022`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def evolve_schema(spark):\n","    # demonstration: Schema evolution\n","    # Add column fare_per_mile to the table\n","    spark.sql('ALTER TABLE lakehouse.demodb.yellow_taxi_2022 ADD COLUMN(fare_per_mile double)')\n","    # describe the table\n","    spark.sql('describe table lakehouse.demodb.yellow_taxi_2022').show(25)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["evolve_schema(spark)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"cln\"></a>\n","### Cleaning database\n","\n","\n","After you finish the tutorial, you no longer require the tables that are present in the `demodb` database. You can use the `clean_database` function to perform the clean up activity to remove the tables."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def clean_database(spark):\n","    # clean-up the demo database\n","    spark.sql('drop table if exists lakehouse.demodb.testTable purge')\n","    spark.sql('drop table if exists lakehouse.demodb.zipcodes purge')\n","    spark.sql('drop table if exists lakehouse.demodb.yellow_taxi_2022 purge')\n","    spark.sql('drop database if exists lakehouse.demodb cascade')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["clean_database(spark)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["list_databases(spark)"]},{"cell_type":"markdown","metadata":{},"source":["<a id=\"summ\"></a>\n","## Summary\n","\n","This notebook shows you how watsonx.data integrates with IBM Analytics Engine to achieve the use-cases like ingestion, table maintenance and complex analytics operations. Also, allows you to quickly and easily get started with Spark usecases."]},{"cell_type":"markdown","metadata":{},"source":["Copyright © 2024 IBM. This notebook and its source code are released under the terms of the Apache License."]}],"metadata":{"kernelspec":{"display_name":"Python 3.10 with Spark","language":"python3","name":"python310"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":1}
