{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Theoretical and Practical Review of Elasticity and Pricing Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pricing is a complex topic, and every business in the world has to deal with it. Whether you are a nine-year-old with a lemonade stand or a transnational corporation, price is the mechanism that you use to operate your business. You provide a good or service. A customer pays the price for that good or service.\n",
    "\n",
    "Over the last twenty-five years or so, I have had the privilege to work on several pricing related projects. Each one was unique, and each required considerable thought and, ultimately, a specific solution. That said, some general concepts are helpful when you tackle pricing problems. That's my goal with this article. Not to provide a one size fits all pricing solution that works everywhere, but a general guide of data science techniques that are useful for pricing related problems.\n",
    "\n",
    "Note that in this exercise, I am using a python Jupyter notebook inside Watson Studio from IBM. For my visualizations, I am using Plotly.  Also, please note that all data used in this notebook is 100% fake.  I manufactured it for this demonstration.  Although it is completely fake, it does accurately represent several projects on which I have worked in the past."
    
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more detail, refer to my article,\n",
    "<a href=\"https://jshadgriffin.medium.com/a-theoretical-and-practical-review-of-elasticity-and-pricing-strategy-a42b1cc09dfe\" target=\"_blank\" rel=\"noopener noreferrer\">A Theoretical and Practical Review of Elasticity and Pricing Strategy</a> and <a href=\"https://youtu.be/45CFpetqR30\" target=\"_blank\" rel=\"noopener noreferrer\">video</a>.",
   
   "\n",
    "This notebook runs on Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Review of Microeconomics](#1.0)<br>\n",
    "    1.1 [Import Libraries](#1.1)<br>\n",
    "    1.2 [Create Demand Curve and Demand Schedule](#1.2)<br>\n",
    "    1.3 [Price Elasticity of Demand and Price Elasticity of Revenue](#1.3)<br>\n",
    "2. [Estimate Elasticity with a Real World Example](#2.0)<br>\n",
    "    2.1 [Import data, transform data and data exploration](#2.1)<br>\n",
    "    2.2 [Estimate Price Elasticity of Demand](#2.2)<br>\n",
    "    2.3 [Creating a Demand Curve](#2.3)<br>\n",
    "    2.4 [Estimate Price Elasticity of Revenue](#2.4)<br>\n",
    "3. [Identify Different Price Elasticities of Demand and Price Discriminate](#3.0)<br>\n",
    "    3.1 [Read, Explore and Transform Data](#3.1)<br>\n",
    "    3.2 [Build a Demand Curve for All Stores](#3.2)<br>\n",
    "    3.3 [Collect and Exploit the Residuals](#3.3)<br>\n",
    "    3.4 [Cluster Stores](#3.4)<br>\n",
    "    3.5 [Estimate Revenue Elasticity for Store Clusters](#3.5)<br>\n",
    "    3.6 [Build Demand Curve for Each Store Cluster](#3.6)<br>\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Review of Microeconomics <a id=\"1.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you took economics in school, you probably remember drawing a bunch of supply and demand curves. Hopefully, you'll remember the elegance of equilibriums and optimal spaces where things seemed to fit \"just right.\" (If you are not a dork like me and remember econ 101 as a series of formulas you had to memorize, that's ok tooÂ :)).\n",
    "\n",
    "You'll probably also remember something called elasticity. Elasticity is an integral part of price theory but also has tons of other applications. In this notebook, we walk through the basic concepts associated with pricing and price elasticity.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Import Libraries <a id=\"1.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import types\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "!pip install plotly --upgrade\n",
    "!pip install chart_studio  --upgrade\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly as plotly\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Create Demand Curve and Demand Schedule <a id=\"1.2\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x = [1,2,3,4,5,6,6.5,7,8,9,10],\n",
    "    y = [100,90,80,70,60,50,45,40,30,20,10],\n",
    "    mode = 'lines'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Demand Curve',\n",
    "    xaxis=dict(\n",
    "        title='Price',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Quantity',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "    \n",
    "data=[trace]  \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A demand curve reflects the relationship between price and quantity. All companies face demand curves. All businesses have customers, and those customers will buy a certain number of items based on the price you charge. If you lower the price, you will sell more. If you raise the price, you will sell less.  This simple fact is called the law of demand.\n",
    "\n",
    "Note that our demand curve has quantity on the vertical axis and price on the horizontal access. This is common in most graduate-level economics textbooks. Undergraduate level textbooks are usually the opposite (price is on the vertical axis). I know that doesn't make any sense, but we'll save a discussion for why this is the case for another day.\n",
    "\n",
    "Let's look specifically at the demand curve we just drew. Based on the relationship between price and quantity, if the firm charges one dollar, it will sell 100 units. At 10 dollars, it will sell ten units, and for 6 dollars, it will sell 50 units.\n",
    "\n",
    "Of course, there are a bunch of other options. You can easily see them in a demand schedule. A demand schedule is just the data underlying a demand curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'price': [1,2,3,4,5,6,6.5,7,8,9,10], 'quantity': [100,90,80,70,60,50,45,40,30,20,10]}\n",
    "df_demand_schedule= pd.DataFrame(data=d)\n",
    "\n",
    "df_demand_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1.3 Price Elasticity of Demand and Price Elasticity of Revenue <a id=\"1.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price elasticity of demand is the percentage change in quantity divided by the percentage change in price. It is a unitless number that details the price sensitivity at a particular point on the linear demand curve. Note that the elasticity of demand will vary as you move up and down a traditional demand curve. For example, on the curve above, the price elasticity of demand when the price is 2 dollars differs from the price elasticity of demand when the price is 8 dollars. I will show this graphically in just a second.\n",
    "\n",
    "Note that there is also something called a constant elasticity demand curve where elasticity doesn't change from point to point. Put that aside for the moment. We'll get back to constant elasticity demand curves in part two of our exercise.\n",
    "\n",
    "Here is the equation for price elasticity of demand.\n",
    "\n",
    "(%change in quantity)/(%change in price) or ((Q2-Q1)/Q1)/(P2-P1)/P1)\n",
    "\n",
    "Note that the equation above will always be a negative number. If you increase (decrease) price, the quantity will always decrease (increase). Sometimes you'll see the equation like this. -1*(((Q2-Q1)/Q1)/(P2-P1)/P1))\n",
    "\n",
    "The -1 at the front of the equation makes it positive. In this exercise, we will just let it be a negative number.\n",
    "\n",
    "In advanced economics studies, the price elasticity of demand is usually expressed as a derivative (calculus).\n",
    "\n",
    "(dQ/dP) * (P1/Q1)\n",
    "\n",
    "Let's add elasticity to our demand schedule, then look at it visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_schedule['Price_Elasticity_of_Demand']=(df_demand_schedule.quantity.pct_change() / df_demand_schedule.price.pct_change())\n",
    "df_demand_schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=df_demand_schedule['price']\n",
    "y1=df_demand_schedule['quantity']\n",
    "z1=df_demand_schedule['Price_Elasticity_of_Demand']\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=y1,\n",
    "    name='Demand Curve'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=z1,\n",
    "    name='Elasticity',\n",
    "    yaxis='y2'\n",
    ")\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Price Elasticity of Demand and the Demand Schedule',\n",
    "    yaxis=dict(\n",
    "        title='Quantity'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Price Elasticity of Demand',\n",
    "        titlefont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "        xaxis=dict(\n",
    "        title='Price',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when the price elasticity of demand is greater than -1, a 1% increase in price will lower quantity by less than 1%. When the price elasticity of demand is less than -1, a 1% increase in price will lead to a greater than 1% decrease in quantity. When it is equal to -1, it is unitary elastic. This means a 1% increase in price will lead to a 1% decrease in quantity.\n",
    "\n",
    "Another critically important metric is revenue. Revenue is (price*quantity).\n",
    "\n",
    "Typically, you'll want to choose the price that maximizes revenue or profit.\n",
    "\n",
    "Price elasticity of revenue is another critical metric. Similar to the price elasticity of demand, the price elasticity of revenue is the percentage change in revenue divided by the percentage change in price.\n",
    "\n",
    "Calculate the price elasticity of revenue with the following formula.\n",
    "\n",
    "(%change in revenue)/(%change in price)\n",
    "\n",
    "or\n",
    "\n",
    "((R2-R1)/R1)/(P2-P1)/P1)\n",
    "\n",
    "In advanced economics studies, the price elasticity of revenue is almost always expressed as a derivative (calculus).\n",
    "\n",
    "(dR/dP) * (P1/R1)\n",
    "\n",
    "Now that we have defined revenue and the price elasticity of revenue let's add them to our demand schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demand_schedule['Revenue']=df_demand_schedule['price']*df_demand_schedule['quantity']\n",
    "df_demand_schedule['Price_Elasticity_of_Revenue']=(df_demand_schedule.Revenue.pct_change() / df_demand_schedule.price.pct_change())\n",
    "df_demand_schedule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, Lets plot revenue with demand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=df_demand_schedule['price']\n",
    "y1=df_demand_schedule['quantity']\n",
    "z1=df_demand_schedule['Revenue']\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=y1,\n",
    "    name='Demand Curve'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=z1,\n",
    "    name='Revenue',\n",
    "    yaxis='y2'\n",
    ")\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Revenue, Quantity and Price',\n",
    "    yaxis=dict(\n",
    "        title='Quantity'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Revenue',\n",
    "        titlefont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "        xaxis=dict(\n",
    "        title='Price',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot price elasticity of revenue next to Demand.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1=df_demand_schedule['price']\n",
    "y1=df_demand_schedule['quantity']\n",
    "z1=df_demand_schedule['Price_Elasticity_of_Revenue']\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=y1,\n",
    "    name='Demand Curve'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=z1,\n",
    "    name='Price Elasticity of Revenue',\n",
    "    yaxis='y2'\n",
    ")\n",
    "data = [trace1, trace2]\n",
    "layout = go.Layout(\n",
    "    title='Price Elasticity of Revenue, Quantity and Price',\n",
    "    yaxis=dict(\n",
    "        title='Quantity'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Price Elasticity of Revenue',\n",
    "        titlefont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "        xaxis=dict(\n",
    "        title='Price',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The price elasticity of revenue is similar but also slightly different from the Price elasticity of demand. \n",
    "Price elasticity of revenue is centered on zero, which makes it easier to interpret. If revenue elasticity is greater than zero, a price increase will increase revenue. If it is less than zero, increasing price will lower revenue.\n",
    "\n",
    "Based on the charts above, you can see that a price between 5 and 6 dollars will maximize revenue. To maximize profit, you'd need to incorporate cost information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Another concept that is critical to understand is price discrimination. Let's say that two 22-year-old men walk into a convenience store off an old highway in rural West Texas to buy a bottle of water. This part of Texas is a desert. It is in the middle of July. The sun is hot, and rain is scarce. One of the men walking into the store drove to the store in a car with air conditioning. The second man walked three miles to get to the store. The man who walked to the store is likely to be thirstier than the man who drove, right? I could even say that the man who walked's demand curve for the bottle of water is higher than the man who drove his car. Even though the store owner knows this, he really can't charge the thirstier man more the water, even though the thirstier man would likely pay more. In other words, the store owner cannot price discriminate between the two customers.\n",
    "\n",
    "Price discrimination is charging different prices to different customers based on their demand for your product. The more a firm can price discriminate, the higher their profits will be. There are books written on why this is so, but for now, understanding this basic fact is good enough. Be careful, though; sometimes, price discrimination is illegal. There are many situations where it is lawful, however. The airlines are masters of price discrimination. Historically, they separate consumers who book travel early versus those who book travel late. The earlier you book a flight, typically the cheaper the fare. Movie theaters that offer a senior discount also use price discrimination. They offer a lower price because seniors have a different demand curve than younger moviegoers.\n",
    "\n",
    "One last economics concept to understand related to pricing is inflation. Inflation is the annual upward drift in pricing that occurs across an economy over time. If you've lived life, you have probably noticed that most things get a little more expensive each year. Another way to think of inflation is the change in the cost of living. In the United States, inflation has been pretty low over the last twenty years. Typically, the US inflation rate is estimated to be about 1% to 3% annually. Understanding inflation is essential because when we set prices in the real world, we must consider inflation. Usually, this means that price changes account for changes in inflation. For example, if you have a market that is prime for a price increase, you'll likely increase prices by inflation plus some percentage. If the inflation rate is 1.5%, you may decide to raise prices by inflation plus 5%. This means the actual change in prices is 6.5%. Likewise, if you want to decrease the price, you can hold it constant. If inflation is 1.5% and the price of your product remains unchanged year over year, essentially, this amounts to a 1.5% decrease in price. Think of it like this.\n",
    "If everything else in the economy increases in price by 1.5% and your product's price did not change, in relative or real terms, the price of your product is lower than it was the previous year.\n",
    "\n",
    "It's all relative, right? I remember when I was a kid, and all my friends were growing taller when I wasn't. I wasn't shrinking, but it sure felt like it.\n",
    "\n",
    "Now that we understand the basic concepts let's move to a real-world example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Estimate Elasticity with a Real World Example <a id=\"2.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this use case, we will examine price elasticity for a regional retailer with 219 locations. The type of retailer isn't essential. It could be a convenience store, a restaurant, or a coffee shop. Whatever the business of the retailer, their data should look the same.\n",
    "\n",
    "It is important to note that this data is 100% fake and it belongs to me. I created this data from scratchÂ . Although it is synthetic, it is very consistent with real data I have worked with in the past.\n",
    "\n",
    "The success of a retailer depends on several factors. One is management and management decisions. Pricing, for example, is 100% controllable by management. Environmental factors surrounding the store are also critical and typically not very controllable. For example, a retailer will naturally do better if the people living around the store are wealthy than if they are poor.\n",
    "\n",
    "Our data set in this exercise is a combination of controllable and non-controllable factors. The controllable factor is the price. We also have numerous non-controllable environmental factors.\n",
    "\n",
    "Our goal in this exercise is to understand better the relationship between price, quantity, and revenue. We achieve this goal by estimating the price elasticity of demand and the price elasticity of revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Import data, transform data and data exploration <a id=\"2.1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to read in the data from Github."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm RETAIL_DATA.csv\n",
    "!wget https://raw.githubusercontent.com/shadgriffin/Pricing_Tutorial/master/RETAIL_DATA.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_datax = pd.read_csv(\"RETAIL_DATA.csv\")\n",
    "\n",
    "df_retail = pd_datax\n",
    "df_retail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first few rows of the data set should appear above.\n",
    "\n",
    "Here is a definition of each field.\n",
    "\n",
    "STORE_ID - is a unique id specific to each retail outlet\n",
    "\n",
    "PERCENTAGE_OF_RENTERS is the percentage of households surrounding the store that rent their housing.\n",
    "\n",
    "PERCENTAGE_OF_CHILDREN is the percentage of households surrounding the store that have children.\n",
    "\n",
    "AVERAGE_INCOME is the average annual income of the households surrounding the store.\n",
    "\n",
    "AVERAGE_AGE_IN_YEARS is the average age of the head of household in the vicinity of the retail outlet.\n",
    "\n",
    "AVERAGE_LENGTH_OF_RESIDENCE is an average of the time individuals surrounding the retail outlet have lived at their current address.\n",
    "\n",
    "PERCENT_SPEAKING_SPANISH is the percentage of households surrounding the store that speak Spanish\n",
    "\n",
    "PRICE is the average price across multiple items sold at the retail outlet.\n",
    "\n",
    "QUANTITY is the number of items sold by the retail outlet in the last year.\n",
    "\n",
    "REVENUE is the total revenue for the store in the last year.\n",
    "\n",
    "This exercise aims to calculate the price elasticity of demand and the price elasticity of revenue. (Please look at the first section in this series to understand these terms.) This isn't that hard. To estimate an elasticity, you can use a standard ordinary least squares regression and natural log (base e) transformed variables.\n",
    "The first step is to take the natural log of each variable. The cell below does this. Note the new variables created at the end of the DF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retail['LN_PRICE'] = np.log((df_retail.PRICE))\n",
    "df_retail['LN_REVENUE'] = np.log((df_retail.REVENUE))\n",
    "df_retail['LN_QUANTITY'] = np.log((df_retail.QUANTITY))\n",
    "df_retail['LN_INCOME'] = np.log((df_retail.AVERAGE_INCOME))\n",
    "df_retail['LN_AVERAGE_AGE_IN_YEARS'] = np.log((df_retail.AVERAGE_AGE_IN_YEARS))\n",
    "df_retail['LN_AVERAGE_LENGTH_OF_RESIDENCE'] = np.log((df_retail.AVERAGE_LENGTH_OF_RESIDENCE))\n",
    "df_retail['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_retail.PERCENT_SPEAKING_SPANISH))\n",
    "df_retail['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_retail.PERCENT_HAVING_CHILDREN))\n",
    "df_retail['LN_PERCENTAGE_OF RENTERS'] = np.log((df_retail.PERCENTAGE_OF_RENTERS))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Estimate Price Elasticity of Demand <a id=\"2.2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's build an ordinary least squares regression using the log-transformed variables.\n",
    "\n",
    "Define your independent and dependent variables with the following code cell.\n",
    "\n",
    "Note that we only include statistically significant variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independentx = df_retail[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n",
    "                          'LN_PERCENT_SPEAKING_SPANISH']]\n",
    "independent = sm.add_constant(independentx, prepend=False)\n",
    "dependent=df_retail['LN_QUANTITY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells run the OLS regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(dependent, independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the price elasticity of demand is -.64. This comes from the ANOVA table above and is the estimated coefficient of LN_PRICE regressed on LN_QUANTITY.\n",
    "\n",
    "A 1% increase in price will lower the quantity sold byÂ .64%.Â \n",
    "\n",
    "The other coefficients can be interpreted similarly.Â \n",
    "\n",
    "A 1% increase in people's average income around a store will increase the quantity sold byÂ .55%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Creating a Demand Curve <a id=\"2.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a demand curve. To do this, we will need to build a demand schedule using the predicted quantity at different prices based on our OLS regression model.\n",
    "\n",
    "First, let's summarize the variables other than price that are significant in the model. We will take the average and then evaluate the relationship between price and quantity when the other variables are at their mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retail['chachacha']=1\n",
    "\n",
    "doodad=['PERCENTAGE_OF_RENTERS', 'PERCENT_HAVING_CHILDREN','AVERAGE_INCOME','PERCENT_SPEAKING_SPANISH']\n",
    "wookie = df_retail.groupby(['chachacha'])[doodad].mean()\n",
    "\n",
    "wookie.reset_index(level=0, inplace=True)\n",
    "wookie.head()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create an array of prices and then convert the list to a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an array of prices\n",
    "price = [1.50,1.75,2.0,2.25,2.50,2.75,3.0,3.25,3.50,3.75,4.0,4.25,4.50,4.75,5.0,5.25,5.50,5.75,6.0,6.25,6.50,6.75,\n",
    "         7.0,7.25,7.5,7.75,8.0,8.25,8.50,8.75,9.0,9.25,9.50,9.75,10.0,10.25,10.50,10.75,11.0,11.25,11.50,11.75,12.0]\n",
    "df_price=pd.DataFrame(price)\n",
    "df_price.columns = ['PRICE']\n",
    "df_price['chachacha']=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, merge the average value for the non-price variables to the prices we constructed above and calculate the log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the array of prices to the average values of the other independent variables.\n",
    "df_price =df_price.merge(wookie, on=['chachacha'], how='inner')\n",
    "\n",
    "#Create Log Transformed Variables\n",
    "df_price['LN_PRICE'] = np.log((df_price.PRICE))\n",
    "df_price['LN_INCOME'] = np.log((df_price.AVERAGE_INCOME))\n",
    "df_price['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_price.PERCENT_SPEAKING_SPANISH))\n",
    "df_price['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_price.PERCENT_HAVING_CHILDREN))\n",
    "df_price['LN_PERCENTAGE_OF_RENTERS'] = np.log((df_price.PERCENTAGE_OF_RENTERS))\n",
    "df_price['const']=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the input variables we manufactured in the last few lines of code and our model to predict quantity at each price point. The result will be a demand schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create our scoring data set.\n",
    "scoring= df_price[['LN_PRICE','LN_PERCENTAGE_OF_RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n",
    "                          'LN_PERCENT_SPEAKING_SPANISH','const']]\n",
    "#score the scoring data set\n",
    "ln_q_hat=pd.DataFrame(res.predict(scoring))\n",
    "#name the columns correctly\n",
    "ln_q_hat.columns = ['LN_Q_HAT']\n",
    "#combine ln of price and ln of predicted q into a new data frame\n",
    "df_ce_demand = pd.concat([scoring['LN_PRICE'], ln_q_hat], axis=1)\n",
    "\n",
    "\n",
    "#exponentiate the ln variables to get predicted quantity and price\n",
    "df_ce_demand['Q_HAT']=np.exp(df_ce_demand['LN_Q_HAT'])\n",
    "df_ce_demand['PRICE']=np.exp(df_ce_demand['LN_PRICE'])\n",
    "\n",
    "#eliminate the ln variables and make the demand schedule.\n",
    "df_ce_demand = df_ce_demand[['Q_HAT','PRICE']]\n",
    "\n",
    "df_ce_demand.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bingo! Now we have a demand schedule and we can plot it as a demand curve.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trace = go.Scatter(\n",
    "    x = df_ce_demand['PRICE'],\n",
    "    y = df_ce_demand['Q_HAT'],\n",
    "    mode = 'lines'\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title='Demand Curve',\n",
    "    xaxis=dict(\n",
    "        title='Price',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Quantity',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "    \n",
    "data=[trace]  \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "\n",
    "#plot_url = py.plot(fig, filename='styling-names')\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wait a minute! That's not linear! Nope, it isn't. This is a variation of the demand curves we developed in part 1 of our exercise. It is what we call a constant elasticity demand curve. The elasticity is the same at all points.\n",
    "\n",
    "Remember, in our earlier discussion, that elasticity was different at each price point of the demand curve. With this demand curve, elasticity is the same at all the price points. It still shows the relationship between price and quantity.\n",
    "\n",
    "For example, for 6 dollars, our firm can sell about 75,000 units at each store.\n",
    "\n",
    "If you wanted a linear demand curve, you could get there. You would regress price on quantity (instead of ln of price on ln of quantity). Of course, if you built your model this way, the coefficient wouldn't be an elasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Estimate Price Elasticity of Revenue <a id=\"2.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating the price elasticity of revenue follows a similar process. The difference is we will use the natural log of revenue as a dependent variable instead of the natural log of quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independentx = df_retail[['LN_PRICE','LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n",
    "                          'LN_PERCENT_SPEAKING_SPANISH']]\n",
    "independent = sm.add_constant(independentx, prepend=False)\n",
    "dependent=df_retail['LN_REVENUE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(dependent, independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mod.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regression results suggest that the price elasticity of revenue isÂ .358. This means that a 1% increase in price will lead to aÂ .358% increase in revenue. In other words, this firm would make more revenue if it increased prices.\n",
    "\n",
    "There are few important caveats that I should probably mention.\n",
    "\n",
    "One, this is a point estimate. That is a fancy way of saying that you shouldn't get too crazy. If you increase prices by 1%, you probably will increase revenue byÂ .35%. However, if you raise prices by 100%, you probably would not realize a 35% increase in revenue. Baked into the model is an established historical relationship between your customers and your prices. If you do something very different than the historical norm, don't expect the model to be predictive.\n",
    "\n",
    "Two, it is essential to understand this elasticity is an average across all stores. The price elasticity of revenue isÂ .358, on average. There are 291 stores in the data set. Some probably have an elasticity greater thanÂ .35. Others probably have an elasticity that is less thanÂ .35. In other words, if you increase prices by 3% across the board, on average, you will realize a 1.05% increase in revenue. This is an average. Some stores will recognize more than 1.05%, and others will discover less than 1.05%. A 3% increase in prices may even cause some stores to lose revenue.\n",
    "\n",
    "What if you could tailor the price increase for each store?\n",
    "\n",
    "That is, increase prices by an average of 3%, but give some stores a higher bump in prices than others. You could even decrease prices in some stores if it makes sense. Tailoring each store's price increase based on their specific market will lead to an even more significant revenue increase. For example, you can raise prices by 3% on average and get an increase in revenue greater than 1.05%.\n",
    "\n",
    "There are many ways to accomplish this goal. In the third part of this exercise, we will examine a relatively simple and straightforward way to make a market-based pricing decision for each of our 291 stores.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Identify Different Price Elasticities of Demand and Price Discriminate <a id=\"3.0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we learned that our organization's price elasticity of revenue isÂ .35. A 1% increase in price will generate aÂ .35% increase in revenue. If we increase prices across all stores by 3%, we will increase revenue by 1.05%.\n",
    "\n",
    "Can we do better than this? Yes, by price discriminating. To price discriminate, we must identify stores that have higher and lower revenue elasticity of demand. The higher the revenue elasticity of demand, the higher the ability to generate revenue from raising prices. The lower the revenue elasticity of demand, the higher our ability to increase revenue from lowering prices.\n",
    "\n",
    "So how do you know which stores can sustain a higher price increase?\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our approach, we engage in the following steps to segment our stores by their price sensitivity.\n",
    "This approach involves the following steps (I will explain why each step is necessary below).\n",
    "1. Build a demand curve that includes all stores.\n",
    "2. Use the residual from the demand curve equation and each store's price to segment the stores into groups.\n",
    "3. Estimate the price elasticity of revenue for each group.\n",
    "4. Set prices based on the price elasticity of revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Read, Explore and Transform Data <a id=\"3.1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retail = pd_datax\n",
    "df_retail.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, here is a definition of each field.\n",
    "\n",
    "STORE_IDâ-âis a unique id specific to each retail outlet\n",
    "\n",
    "PERCENTAGE_OF_RENTERS is the percentage of households surrounding the store that rent their housing.\n",
    "\n",
    "PERCENTAGE_OF_CHILDREN is the percentage of households surrounding the store that have children.\n",
    "\n",
    "AVERAGE_INCOME is the average annual income of the households surrounding the store.\n",
    "\n",
    "AVERAGE_AGE_IN_YEARS is the average age of the head of household in the vicinity of the retail outlet.\n",
    "\n",
    "AVERAGE_LENGTH_OF_RESIDENCE is an average of the time individuals surrounding the retail outlet have lived at their current address.\n",
    "\n",
    "PERCENT_SPEAKING_SPANISH is the percentage of households surrounding the store that speak Spanish\n",
    "\n",
    "PRICE is the average price across multiple items sold at the retail outlet.\n",
    "\n",
    "QUANTITY is the number of items sold by the retail outlet in the last year.\n",
    "\n",
    "REVENUE is the total revenue for the store in the last year.\n",
    "\n",
    "Now, create new fields by taking the natural log of each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retail['LN_PRICE'] = np.log((df_retail.PRICE))\n",
    "df_retail['LN_REVENUE'] = np.log((df_retail.REVENUE))\n",
    "df_retail['LN_QUANTITY'] = np.log((df_retail.QUANTITY))\n",
    "df_retail['LN_INCOME'] = np.log((df_retail.AVERAGE_INCOME))\n",
    "df_retail['LN_AVERAGE_AGE_IN_YEARS'] = np.log((df_retail.AVERAGE_AGE_IN_YEARS))\n",
    "df_retail['LN_AVERAGE_LENGTH_OF_RESIDENCE'] = np.log((df_retail.AVERAGE_LENGTH_OF_RESIDENCE))\n",
    "df_retail['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_retail.PERCENT_SPEAKING_SPANISH))\n",
    "df_retail['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_retail.PERCENT_HAVING_CHILDREN))\n",
    "df_retail['LN_PERCENTAGE_OF RENTERS'] = np.log((df_retail.PERCENTAGE_OF_RENTERS))\n",
    "\n",
    "df_retail.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Build a Demand Curve for All Stores <a id=\"3.2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retaily=df_retail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the regressors and the dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independentx = df_retaily[['LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n",
    "                          'LN_PERCENT_SPEAKING_SPANISH','LN_PRICE']]\n",
    "independent = sm.add_constant(independentx, prepend=False)\n",
    "dependent=df_retaily['LN_QUANTITY']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = sm.OLS(dependent, independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = mod.fit()\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, the price elasticity of demand for all stores is -.64.  I 1% increase in price will lead to a .64% decrease in quantity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Collect and Exploit the Residuals <a id=\"3.3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use the residual from the demand equation to segment our stores into groups.  The first step is to save the residuals as a variable in our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q_pred = pd.DataFrame(results.predict(independent))\n",
    "\n",
    "df_q_pred.columns = ['P_LN_QUANTITY']\n",
    "df_q_pred['P_QUANTITY']=np.exp(df_q_pred['P_LN_QUANTITY'])\n",
    "df_retailx = pd.concat([df_retaily, df_q_pred], axis=1)\n",
    "df_retailx['R_QUANTITY']=df_retailx['QUANTITY']-df_retailx['P_QUANTITY']\n",
    "\n",
    "df_forview=df_retailx[['STORE_ID','QUANTITY','P_QUANTITY','R_QUANTITY']]\n",
    "\n",
    "\n",
    "df_forview.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a minute and discuss what the residual from this linear equation means. The residual, as you recall, is the difference between the actual quantity and the predicted quantity.\n",
    "\n",
    "Let's look at the two rows above. In the first row, the actual quantity is about 95,000, and the amount predicted is about 75,000 for a difference of about 20,000.\n",
    "Given the price and market factors of STORE_ID 177473, we would expect the quantity sold to be about 75,000. In reality, this store is doing much better than that. It is selling almost 20,000 more units than we would expect. In other words, it is over-performing.\n",
    "\n",
    "In the second row, the actual quantity sold is about 95,000, and the expected quantity sold (given the price and market factors) is about 95,000. In other words, STORE_ID 177467 is an average performer.\n",
    "\n",
    "So, what do the previous statements tell us about the current prices at each store? It is hard to say at this point. It could be that the first store is over-performing because it is priced optimally. Or, it could be telling us that this store is on a completely different demand curve, and there is an opportunity to raise prices.\n",
    "One thing is for sure. These stores are different.\n",
    "\n",
    "Next, let's take a look at how each store is priced relative to each other. That is, what stores are priced higher than average and what stores are priced lower than average.\n",
    "\n",
    "In the next step, we will calculate the PRICE_DELTA. The difference between the price of each store and the average price of all stores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_retailx.copy()\n",
    "df['AVERAGE_PRICE']=df['PRICE'].mean()\n",
    "\n",
    "\n",
    "df['PRICE_DELTA']=df['PRICE']-df['AVERAGE_PRICE']\n",
    "\n",
    "\n",
    "df_forview=df[['STORE_ID','QUANTITY','P_QUANTITY','R_QUANTITY', 'PRICE', 'PRICE_DELTA']]\n",
    "\n",
    "\n",
    "df_forview.head(2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the store in the first row, we can now see that the store is overperforming, and its price is 2.4 cents higher than average. For the store in the second row, it is priced 1.4 cents higher than average.\n",
    "\n",
    "We still donât have enough information to know whether these stores deserve a price increase or decrease. Letâs see if we can use these new metrics to cluster our stores into groups that likely face a similar demand curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Cluster Stores <a id=\"3.4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import K-means libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the features for the K-Means procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df[['PRICE_DELTA','R_QUANTITY']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the clusters.  Note that I came up with 5 clusters after a little trial and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "init=\"random\",\n",
    "n_clusters=5,\n",
    "n_init=10,\n",
    "max_iter=300,\n",
    "random_state=42)\n",
    "\n",
    "kmeans.fit(scaled_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine statistics of clusters and kmeans procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_iter_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Append new clusters to the original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz=kmeans.labels_\n",
    "df['CLUSTER'] = pd.Series(zz, index=df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the average price delta and quantity residual for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output = df.groupby([\"CLUSTER\"])[[\"PRICE_DELTA\", \"R_QUANTITY\"]].mean()\n",
    "df_output= df_output.sort_values([\"PRICE_DELTA\"])\n",
    "df_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very insightful information.  \n",
    "    \n",
    "    Cluster 3 represents stores that are priced very low and are under-performing.  \n",
    "    Cluster 4 represents stores that are priced a little low and are way over-performing.\n",
    "    Cluster 0 represents stores that are priced a little high and over-performing.\n",
    "    Cluster 1 represents stores that are priced a little high and are severely under-performing.\n",
    "    Cluster 2 represents stores that are priced very high and under-performing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the clusters graphically to get even more clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.CLUSTER = df.CLUSTER.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(df, y=\"PRICE_DELTA\", x=\"R_QUANTITY\", color=\"CLUSTER\", size_max=45)\n",
    "\n",
    "fig.update_layout(legend=dict(\n",
    "    orientation=\"h\",\n",
    "    yanchor=\"bottom\",\n",
    "    y=1.02,\n",
    "    xanchor=\"right\",\n",
    "    x=1\n",
    "))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Estimate Revenue Elasticity for Store Clusters <a id=\"3.5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now incorporate these segments into the revenue elasticity regression model to estimate the revenue elasticity of demand for each segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomplish this, we will need to create dummy variables that represent each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['CLUSTER'] = pd.Series(zz, index=df.index)\n",
    "xx=pd.get_dummies(df['CLUSTER'],prefix='seg')\n",
    "df = pd.concat([df, xx], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we now have the segments expressed as dummy variables at the end of the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create interactive dummy variables.  Segment 0 is the base value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LN_P_SEG_1']=(df['LN_PRICE']*df['seg_1'])\n",
    "df['LN_P_SEG_2']=(df['LN_PRICE']*df['seg_2'])\n",
    "df['LN_P_SEG_3']=(df['LN_PRICE']*df['seg_3'])\n",
    "df['LN_P_SEG_4']=(df['LN_PRICE']*df['seg_4'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Include the interactive dummy variables into the revenue model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independentx = df[['LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n",
    "                          'LN_PERCENT_SPEAKING_SPANISH','LN_PRICE','LN_P_SEG_1','LN_P_SEG_2','LN_P_SEG_3','LN_P_SEG_4']]\n",
    "independent = sm.add_constant(independentx, prepend=False)\n",
    "dependent=df['LN_REVENUE']\n",
    "\n",
    "mod = sm.OLS(dependent, independent)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the Coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = results.params\n",
    "\n",
    "results_df = pd.DataFrame({#\"pvals\":pvals,\n",
    "                               \"coeff\":coeff,\n",
    "                               #\"conf_lower\":conf_lower,\n",
    "                               #\"conf_higher\":conf_higher\n",
    "                                })\n",
    "\n",
    "#Reordering...\n",
    "results_df = results_df[[\"coeff\"]]\n",
    "\n",
    "results_df.reset_index(level=0, inplace=True)\n",
    "\n",
    "xx=results_df.loc[(results_df['index']=='LN_PRICE') | (results_df['index']=='LN_P_SEG_1')| (results_df['index']=='LN_P_SEG_2')| (results_df['index']=='LN_P_SEG_3')\n",
    "                 | (results_df['index']=='LN_P_SEG_4')]\n",
    "xx=xx.copy()\n",
    "xx['index'] = xx['index'].astype('string')\n",
    "xx = xx.rename(columns={'index': 'THINGY'})\n",
    "\n",
    "xx['CLUSTER']=np.where((xx.THINGY==('LN_P_SEG_1')),1,\n",
    "                      np.where((xx.THINGY==('LN_PRICE')),0,\n",
    "                              np.where((xx.THINGY==('LN_P_SEG_2')),2,\n",
    "                                      np.where((xx.THINGY==('LN_P_SEG_3')),3,\n",
    "                                              np.where((xx.THINGY==('LN_P_SEG_4')),4,99)))))\n",
    "\n",
    "xx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the coefficient so that it represents the elasticity for each segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx['wookie']=1\n",
    "zz=xx[xx['THINGY']=='LN_PRICE']\n",
    "\n",
    "zz = zz.rename(columns={'coeff': 'BASE_P'})\n",
    "zz=zz[['wookie','BASE_P']]\n",
    "\n",
    "xx =xx.merge(zz, on=['wookie'], how='inner')\n",
    "\n",
    "xx['BASE_P']=np.where((xx.THINGY==('LN_PRICE')),0,xx['BASE_P'])\n",
    "\n",
    "xx['REV_ELASTICITY']=xx['coeff']+xx['BASE_P']\n",
    "qq=xx[['CLUSTER','REV_ELASTICITY']]\n",
    "qq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge Revenue Elasticity to Other segment metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.reset_index(level=0, inplace=True)\n",
    "df_output=df_output[['CLUSTER','PRICE_DELTA','R_QUANTITY']]\n",
    "df_output =df_output.merge(qq, on=['CLUSTER'], how='inner')\n",
    "df_output=df_output[['CLUSTER','PRICE_DELTA','R_QUANTITY','REV_ELASTICITY']]\n",
    "df_output= df_output.sort_values([\"REV_ELASTICITY\"])\n",
    "df_output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take another look at our Clusters.\n",
    "\n",
    "Cluster 4 includes stores with the highest revenue elasticity. This makes sense, given that it is priced less than average and is performing well.\n",
    "Cluster 0 has the second-highest revenue elasticity. It is priced higher than average and also appears to face a strong demand.\n",
    "\n",
    "The other three clusters also have a consequential revenue elasticity.\n",
    "\n",
    "There should be one thing that jumps right out as you review these elasticity numbers. Can you guess?\n",
    "\n",
    "Remember the overall price elasticity of revenue for all stores? It wasÂ .36. All of the segments above have a price elasticity of revenue greater thanÂ .36. Some should be lower, and some should be higher, right?\n",
    "\n",
    "Yeah, they should. While the five individual segments can have a higher elasticity than the five combined, it isn't likely.\n",
    "\n",
    "This anomaly allows me to make an essential point about this data and all data observed outside of a laboratory. A friend of mine once said that using a sophisticated model on data from a corporate data warehouse is like using a laser beam to slice bologna. Nothing could be more accurate. We live in an imperfect world filled with random noise and random error. You have to remember this as you \"do\" your data science. Most of the time, your models' output is directional and subject to the imperfect world that we inhabit. That doesn't mean that the work we do as data scientists are not useful; it is. Just remember that the goal is to guide the business towards better revenue, lower costs, and higher profit. Perfection is a goal, not a destination. We are subject to an imperfect world, but data science can make it better.\n",
    "\n",
    "Given this, it makes me sometimes wonder about the emphasis on prediction accuracy that I've seen in the last few years. I mean, with the Kaggle competitions and the highly complex algorithms, are we really improving things? Does improving the accuracy fromÂ .1% toÂ .01% matter? I mean, at the end of the day, aren't still just left with a sliced piece of bologna? Not always, but in many situations, I think you are.\n",
    "\n",
    "So, now what do we do? That isn't easy to say. Moving from your Jupyter notebook to the real-world is never a straightforward task, especially given the limitations of our imperfect world. Having said that, given all the information presented so far, we can take action.\n",
    "\n",
    "Here are a few things to keep in mind. One, history is essential. If the historical rate increase for a market is 1%, I wouldn't recommend implementing a 20% increase in prices. I would want to keep it consistent with what consumers are used to seeing. Two, listen to others in the organization, especially those in customer-facing roles. When it comes to antidote versus data, I am 100% in the data camp. Having said that, I don't think you can ignore those who are intimately connected to the business. For example, if a store manager is convinced his store cannot handle a price increase and your model says it is prime for one, I wouldn't go as big as I would if the store manager was indifferent or advocating strongly for a price increase.\n",
    "\n",
    "Here is an example of how you can use our analysis to set price changes. We will assume that this firm increased prices for the last few years by the inflation rate across the board.\n",
    "\n",
    "For segment 4, price is below average, quantity is way above average, and the revenue elasticity is 0.72. I would increase prices by inflation + 4%. This should yield a 2.88% real increase in revenue for these stores.\n",
    "\n",
    "For segment 0, price is above average, quantity is above average, and the revenue elasticity isÂ .59, I would increase prices by inflation + 2%. This would yield a 1.2 percent real increase in revenue.\n",
    "\n",
    "For segment 3, price is way below average, quantity is below average, and revenue elasticity isÂ .48, I would increase prices by inflation + 1%. This should yield aÂ .48% increase in real revenue.\n",
    "\n",
    "For segment 2, price is way above average, quantity is below average, and revenue elasticity isÂ .45, I would increase prices by inflation +Â .5%. This should yield aÂ .225% increase in real revenue.\n",
    "\n",
    "For segment 1, price is above average, quantity is below average, and revenue elasticity isÂ .39, I would increase prices by inflation. This should yield a 0% increase in real revenue.\n",
    "\n",
    "Again, there is no perfect answer here, but the recommendations above are solid and consistent with our analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Build Demand Curve for Each Store Cluster <a id=\"3.6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the heck of it, let's look at the demand curves for each of the segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "independentx = df[['LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME',\n",
    "                          'LN_PERCENT_SPEAKING_SPANISH','LN_PRICE','LN_P_SEG_1','LN_P_SEG_2','LN_P_SEG_3','LN_P_SEG_4']]\n",
    "independent = sm.add_constant(independentx, prepend=False)\n",
    "dependent=df['LN_QUANTITY']\n",
    "\n",
    "mod = sm.OLS(dependent, independent)\n",
    "\n",
    "results = mod.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export coefficients\n",
    "coeff = results.params\n",
    "\n",
    "results_df = pd.DataFrame({#\"pvals\":pvals,\n",
    "                               \"coeff\":coeff,\n",
    "                               #\"conf_lower\":conf_lower,\n",
    "                               #\"conf_higher\":conf_higher\n",
    "                                })\n",
    "\n",
    "#Reordering...\n",
    "results_df = results_df[[\"coeff\"]]\n",
    "results_df.reset_index(level=0, inplace=True)\n",
    "#format and rename columns\n",
    "xx=results_df.copy()\n",
    "xx['index'] = xx['index'].astype('string')\n",
    "\n",
    "xx = xx.rename(columns={'index': 'THINGY'})\n",
    "\n",
    "#transpose the results\n",
    "results_flipped = xx.transpose()\n",
    "df2=results_flipped\n",
    "header=df2.iloc[0]\n",
    "df2=df2[1:]\n",
    "df2.columns=header\n",
    "results_flipped=df2\n",
    "\n",
    "#create stand alone pricing coefficients.\n",
    "results_flipped['LN_P_SEG_1X']=results_flipped['LN_PRICE']+results_flipped['LN_P_SEG_1']\n",
    "results_flipped['LN_P_SEG_2X']=results_flipped['LN_PRICE']+results_flipped['LN_P_SEG_2']\n",
    "results_flipped['LN_P_SEG_3X']=results_flipped['LN_PRICE']+results_flipped['LN_P_SEG_3']\n",
    "results_flipped['LN_P_SEG_4X']=results_flipped['LN_PRICE']+results_flipped['LN_P_SEG_4']\n",
    "results_flipped['LN_P_SEG_0X']=results_flipped['LN_PRICE']\n",
    "#pair down the dataframe\n",
    "results_flipped=results_flipped[['LN_PERCENTAGE_OF RENTERS','LN_PERCENT_HAVING_CHILDREN','LN_INCOME','LN_PERCENT_SPEAKING_SPANISH',\n",
    "                                 'LN_P_SEG_1X','LN_P_SEG_2X','LN_P_SEG_3X','LN_P_SEG_4X','LN_P_SEG_0X','const']].copy()\n",
    "\n",
    "#create a flag to join \n",
    "results_flipped['chachacha']=1\n",
    "#rename columns\n",
    "results_flipped = results_flipped.rename(columns={'LN_PERCENTAGE_OF RENTERS': 'B_LN_PERCENTAGE_OF_RENTERS','LN_PERCENT_HAVING_CHILDREN':'B_LN_PERCENT_HAVING_CHILDREN',\n",
    "                                                 'LN_INCOME':'B_LN_INCOME','LN_PERCENT_SPEAKING_SPANISH':'B_LN_PERCENT_SPEAKING_SPANISH',\n",
    "                                                 'LN_P_SEG_1X':'B_LN_P_SEG_1', 'LN_P_SEG_2X':'B_LN_P_SEG_2','LN_P_SEG_3X':'B_LN_P_SEG_3',\n",
    "                                                 'LN_P_SEG_4X':'B_LN_P_SEG_4','LN_P_SEG_0X':'B_LN_P_SEG_0','const':'const'})\n",
    "\n",
    "\n",
    "#create a list to get averages for the non-price independent variables\n",
    "dingle=['PERCENTAGE_OF_RENTERS', 'PERCENT_HAVING_CHILDREN','AVERAGE_INCOME','PERCENT_SPEAKING_SPANISH','QUANTITY']\n",
    "#get averages of non-price independent variables\n",
    "df['chachacha']=1\n",
    "wookie = df.groupby(['chachacha'])[dingle].mean()\n",
    "\n",
    "wookie.reset_index(level=0, inplace=True)\n",
    "\n",
    "#merge the independent variables and coefficients into one dataframe\n",
    "\n",
    "wookie =wookie.merge(results_flipped, on=['chachacha'], how='inner')\n",
    "#create an array of prices to score\n",
    "price = [4.00,4.10,4.20,4.30,4.40,4.50,4.60,4.70,4.80,4.90,5.00,5.10,5.20,5.30,5.40,5.50,5.60,5.70,5.80,5.90,\n",
    "        6.00,6.10,6.20,6.30,6.40,6.50,6.60,6.70,6.80,6.90,7.00,7.10,7.20,7.30,7.40,7.50,7.60,7.70,7.80,7.90,\n",
    "        8.00,8.10,8.20,8.30,8.40,8.50,8.60,8.70,8.80,8.90,9.00,9.10,9.20,9.30,9.40,9.50,9.60,9.70,9.80,9.90]\n",
    "df_price=pd.DataFrame(price)\n",
    "df_price.columns = ['PRICE']\n",
    "df_price['chachacha']=1\n",
    "\n",
    "#merge prices, coefficients and non-price independent variables\n",
    "df_price =df_price.merge(wookie, on=['chachacha'], how='inner')\n",
    "\n",
    "#Create Log Transformed Variables\n",
    "df_price['LN_PRICE'] = np.log((df_price.PRICE))\n",
    "df_price['LN_INCOME'] = np.log((df_price.AVERAGE_INCOME))\n",
    "df_price['LN_PERCENT_SPEAKING_SPANISH'] = np.log((df_price.PERCENT_SPEAKING_SPANISH))\n",
    "df_price['LN_PERCENT_HAVING_CHILDREN'] = np.log((df_price.PERCENT_HAVING_CHILDREN))\n",
    "df_price['LN_PERCENTAGE_OF_RENTERS'] = np.log((df_price.PERCENTAGE_OF_RENTERS))\n",
    "\n",
    "#score the quantity variables\n",
    "\n",
    "df_price['P_LN_Q_SEG_0']=(df_price['LN_PRICE']*df_price['B_LN_P_SEG_0']+\\\n",
    "                          df_price['B_LN_PERCENTAGE_OF_RENTERS']*df_price['LN_PERCENTAGE_OF_RENTERS']+\\\n",
    "df_price['B_LN_PERCENT_HAVING_CHILDREN']*df_price['LN_PERCENT_HAVING_CHILDREN']+df_price['B_LN_INCOME']*df_price['LN_INCOME']+\\\n",
    "df_price['B_LN_PERCENT_SPEAKING_SPANISH']*df_price['LN_PERCENT_SPEAKING_SPANISH']+df_price['const']).astype(float)\n",
    "\n",
    "df_price['P_LN_Q_SEG_1']=(df_price['LN_PRICE']*df_price['B_LN_P_SEG_1']+df_price['B_LN_PERCENTAGE_OF_RENTERS']*df_price['LN_PERCENTAGE_OF_RENTERS']+\\\n",
    "df_price['B_LN_PERCENT_HAVING_CHILDREN']*df_price['LN_PERCENT_HAVING_CHILDREN']+df_price['B_LN_INCOME']*df_price['LN_INCOME']+\\\n",
    "df_price['B_LN_PERCENT_SPEAKING_SPANISH']*df_price['LN_PERCENT_SPEAKING_SPANISH']+df_price['const']).astype(float)\n",
    "\n",
    "df_price['P_LN_Q_SEG_2']=(df_price['LN_PRICE']*df_price['B_LN_P_SEG_2']+df_price['B_LN_PERCENTAGE_OF_RENTERS']*df_price['LN_PERCENTAGE_OF_RENTERS']+\\\n",
    "df_price['B_LN_PERCENT_HAVING_CHILDREN']*df_price['LN_PERCENT_HAVING_CHILDREN']+df_price['B_LN_INCOME']*df_price['LN_INCOME']+\\\n",
    "df_price['B_LN_PERCENT_SPEAKING_SPANISH']*df_price['LN_PERCENT_SPEAKING_SPANISH']+df_price['const']).astype(float)\n",
    "\n",
    "df_price['P_LN_Q_SEG_3']=(df_price['LN_PRICE']*df_price['B_LN_P_SEG_3']+df_price['B_LN_PERCENTAGE_OF_RENTERS']*df_price['LN_PERCENTAGE_OF_RENTERS']+\\\n",
    "df_price['B_LN_PERCENT_HAVING_CHILDREN']*df_price['LN_PERCENT_HAVING_CHILDREN']+df_price['B_LN_INCOME']*df_price['LN_INCOME']+\\\n",
    "df_price['B_LN_PERCENT_SPEAKING_SPANISH']*df_price['LN_PERCENT_SPEAKING_SPANISH']+df_price['const']).astype(float)\n",
    "\n",
    "df_price['P_LN_Q_SEG_4']=(df_price['LN_PRICE']*df_price['B_LN_P_SEG_4']+df_price['B_LN_PERCENTAGE_OF_RENTERS']*df_price['LN_PERCENTAGE_OF_RENTERS']+\\\n",
    "df_price['B_LN_PERCENT_HAVING_CHILDREN']*df_price['LN_PERCENT_HAVING_CHILDREN']+df_price['B_LN_INCOME']*df_price['LN_INCOME']+\\\n",
    "df_price['B_LN_PERCENT_SPEAKING_SPANISH']*df_price['LN_PERCENT_SPEAKING_SPANISH']+df_price['const']).astype(float)\n",
    "\n",
    "df_price['P_Q_SEG_0']=np.exp(df_price['P_LN_Q_SEG_0'])\n",
    "df_price['P_Q_SEG_1']=np.exp(df_price['P_LN_Q_SEG_1'])\n",
    "df_price['P_Q_SEG_2']=np.exp(df_price['P_LN_Q_SEG_2'])\n",
    "df_price['P_Q_SEG_3']=np.exp(df_price['P_LN_Q_SEG_3'])\n",
    "df_price['P_Q_SEG_4']=np.exp(df_price['P_LN_Q_SEG_4'])\n",
    "\n",
    "#plot in two dimensions\n",
    "\n",
    "x1=df_price['PRICE']\n",
    "y1=df_price['P_Q_SEG_0']\n",
    "z1=df_price['P_Q_SEG_1']\n",
    "a1=df_price['P_Q_SEG_2']\n",
    "b1=df_price['P_Q_SEG_3']\n",
    "c1=df_price['P_Q_SEG_4']\n",
    "\n",
    "trace1 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=y1,\n",
    "    name='Segment 0'\n",
    ")\n",
    "trace2 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=z1,\n",
    "    name='Segment 1',\n",
    "    yaxis='y1'\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=a1,\n",
    "    name='Segment 2',\n",
    "    yaxis='y1'\n",
    ")\n",
    "trace4 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=b1,\n",
    "    name='Segment 3',\n",
    "    yaxis='y1'\n",
    ")\n",
    "trace5 = go.Scatter(\n",
    "    x=x1,\n",
    "    y=c1,\n",
    "    name='Segment 4',\n",
    "    yaxis='y1'\n",
    ")\n",
    "data = [trace1, trace2,trace3,trace4,trace5]\n",
    "layout = go.Layout(\n",
    "    title='Demand Curves by Segment',\n",
    "    yaxis=dict(\n",
    "        title='Quantity'\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title='Price Elasticity of Revenue',\n",
    "        titlefont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        tickfont=dict(\n",
    "            color='rgb(148, 103, 189)'\n",
    "        ),\n",
    "        overlaying='y',\n",
    "        side='right'\n",
    "    ),\n",
    "        xaxis=dict(\n",
    "        title='Price',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "))\n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope this was helpful.  Please reach out if you have any questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "\n",
    "Shad Griffin is a Certified Thought Leader and a Data Scientist at IBM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2021. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
