{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:100px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Want to do more?</span><span style=\"border: 1px solid #3d70b2;padding: 15px;float:right;margin-right:40px; color:#3d70b2; \"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "<span style=\"color:#5A6872;\"> Try out this notebook with your free trial of IBM Watson Studio.</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Loan Applicant Behavior with TensorFlow Neural Networking\n",
    "\n",
    "This notebook guides you through the basic concepts to construct a neural net\n",
    "model with the\n",
    "TensorFlow library in Watson Studio. It includes instructions on how to import the predictive\n",
    "data, train the model to predict\n",
    "client behaviors, and save the model to use for future inference.\n",
    "\n",
    "Some familiarity with Python is recommended. This notebook runs on Python 2 with\n",
    "Spark 2.1.\n",
    "\n",
    "This TensorFlow neural network tutorial has several aspects that are unique or\n",
    "not evident in other TensorFlow tutorials, such as the MNIST handwritten digits\n",
    "tutorial. The focus is on business, both in terms of the use case and data and\n",
    "in terms of extra steps that are needed to help take your data science results to\n",
    "production.\n",
    "\n",
    "## Describe the business problem\n",
    "\n",
    "This notebook describes how to use the TensorFlow library for a Neural Network\n",
    "model to predict whether a loan applicant is likely to ‘default’ on a bank loan\n",
    "based on specific client characteristics. The model must be trained over a\n",
    "serial of data batches to increase prediction accuracy.\n",
    "\n",
    "The following characteristics are considered:\n",
    "\n",
    "   * Predictor variables such as age, education, income.\n",
    "   * Differing business objectives, for example, curbing the typical defaulters,\n",
    "or filtering the best applicants only.\n",
    "\n",
    "To train the machine learning model, a sample data set is provided and run\n",
    "through in this notebook that specifies the data and\n",
    "variables.\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Import the Data](#import)<br>\n",
    "2. [Download the TensorFlow library](#download)<br>\n",
    "3. [Tuning the Hyperparameters](#tuning)<br>\n",
    "4. [Save the Model](#save-model)<br>\n",
    "5. [Derive Confidence Values](#derive)<br>\n",
    "6. [Summary](#summary)<br>\n",
    "    a. [Related Links](#rel_links)<br>\n",
    "    b. [Author information](#author)<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "You can follow along if you already installed\n",
    "Jupyter, Python and TensorFlow, or you can take a few minutes to sign up for a free\n",
    "trial of IBM Watson Studio on IBM Cloud to run the notebook on a data science\n",
    "platform <a href=\"https://datascience.ibm.com/\" target=\"_blank\">here</a>.\n",
    "</div>\n",
    "\n",
    "<a id=\"import\"></a>\n",
    "## Import the Data\n",
    "\n",
    "As a prerequisite, you must prepare a CSV file that you would like to use the\n",
    "TensorFlow neural net with.\n",
    "\n",
    "To provide an example, a sample data file that is called `bankloanData.csv` from the IBM SPSS Statistics package is extracted and is available for download here: <a href=\"https://github.com/john-boyer-phd/TensorFlow-Samples/tree/master/Neural%20Net\" target=\"_blank\">John Boyer's GitHub repo</a>. The advantages are that theresulting TensorFlow code can be easily adapted to build bigger neural nets that can learn from much larger data sets.\n",
    "\n",
    "This particular sample is selected because it is easy to use SPSS to double\n",
    "check whether the TensorFlow code is behaving as expected (for more\n",
    "information on IBM SPSS, see [Related Links](#rel-links)).\n",
    "\n",
    "The dependent variable is called the ‘label’, and the data in the column is\n",
    "called the ‘labeled data’. In this case, the dependent variable that is\n",
    "predicted is the column that is named ‘default’ in the CSV file.\n",
    "\n",
    "To read the CSV file from cloud Object Storage into a Pandas Dataframe, open the\n",
    "**Files** window by clicking the binary icon in the upper right corner and\n",
    "upload the file. Then, select the empty cell below and click Insert to code, and\n",
    "then Insert Pandas DataFrame. After the code is\n",
    "loaded, the cell can be run to read the CSV file.\n",
    "\n",
    "For larger data sets, you can prefer to use a SparkSession Dataframe instead, but\n",
    "in that case, you’ll need to slightly adjust the numpy extraction code in the\n",
    "next notebook cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "1"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿age</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>debtinc</th>\n",
       "      <th>creddebt</th>\n",
       "      <th>othdebt</th>\n",
       "      <th>default</th>\n",
       "      <th>preddef1</th>\n",
       "      <th>preddef2</th>\n",
       "      <th>preddef3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>176</td>\n",
       "      <td>9.3</td>\n",
       "      <td>11.359392</td>\n",
       "      <td>5.008608</td>\n",
       "      <td>1</td>\n",
       "      <td>0.808394</td>\n",
       "      <td>0.788640</td>\n",
       "      <td>0.213043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.362202</td>\n",
       "      <td>4.000798</td>\n",
       "      <td>0</td>\n",
       "      <td>0.198297</td>\n",
       "      <td>0.128445</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>55</td>\n",
       "      <td>5.5</td>\n",
       "      <td>0.856075</td>\n",
       "      <td>2.168925</td>\n",
       "      <td>0</td>\n",
       "      <td>0.010036</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.141023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>120</td>\n",
       "      <td>2.9</td>\n",
       "      <td>2.658720</td>\n",
       "      <td>0.821280</td>\n",
       "      <td>0</td>\n",
       "      <td>0.022138</td>\n",
       "      <td>0.010273</td>\n",
       "      <td>0.104422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>17.3</td>\n",
       "      <td>1.787436</td>\n",
       "      <td>3.056564</td>\n",
       "      <td>1</td>\n",
       "      <td>0.781588</td>\n",
       "      <td>0.737885</td>\n",
       "      <td>0.436903</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ﻿age  ed  employ  address  income  debtinc   creddebt   othdebt default  \\\n",
       "0    41   3      17       12     176      9.3  11.359392  5.008608       1   \n",
       "1    27   1      10        6      31     17.3   1.362202  4.000798       0   \n",
       "2    40   1      15       14      55      5.5   0.856075  2.168925       0   \n",
       "3    41   1      15       14     120      2.9   2.658720  0.821280       0   \n",
       "4    24   2       2        0      28     17.3   1.787436  3.056564       1   \n",
       "\n",
       "   preddef1  preddef2  preddef3  \n",
       "0  0.808394  0.788640  0.213043  \n",
       "1  0.198297  0.128445  0.436903  \n",
       "2  0.010036  0.002987  0.141023  \n",
       "3  0.022138  0.010273  0.104422  \n",
       "4  0.781588  0.737885  0.436903  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @hidden_cell\n",
    "# USE 'INSERT CODE' HERE\n",
    "# This function accesses a file in your Object Storage. The definition contains your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "\n",
    "# Please change last two lines to 'df_data_1...' rather than 'df_data_2...' etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the chart that is generated, the predictor variables are as follows:\n",
    "\n",
    "- *age*\n",
    "- *ed* (level of education)\n",
    "- *employ*\n",
    "- *address* (years at current address)\n",
    "- *income* (household income in thousands)\n",
    "- *debtinc*\n",
    "- *creddebt*\n",
    "- *othdebt* (other debt in thousands)\n",
    "\n",
    "The predictor variables are also called ‘features’. The remaining columns of\n",
    "data are unnecessary for this particular model.\n",
    "\n",
    "Now, look at the details of the data columns.\n",
    "\n",
    "If the following code cells generate an error message, please recitify the last two lines of the previous import code to \n",
    "<br><br>\n",
    "*df_data_1 = pd.read_csv(body)<br>\n",
    "df_data_1.head()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "2"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "﻿age          int64\n",
       "ed            int64\n",
       "employ        int64\n",
       "address       int64\n",
       "income        int64\n",
       "debtinc     float64\n",
       "creddebt    float64\n",
       "othdebt     float64\n",
       "default      object\n",
       "preddef1    float64\n",
       "preddef2    float64\n",
       "preddef3    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "3"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[41, 3, 17, ..., 0.808394327359702, 0.7886404318214371,\n",
       "        0.21304337612811897],\n",
       "       [27, 1, 10, ..., 0.19829747615910395, 0.128445387038174,\n",
       "        0.43690300550604605],\n",
       "       [40, 1, 15, ..., 0.0100361080990023, 0.00298677834821412,\n",
       "        0.141022623460993],\n",
       "       ..., \n",
       "       [48, 1, 13, ..., 0.0301374981044824, 0.0325702625943738,\n",
       "        0.24801041775523303],\n",
       "       [35, 2, 1, ..., 0.26900345101699397, 0.37854649636973203,\n",
       "        0.181814378077261],\n",
       "       [37, 1, 20, ..., 0.006397812918809229, 0.0111731232851226,\n",
       "        0.30304155578236497]], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data_1.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next couple of cells prunes the data. The code assumes that a\n",
    "Pandas dataframe that are named `df_data_1` exists and uses it to extract the data into\n",
    "numpy arrays that are needed as input to the TensorFlow API. The comprehension in the\n",
    "first np.array() removes instances (rows) that are missing a label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "4"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make a numpy array from the dataframe, except remove rows with no value for 'default'\n",
    "i = list(df_data_1.columns.values).index('default')\n",
    "data = np.array([x for x in df_data_1.values if x[i] in ['0', '1']])\n",
    "\n",
    "# Remove the columns for preddef1, predef2 and preddef3\n",
    "data = np.delete(data, slice(9,12), axis=1)\n",
    "\n",
    "# Separate the 'predictors' (aka 'features') from the dependent variable (aka 'label') \n",
    "# that we will learn how to predict\n",
    "predictors = np.delete(data, 8, axis=1)\n",
    "dependent = np.delete(data, slice(0, 8), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell reshapes the data slightly, with the following steps:\n",
    "\n",
    "1. Separate predictors from the dependent variable.\n",
    "2. Convert labeled data from type string to type integer.\n",
    "3. Flatten the dependent array to one dimension to match the shape of the data\n",
    "that comes from the neural network output layer.\n",
    "4. Convert the predictor type to Float to facilitate matrix multiplication with\n",
    "weights and biases within the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "5"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(700, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the label type to numeric categorical representing the classes to predict (binary classfier)\n",
    "dependent = dependent.astype(int)\n",
    "\n",
    "# And flatten it to one dimensional for use as the expected output label vector in TensorFlow\n",
    "dependent = dependent.flatten()\n",
    "dependent\n",
    "\n",
    "# Convert all the predictors to float to simplify this demo TensorFlow code\n",
    "predictors = predictors.astype(float)\n",
    "\n",
    "# Get the shape of the predictors\n",
    "m, n = predictors.shape\n",
    "m, n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "6"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  41.      ,    3.      ,   17.      ,   12.      ,  176.      ,\n",
       "          9.3     ,   11.359392,    5.008608])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peak at the first sample of the predictors\n",
    "predictors[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "7"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "       0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a peak at the dependent variable values\n",
    "dependent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you train the model, the feature values of the instances (rows) of data are\n",
    "input to the neural net, and the weights and biases of the neural network are\n",
    "adjusted to minimize ‘loss’, which coarsely maps to maximizing accuracy of the\n",
    "neural network’s output layer predictions of the labeled data.\n",
    "\n",
    "<a id=\"random\"></a>\n",
    "### Randomize the training data\n",
    "\n",
    "The next cell takes the first 500 instances as training data, and leaves the\n",
    "remaining 200 instances as a test set. This particular sample is already\n",
    "randomly enumerated, so there is no need to randomly select the training and\n",
    "test sets from the data. The code also chooses about a 70:30 percent split\n",
    "for training and testing. Though, it does not round the split sample to a size\n",
    "divisible by the training batch size, which is defined later in the notebook.\n",
    "\n",
    "This cell also defines a method that returns batch-sized slices of the training\n",
    "data. If the training data is too large to fit in memory, then this method can\n",
    "instead load data one batch at a time, such as with an SQL query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "8"
    }
   },
   "outputs": [],
   "source": [
    "# Partition the input data into a training set and a test set\n",
    "\n",
    "m_train = 500\n",
    "m_test = m - m_train\n",
    "\n",
    "predictors_train = predictors[:m_train]\n",
    "dependent_train = dependent[:m_train]\n",
    "\n",
    "predictors_test = predictors[m_train:]\n",
    "dependent_test = dependent[m_train:]\n",
    "\n",
    "# Gets a batch of the training data. \n",
    "# NOTE: Rather than loading a whole large data set as above and then taking array slices as done here, \n",
    "#       This method can connect to a data source and select just the batch needed.\n",
    "def get_training_batch(batch_num, batch_size):\n",
    "    lower = batch_num * (m_train // batch_size)\n",
    "    upper = lower + batch_size\n",
    "    return predictors_train[lower:upper], dependent_train[lower:upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"download\"></a>\n",
    "## Download the TensorFlow library\n",
    "\n",
    "Now you are set to start the training with some actual TensorFlow code. The next\n",
    "cell imports the TensorFlow library and makes a few initializations. After, it\n",
    "defines a method that will build a neural network layer of any size, connect it\n",
    "to a preceding layer, and set the output activation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "9"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Make this notebook's output stable across runs\n",
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# A method to build a new neural net layer of a given size,  \n",
    "# fully connect it to a given preceding layer X, and \n",
    "# compute its output Z either with or without (default) an activation function\n",
    "# Call with activation=tf.nn.relu or tf.nn.sigmoid or tf.nn.tanh, for examples\n",
    "\n",
    "def make_nn_layer(layer_name, layer_size, X, activation=None):\n",
    "    with tf.name_scope(layer_name):\n",
    "        X_size = int(X.get_shape()[1])\n",
    "        SD = 2 / np.sqrt(X_size)\n",
    "        weights = tf.truncated_normal((X_size, layer_size), dtype=tf.float64, stddev=SD)\n",
    "        W = tf.Variable(weights, name='weights')\n",
    "        b = tf.Variable(tf.zeros([layer_size], dtype=tf.float64), name='biases')\n",
    "        Z = tf.matmul(X, W) + b\n",
    "        if activation is not None:\n",
    "            return activation(Z)\n",
    "        else:\n",
    "            return Z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next section is where most of the tuning of hyperparameters occurs.\n",
    "\n",
    "<a id=\"tuning\"></a>\n",
    "## Tuning network structure and activation function\n",
    "\n",
    "Now you can add the code cell that builds the neural network structure. In this\n",
    "case, you have one input layer `X`, one hidden layer `hidden1`, and one output\n",
    "layer `outputs`. The comments in the following code cell explain how to add more\n",
    "hidden layers, but with this sample data, it is possible to learn everything\n",
    "with only one layer. The output layer has two nodes, one for outputting class 0\n",
    "(the loan applicant will not default) and the other for class 1 (the loan applicant\n",
    "will default). The `y` variable will be used during tis usedtore the\n",
    "labeled data that is expected to match with the output layer.\n",
    "\n",
    "One line of code that helps makes this tutorial unique is the one that creates a\n",
    "tf.identity() node that gives the name `nn_output`. This allows you to save a\n",
    "name for the output layer so that you can recover and use the output layer after\n",
    "a restore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "10"
    }
   },
   "outputs": [],
   "source": [
    "# Make the neural net structure\n",
    "\n",
    "n_inputs = n\n",
    "n_hidden1 = n \n",
    "### n_hidden2 = n // 2\n",
    "n_outputs = 2   # Two output classes: defaulting or non-defaulting on loan\n",
    "\n",
    "X = tf.placeholder(tf.float64, shape=(None, n_inputs), name='X')\n",
    "\n",
    "with tf.name_scope('nn'):\n",
    "    hidden1 = make_nn_layer('hidden1', n_hidden1, X, activation=tf.nn.relu)\n",
    "    hidden2 = hidden1\n",
    "    ### hidden2 = make_nn_layer('hidden2', n_hidden2, hidden1, activation=tf.nn.relu)\n",
    "    outputs = make_nn_layer('outputs', n_outputs, hidden2) \n",
    "    outputs = tf.identity(outputs, \"nn_output\")\n",
    "    \n",
    "y = tf.placeholder(tf.int64, shape=(None), name='y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input parameters that are passed to a neural network during inference are the feature\n",
    "values. During training, the input parameters are feature values and the\n",
    "expected output are labeled data. But the neural network is adaptable beyond\n",
    "previously mentioned input parameters, and these configurable parts are called\n",
    "hyperparameters. The hyperparameters contain the number and size of the hidden\n",
    "layers and the activation function.  You can try other numbers and sizes of\n",
    "hidden layers. ‘tanh’ and ‘sigmoid’ are other activation functions that you can try.\n",
    "However, the default configuration works best on this data.\n",
    "\n",
    "What you accomplished so far in this notebook is create the main part of a\n",
    "TensorFlow compute graph that has the shape for a neural network. Next,\n",
    "you must attach two different root nodes to the output layer, one that adds\n",
    "functions for training and the other for testing, like in the following\n",
    "cell. The ‘training_op’ uses the gradient descent method for minimizing loss (of\n",
    "perfect confidence in the correct answers and zero confidence in incorrect\n",
    "answers, where the correct answers are provided by the labeled data that are\n",
    "in ‘y’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "11"
    }
   },
   "outputs": [],
   "source": [
    "# Define how the neural net will learn\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=outputs)\n",
    "    loss = tf.reduce_mean(xentropy, name='l')\n",
    "    \n",
    "learning_rate = 0.01\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    \n",
    "with tf.name_scope(\"test\"):\n",
    "    correct = tf.nn.in_top_k(tf.cast(outputs, tf.float32), y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save-dir\"></a>\n",
    "### Create save directories\n",
    "\n",
    "Now you must run this cell that sets up your ability to save the model when it\n",
    "is trained. You need to do these mkdir commands the first time that you run\n",
    "this notebook only, so you might want to put them in a separate cell to make it\n",
    "easier to skip them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "12"
    }
   },
   "outputs": [],
   "source": [
    "# Set up the ability to save and restore the trained neural net...\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"save\"></a>\n",
    "## Train the Data\n",
    "\n",
    "Now you can run the magic notebook cell that trains and saves the trained model.\n",
    "Each epoch of training exposes the neural net to the entire set of training\n",
    "data. When you run the code, you can see the accuracy of results increase over\n",
    "the many epochs of runs, just as biological neural networks learn through\n",
    "repetition. For each epoch, you run through the training data in batches to\n",
    "simulate how you’d handle a larger training set. Each batch of features and\n",
    "corresponding labeled data is fed to the `training_op` root node in the compute\n",
    "graph, which is run by *training_session.run()*.\n",
    "\n",
    "One aspect of this tutorial that is unique (relative to other tutorials) is the\n",
    "randomization of the training data that takes place at the beginning of each\n",
    "epoch.  This essentially drives different data into the batches in each epoch,\n",
    "which dramatically improves accuracy over a larger number of epochs (though it\n",
    "is much easier programmatically to do this randomization when all data fits into\n",
    "memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "14"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 'Training accuracy:', 0.75999999, 'Testing accuracy:', 0.755)\n",
      "(200, 'Training accuracy:', 0.77399999, 'Testing accuracy:', 0.79000002)\n",
      "(300, 'Training accuracy:', 0.79799998, 'Testing accuracy:', 0.81999999)\n",
      "(400, 'Training accuracy:', 0.79799998, 'Testing accuracy:', 0.83499998)\n",
      "(500, 'Training accuracy:', 0.80800003, 'Testing accuracy:', 0.82499999)\n",
      "(600, 'Training accuracy:', 0.796, 'Testing accuracy:', 0.815)\n",
      "(700, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.81)\n",
      "(800, 'Training accuracy:', 0.78799999, 'Testing accuracy:', 0.815)\n",
      "(900, 'Training accuracy:', 0.81, 'Testing accuracy:', 0.815)\n",
      "(1000, 'Training accuracy:', 0.81999999, 'Testing accuracy:', 0.81)\n",
      "(1100, 'Training accuracy:', 0.81800002, 'Testing accuracy:', 0.81)\n",
      "(1200, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.80500001)\n",
      "(1300, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.815)\n",
      "(1400, 'Training accuracy:', 0.81199998, 'Testing accuracy:', 0.81999999)\n",
      "(1500, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.80000001)\n",
      "(1600, 'Training accuracy:', 0.81599998, 'Testing accuracy:', 0.81)\n",
      "(1700, 'Training accuracy:', 0.80599999, 'Testing accuracy:', 0.81)\n",
      "(1800, 'Training accuracy:', 0.82200003, 'Testing accuracy:', 0.82499999)\n",
      "(1900, 'Training accuracy:', 0.80199999, 'Testing accuracy:', 0.80500001)\n",
      "(2000, 'Training accuracy:', 0.82999998, 'Testing accuracy:', 0.81999999)\n",
      "(2100, 'Training accuracy:', 0.81199998, 'Testing accuracy:', 0.81)\n",
      "(2200, 'Training accuracy:', 0.824, 'Testing accuracy:', 0.82499999)\n",
      "(2300, 'Training accuracy:', 0.80400002, 'Testing accuracy:', 0.815)\n",
      "(2400, 'Training accuracy:', 0.81800002, 'Testing accuracy:', 0.815)\n",
      "(2500, 'Training accuracy:', 0.82800001, 'Testing accuracy:', 0.82499999)\n",
      "(2600, 'Training accuracy:', 0.82999998, 'Testing accuracy:', 0.815)\n",
      "(2700, 'Training accuracy:', 0.82200003, 'Testing accuracy:', 0.815)\n",
      "(2800, 'Training accuracy:', 0.81999999, 'Testing accuracy:', 0.80500001)\n",
      "(2900, 'Training accuracy:', 0.81400001, 'Testing accuracy:', 0.82999998)\n",
      "(3000, 'Training accuracy:', 0.82599998, 'Testing accuracy:', 0.82499999)\n",
      "\n",
      "('Actual classes:   ', array([0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]))\n",
      "('Predicted classes:', array([1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n"
     ]
    }
   ],
   "source": [
    "# TRAINING TIME\n",
    "\n",
    "# This is how many times to use the full set of training data\n",
    "n_epochs = 3000\n",
    "\n",
    "# For a larger training set, it's typically necessary to break training into\n",
    "# batches so only the memory needed to store one batch of training data is used\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as training_session:\n",
    "    init.run()\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        # Shuffling (across batches) is easier to do for small data sets and\n",
    "        # helps increase accuracy\n",
    "        training_set = [[pt_elem, dependent_train[i]] for i, pt_elem in enumerate(predictors_train)]\n",
    "        np.random.shuffle(training_set)\n",
    "        predictors_train = [ts_elem[0] for ts_elem in training_set]\n",
    "        dependent_train = [ts_elem[1] for ts_elem in training_set]\n",
    "        \n",
    "        # Loop through the whole training set in batches\n",
    "        for batch_num in range(m_train // batch_size):\n",
    "            X_batch, y_batch = get_training_batch(batch_num, batch_size)\n",
    "            training_session.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "\n",
    "        if epoch % 100 == 99:\n",
    "            acc_train = accuracy.eval(feed_dict={X: predictors_train, y: dependent_train})\n",
    "            acc_test = accuracy.eval(feed_dict={X: predictors_test, y: dependent_test})\n",
    "            print(epoch+1, \"Training accuracy:\", acc_train, \"Testing accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(training_session, \"../datasets/Neural Net/Neural Net.ckpt\")\n",
    "    \n",
    "    # A quick test with the trained model \n",
    "    Z = outputs.eval(feed_dict={X: predictors_test[:20]})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    print(\"\")\n",
    "    print(\"Actual classes:   \", dependent_test[:20])  \n",
    "    print(\"Predicted classes:\", dependent_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"><b>Author's Sidebar:</b>\n",
    "When you do business with a real stakeholder customer, you need to have a second\n",
    "test set, often called a validation set or a blind set. Why do you need a second\n",
    "test set? The typical reply might be akin to, “I don’t know, to\n",
    "double-check accuracy?”  Well, that is not incorrect. But if you look at the structure of\n",
    "training, the weights and biases are affected not just by the training data.\n",
    "Indirectly, they are also affected by the test data because you choose\n",
    "<i>n_epochs</i> to keep running training epochs until you get the best accuracy\n",
    "on the test set. In other words, you're teaching to the test. The validation set\n",
    "or blind set has no such indirect effect on the weights and biases that are computed for\n",
    "the neural network. It is another test set that to ensure construct\n",
    "validity, must be randomly from the same pool of data that the training set\n",
    "and test set are randomly selected from. In this way, the validation set is not\n",
    "just the ‘final exam’, it’s the first experience of the real world.\n",
    "</div>\n",
    "\n",
    "<a id=\"save-model\"></a>\n",
    "## Save and Restore models for a Production Environment\n",
    "\n",
    "When all training is done, the model is saved into the data sets subdirectory that is created previously. For larger training data sets, you can call on\n",
    "TensorFlow to save on checkpoints throughout the epoch-based training so that\n",
    "you can stop and resume training if needed. The list of the model files is\n",
    "generated by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "15"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\r\n",
      "-rw------- 1 s646-4d27fbe1c5e9eb-6e7f59c73e81 users    87 Feb 21 14:02 checkpoint\r\n",
      "-rw------- 1 s646-4d27fbe1c5e9eb-6e7f59c73e81 users   720 Feb 21 14:02 Neural Net.ckpt.data-00000-of-00001\r\n",
      "-rw------- 1 s646-4d27fbe1c5e9eb-6e7f59c73e81 users   238 Feb 21 14:02 Neural Net.ckpt.index\r\n",
      "-rw------- 1 s646-4d27fbe1c5e9eb-6e7f59c73e81 users 25750 Feb 21 14:02 Neural Net.ckpt.meta\r\n"
     ]
    }
   ],
   "source": [
    "# List the model files\n",
    "!ls -l \"../datasets/Neural Net\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data files that TensorFlow created during that save operation can be\n",
    "transported to a production environment. The neural network can then be restored\n",
    "by using the code in the next cell, and the input and output layers can be obtained\n",
    "and used for inference, by using *get_tensor_by_name()*.\n",
    "\n",
    "The next cell also shows how to reference into the hierarchy of a name scope to get the output layer from within the neural network structure. It mocks\n",
    "up by having a REST API receive a batch of feature instances and converting them\n",
    "to a numpy array by taking a slice of the test data.  With the inference\n",
    "TensorFlow session, the compute graph and the values it contained are restored.\n",
    "After that, you obtain the tensors corresponding to the neural network input and output\n",
    "layers by using the names that you previously assigned.  Then, you run the output\n",
    "layer, and give the batch of feature instances to the input layer `X`\n",
    "(*inference_session.run()*). The predictions of the dependent variable are then\n",
    "obtained by choosing whichever of the two output layer nodes that have the higher\n",
    "value (by using *np.argmax()*)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "16"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../datasets/Neural Net/Neural Net.ckpt\n",
      "('Actual classes:   ', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1]))\n",
      "('Predicted classes:', array([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1]))\n"
     ]
    }
   ],
   "source": [
    "# Restore the saved model and use it to perform inference on a \"received\" new set of data\n",
    "\n",
    "# We will simulate \"receiving\" the new data by taking some slice of the test set (all in this case).\n",
    "predictors_received = predictors_test[:]\n",
    "\n",
    "import tensorflow as tf_inference\n",
    "\n",
    "with tf_inference.Session() as inference_session:\n",
    "    inf_saver = tf_inference.train.import_meta_graph('../datasets/Neural Net/Neural Net.ckpt.meta')\n",
    "    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../datasets/Neural Net/'))\n",
    "    \n",
    "    graph = tf_inference.get_default_graph()\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n",
    "\n",
    "    Z = inference_session.run(nn_output, feed_dict={X: predictors_received})\n",
    "    dependent_pred = np.argmax(Z, axis=1)\n",
    "    \n",
    "print(\"Actual classes:   \", dependent_test[20:40])\n",
    "print(\"Predicted classes:\", dependent_pred[20:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer learning\n",
    "\n",
    "As another unique aspect of this tutorial, you can also use this\n",
    "method of naming with <i>tf.identity</i> and then getting the tensor from a\n",
    "restored graph to transfer learning between neural nets. Specifically, when you\n",
    "create a hidden layer with <i>make_nn_layer()</i>, you can name it with\n",
    "<i>tf.identity</i>. Then, you train and save as shown above. To transfer to a\n",
    "second neural net, you restore the trained and saved one, get the hidden layer\n",
    "by name, attach alternative fully connected hidden layers as needed, and an\n",
    "alternative output layer, and then train the new second neural network by using the method above.\n",
    "\n",
    "<a id=\"derive\"></a>\n",
    "## Derive confidence values for neural net output\n",
    "\n",
    "Finally, to showcase more feature that makes this tutorial unique, you can look\n",
    "at how to get the actual confidence values for the predictions. This might not\n",
    "seem as important when you do the MNIST hand-written digit tutorial, but\n",
    "in a business context it’s important to know how much confidence we have in an\n",
    "answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "17"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../datasets/Neural Net/Neural Net.ckpt\n",
      "('Confidences: ', [0.72115788841227291, 0.80607047578305713, 0.83934041973914708, 0.99522251895376912, 0.63583707224033514, 0.92301370061024191, 0.9246727235093638, 0.78439759502501394, 0.99999999999583422, 0.64561868763996899, 0.72736408931601482, 0.64561868763996899, 0.83959328566375235, 0.94291801514712859, 0.68170960394665236, 0.99981345698850999, 0.96778595641239318, 0.59248611260528017, 0.99293831799558985, 0.5220483545876663])\n",
      "\n",
      "('Probabilities: ', array([[  7.21157888e-01,   2.78842112e-01],\n",
      "       [  8.06070476e-01,   1.93929524e-01],\n",
      "       [  8.39340420e-01,   1.60659580e-01],\n",
      "       [  9.95222519e-01,   4.77748105e-03],\n",
      "       [  6.35837072e-01,   3.64162928e-01],\n",
      "       [  9.23013701e-01,   7.69862994e-02],\n",
      "       [  9.24672724e-01,   7.53272765e-02],\n",
      "       [  7.84397595e-01,   2.15602405e-01],\n",
      "       [  1.00000000e+00,   4.16586334e-12],\n",
      "       [  3.54381312e-01,   6.45618688e-01],\n",
      "       [  7.27364089e-01,   2.72635911e-01],\n",
      "       [  3.54381312e-01,   6.45618688e-01],\n",
      "       [  8.39593286e-01,   1.60406714e-01],\n",
      "       [  9.42918015e-01,   5.70819849e-02],\n",
      "       [  6.81709604e-01,   3.18290396e-01],\n",
      "       [  9.99813457e-01,   1.86543011e-04],\n",
      "       [  9.67785956e-01,   3.22140436e-02],\n",
      "       [  4.07513887e-01,   5.92486113e-01],\n",
      "       [  9.92938318e-01,   7.06168200e-03],\n",
      "       [  4.77951645e-01,   5.22048355e-01]]))\n"
     ]
    }
   ],
   "source": [
    "with tf_inference.Session() as inference_session:\n",
    "    inf_saver = tf_inference.train.import_meta_graph('../datasets/Neural Net/Neural Net.ckpt.meta')\n",
    "    inf_saver.restore(inference_session, tf_inference.train.latest_checkpoint('../datasets/Neural Net/'))\n",
    "    \n",
    "    graph = tf_inference.get_default_graph()\n",
    "    X = graph.get_tensor_by_name(\"X:0\")\n",
    "    nn_output = graph.get_tensor_by_name(\"nn/nn_output:0\")\n",
    "\n",
    "    dependent_prob = inference_session.run(tf_inference.nn.softmax(nn_output), feed_dict={X: predictors_received})\n",
    "\n",
    "confidences = [p[dependent_pred[i]] for i, p in enumerate(dependent_prob)]\n",
    "    \n",
    "print(\"Confidences: \", confidences[20:40])\n",
    "print(\"\")\n",
    "print(\"Probabilities: \", dependent_prob[20:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the confidence values, a new root node was inserted onto the output layer\n",
    "to apply the *softmax* function. Now the compute graph produces the probability\n",
    "of occurrence for each output value. Then, one final comprehension was done to\n",
    "ferret out the confidences of the predicted labels for each feature instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "18"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.85273972602739712"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we're going to assess the quality of the neural net using ROC curve and AUC\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# send the actual dependent variable classifications for param 1, \n",
    "# and the confidences of the true classification for param 2.\n",
    "FPR, TPR, _ = roc_curve(dependent_test, dependent_prob[:, 1])\n",
    "\n",
    "# Calculate the area under the confidence ROC curve.\n",
    "# This area is equated with the probability that the classifier will rank \n",
    "# a randomly selected defaulter higher than a randomly selected non-defaulter.\n",
    "AUC = auc(FPR, TPR)\n",
    "\n",
    "# What is \"good\" can dependm but an AUC of 0.7+ is generally regarded as good, \n",
    "# and 0.8+ is generally regarded as being excellent \n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "19"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niIhEpQQhIiJR/X+AGrZon3KJJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f086c037b10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we'll plot the confidence ROC curve \n",
    "plt.figure()\n",
    "plt.plot(FPR, TPR, label='ROC curve (area = %0.2f)' % AUC)\n",
    "plt.plot([0, 1], [0, 1], 'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.02])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run a classification report to see how the model is behaving within\n",
    "each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "20"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.91      0.88       146\n",
      "          1       0.71      0.59      0.65        54\n",
      "\n",
      "avg / total       0.82      0.82      0.82       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(dependent_test, dependent_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the recall on non-defaulters is 91%, which means the model\n",
    "allows most non-defaulters proceed to getting a loan. The recall for defaulters\n",
    "is only 59%, so many defaulters are flagged, but a substantial number are not.\n",
    "\n",
    "If the business objective of the model is to curb many but not all of the deficient\n",
    "applications while not perturbing business for better customers, then this is a good model.  But suppose that the objective is to have a model that lets through\n",
    "only the best applications and flags the rest for review. It might be costly to\n",
    "review many applications, but that cost might be less than the cost of making\n",
    "considerable loans that don't get repaid.\n",
    "\n",
    "To accommodate for the different objectives, this notebook calculates the\n",
    "confidence threshold that it would take to get a perfect true positive rate (that is, to\n",
    "detect all of the defaulters in the test set) even if that means you also get many false positives (that is, non-defaulters that the model classifies as\n",
    "defaulters).  If you look up at the ROC curve, you can see that a perfect 100%\n",
    "TPR happens just above 60% FPR, so the overall accuracy will be lower due to the number of false positives, but the\n",
    "confidence threshold is better from the standpoint of achieving the alternative desired business objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "21"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.043730491447763684"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Say you want a model that is very accurate at recalling true positives (defaulters), \n",
    "# even if it gets a lot a false positives (non-defaulters). You might be automatically\n",
    "# accepting the false classifications and, for true classifications, you may send them\n",
    "# for human review rather than rejecting their loan applications.\n",
    "\n",
    "# The lowest confidence that can give 100% TPR on the test set is equal to the \n",
    "# true class with the lowest confidence, so we'll find that now\n",
    "defaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 1]\n",
    "min_conf = np.min(defaulter_probs)\n",
    "min_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "attributes": {
     "classes": [],
     "id": "",
     "n": "22"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Each non-defaulter with a confidence at or above min_conf would be predicted \n",
    "# to be a defaulter (which would be a false positve prediction for a non-defaulter)\n",
    "\n",
    "non_defaulter_probs = [dependent_prob[i][1] for i, p in enumerate(dependent_test) if p == 0]\n",
    "false_positives = [x for x in non_defaulter_probs if x >= min_conf]\n",
    "\n",
    "total = len(defaulter_probs) + len(non_defaulter_probs)\n",
    "total_correct = total - len(false_positives)\n",
    "accuracy = float(total_correct) / total\n",
    "\n",
    "# Overall accuracy would suffer quite a bit, but this achieves \n",
    "# the desired high accuracy on true positive identification (defaulters)  \n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For when you want to wipe out the training and do it again\n",
    "# !rm -rf \"../datasets/Neural Net\"\n",
    "## !rm -rf \"../datasets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "## Summary\n",
    "\n",
    "You learned how to import the TensorFlow library from IBM Watson Studio, create\n",
    "a neural network with layers, train the model to accurately predict business\n",
    "behavior to achieve your business objective.\n",
    "\n",
    "<a id=\"rel_links\"></a>\n",
    "## Related Links\n",
    "\n",
    "1. <a href=\"https://datascience.ibm.com/\" target=\"_blank\">See Watson Studio</a>\n",
    "2. <a href=\"https://www.ibm.com/developerworks/community/profiles/html/profileView.do?userid=060000VMNY&lang=en\" target=\"_blank\">Author's Blog on IBM Developer Works</a>\n",
    "3. <a href=\"https://www.ibm.com/developerworks/community/blogs/JohnBoyer/entry/Measuring_the_Quality_of_a_TensorFlow_Neural_Network_An_IBM_Data_Science_Experience?lang=en\" target=\"_blank\">Author Blog: Measuring the Quality of a TensorFlow Neural Network</a>\n",
    "4. <a href=\"https://www.ibm.com/ca-en/marketplace/spss-statistics\" target=\"_blank\">IBM SPSS Statistics</a>\n",
    "\n",
    "\n",
    "<a id=\"author\"></a>\n",
    "### Author\n",
    "\n",
    "John M. Boyer, IBM Global Chief Data Office\n",
    "\n",
    "<hr>\n",
    "Copyright © IBM Corp. 2018. This notebook and its source code are released under\n",
    "the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2 with Spark 2.1",
   "language": "python",
   "name": "python2-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
