{"cells": [{"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "# Use Spark to predict credit risk with `ibm-watson-machine-learning`"}, {"metadata": {}, "cell_type": "markdown", "source": "This notebook introduces commands for model persistance to Watson Machine Learning repository, model deployment, and scoring.\n\nSome familiarity with Python is helpful. This notebook runs on Python and Spark.\n\nYou will use **German Credit Risk** dataset.\n\n\n## Learning goals\n\nThe learning goals of this notebook are:\n\n-  Load a CSV file into an Apache\u00ae Spark DataFrame.\n-  Explore data.\n-  Prepare data for training and evaluation.\n-  Persist a pipeline and model in Watson Machine Learning repository from tar.gz files.\n-  Deploy a model for online scoring using Wastson Machine Learning API.\n-  Score sample scoring data using the Watson Machine Learning API.\n-  Explore and visualize prediction result using the plotly package.\n\n\n## Contents\n\nThis notebook contains the following parts:\n\n1. [Set up](#setup)\n2. [Load and explore data](#load)\n3. [Persist model](#persistence)\n4. [Predict locally](#visualization)\n5. [Deploy and score in a Cloud](#scoring)\n6. [Clean up](#cleanup)\n7. [Summary and next steps](#summary)"}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"setup\"></a>\n## 1. Set up the environment\n\nBefore you use the sample code in this notebook, you must perform the following setup tasks:\n\n-  Create a <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance (a free plan is offered and information about how to create the instance can be found <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/admin/create-services.html?context=cpdaas&audience=wdp\" target=\"_blank\" rel=\"noopener no referrer\">here</a>)."}, {"metadata": {}, "cell_type": "markdown", "source": "### Connection to WML\n\nAuthenticate the Watson Machine Learning service on IBM Cloud. You need to provide platform `api_key` and instance `location`.\n\nYou can use [IBM Cloud CLI](https://cloud.ibm.com/docs/cli/index.html) to retrieve platform API Key and instance location.\n\nAPI Key can be generated in the following way:\n```\nibmcloud login\nibmcloud iam api-key-create API_KEY_NAME\n```\n\nIn result, get the value of `api_key` from the output.\n\n\nLocation of your WML instance can be retrieved in the following way:\n```\nibmcloud login --apikey API_KEY -a https://cloud.ibm.com\nibmcloud resource service-instance WML_INSTANCE_NAME\n```\n\nIn result, get the value of `location` from the output."}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: Your `Cloud API key` can be generated by going to the [**Users** section of the Cloud console](https://cloud.ibm.com/iam#/users). From that page, click your name, scroll down to the **API Keys** section, and click **Create an IBM Cloud API key**. Give your key a name and click **Create**, then copy the created key and paste it below. You can also get a service specific url by going to the [**Endpoint URLs** section of the Watson Machine Learning docs](https://cloud.ibm.com/apidocs/machine-learning).  You can check your instance location in your  <a href=\"https://cloud.ibm.com/catalog/services/watson-machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">Watson Machine Learning (WML) Service</a> instance details.\n\nYou can also get service specific apikey by going to the [**Service IDs** section of the Cloud Console](https://cloud.ibm.com/iam/serviceids).  From that page, click **Create**, then copy the created key and paste it below.\n\n**Action**: Enter your `api_key` and `location` in the following cell."}, {"metadata": {}, "cell_type": "code", "source": "api_key = 'PASTE YOUR PLATFORM API KEY HERE'\nlocation = 'PASTE YOUR INSTANCE LOCATION HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "wml_credentials = {\n    \"apikey\": api_key,\n    \"url\": 'https://' + location + '.ml.cloud.ibm.com'\n}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Install and import the `ibm-watson-machine-learning` package\n**Note:** `ibm-watson-machine-learning` documentation can be found <a href=\"https://cloud.ibm.com/apidocs/machine-learning\" target=\"_blank\" rel=\"noopener no referrer\">here</a>."}, {"metadata": {}, "cell_type": "code", "source": "!rm -rf /home/spark/shared/user-libs/python3.10*", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "!pip install -U --user ibm-watson-machine-learning", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from ibm_watson_machine_learning import APIClient\n\nclient = APIClient(wml_credentials)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Working with spaces\n\nFirst of all, you need to create a space that will be used for your work. If you do not have space already created, you can use [Deployment Spaces Dashboard](https://dataplatform.cloud.ibm.com/ml-runtime/spaces) to create one.\n\n- Click New Deployment Space\n- Create an empty space\n- Select Cloud Object Storage\n- Select Watson Machine Learning instance and press Create\n- Copy `space_id` and paste it below\n\n**Tip**: You can also use SDK to prepare the space for your work. More information can be found [here](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Space%20management.ipynb).\n\n**Action**: Assign space ID below"}, {"metadata": {}, "cell_type": "code", "source": "space_id = 'PASTE YOUR SPACE ID HERE'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can use `list` method to print all existing spaces."}, {"metadata": {}, "cell_type": "code", "source": "client.spaces.list(limit=10)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "To be able to interact with all resources available in Watson Machine Learning, you need to set **space** which you will be using."}, {"metadata": {}, "cell_type": "code", "source": "client.set.default_space(space_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Test Spark"}, {"metadata": {}, "cell_type": "code", "source": "try:\n    from pyspark.sql import SparkSession\nexcept:\n    print('Error: Spark runtime is missing. If you are using Watson Studio change the notebook runtime to Spark.')\n    raise", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"load\"></a>\n## 2. Load and explore data"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will load the data as an Apache\u00ae Spark DataFrame and perform a basic exploration.\n "}, {"metadata": {}, "cell_type": "markdown", "source": "The csv file for German Credit Risk is available on the same repository as this notebook. Load the file to Apache\u00ae Spark DataFrame using code below."}, {"metadata": {}, "cell_type": "code", "source": "!pip install wget", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "import os\nfrom wget import download\n\nsample_dir = 'spark_sample_model'\nif not os.path.isdir(sample_dir):\n    os.mkdir(sample_dir)\n    \nfilename = os.path.join(sample_dir, 'credit_risk_training.csv')\nif not os.path.isfile(filename):\n    filename = download('https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/data/credit_risk/credit_risk_training.csv', out=sample_dir)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "spark = SparkSession.builder.getOrCreate()\n\ndf_data = spark.read\\\n  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n  .option('header', 'true')\\\n  .option('inferSchema', 'true')\\\n  .load(filename)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Explore the loaded data by using the following Apache\u00ae Spark DataFrame methods:\n-  print schema\n-  print top ten records\n-  count all records"}, {"metadata": {}, "cell_type": "code", "source": "df_data.printSchema()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see, the data contains 21 fields. Risk field is the one we would like to predict (label)."}, {"metadata": {}, "cell_type": "code", "source": "df_data.show(n=5, truncate=False, vertical=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(\"Number of records: \" + str(df_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see, the data set contains 5000 records."}, {"metadata": {}, "cell_type": "markdown", "source": "### 2.1 Prepare data\n\nIn this subsection you will split your data into: train, test and predict datasets."}, {"metadata": {}, "cell_type": "code", "source": "splitted_data = df_data.randomSplit([0.8, 0.18, 0.02], 24)\ntrain_data = splitted_data[0]\ntest_data = splitted_data[1]\npredict_data = splitted_data[2]\n\nprint(\"Number of training records: \" + str(train_data.count()))\nprint(\"Number of testing records : \" + str(test_data.count()))\nprint(\"Number of prediction records : \" + str(predict_data.count()))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As you can see our data has been successfully split into three datasets: \n\n-  The train data set, which is the largest group, is used for training.\n-  The test data set will be used for model evaluation and is used to test the assumptions of the model.\n-  The predict data set will be used for prediction."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"persistence\"></a>\n## 3. Persist model"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will learn how to store your pipeline and model in Watson Machine Learning repository by using python client libraries."}, {"metadata": {}, "cell_type": "markdown", "source": "**Note**: Apache\u00ae Spark is required."}, {"metadata": {}, "cell_type": "markdown", "source": "#### Save training data in your Cloud Object Storage"}, {"metadata": {}, "cell_type": "markdown", "source": "ibm-cos-sdk library allows Python developers to manage Cloud Object Storage (COS)."}, {"metadata": {}, "cell_type": "code", "source": "import ibm_boto3\nfrom ibm_botocore.client import Config", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Action**: Put credentials from Object Storage Service in Bluemix here."}, {"metadata": {}, "cell_type": "code", "source": "cos_credentials = {\nPASTE YOUR COS CREDENTIALS HERE\n}\n\n# example:\n# cos_credentials = {\n#   \"apikey\": \"***\",\n#   \"cos_hmac_keys\": {\n#     \"access_key_id\": \"***\",\n#     \"secret_access_key\": \"***\"\n#   },\n#   \"endpoints\": \"https://cos-service.bluemix.net/endpoints\",\n#   \"iam_apikey_description\": \"***\",\n#   \"iam_apikey_name\": \"***\",\n#   \"iam_role_crn\": \"crn:v1:bluemix:public:iam::::serviceRole:Writer\",\n#   \"iam_serviceid_crn\": \"***\",\n#   \"resource_instance_id\": \"***\"\n# }", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "connection_apikey = cos_credentials['apikey']\nconnection_resource_instance_id = cos_credentials[\"resource_instance_id\"]\nconnection_access_key_id = cos_credentials['cos_hmac_keys']['access_key_id']\nconnection_secret_access_key = cos_credentials['cos_hmac_keys']['secret_access_key']", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Action**: Define the service endpoint we will use. <br>\n**Tip**: You can find this information in Endpoints section of your Cloud Object Storage intance's dashbord."}, {"metadata": {}, "cell_type": "code", "source": "service_endpoint = 'https://s3.us.cloud-object-storage.appdomain.cloud'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You also need IBM Cloud authorization endpoint to be able to create COS resource object."}, {"metadata": {}, "cell_type": "code", "source": "auth_endpoint = 'https://iam.cloud.ibm.com/identity/token'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "We create COS resource to be able to write data to Cloud Object Storage."}, {"metadata": {}, "cell_type": "code", "source": "cos = ibm_boto3.resource('s3',\n                         ibm_api_key_id=cos_credentials['apikey'],\n                         ibm_service_instance_id=cos_credentials['resource_instance_id'],\n                         ibm_auth_endpoint=auth_endpoint,\n                         config=Config(signature_version='oauth'),\n                         endpoint_url=service_endpoint)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now you will create bucket in COS and copy `training dataset` for model from **credit_risk_training.csv**."}, {"metadata": {}, "cell_type": "code", "source": "from uuid import uuid4\n\nbucket_uid = str(uuid4())\n\nscore_filename = \"credit_risk_training.csv\"\nbuckets = [\"credit-risk-\" + bucket_uid]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "for bucket in buckets:\n    if not cos.Bucket(bucket) in cos.buckets.all():\n        print('Creating bucket \"{}\"...'.format(bucket))\n        try:\n            cos.create_bucket(Bucket=bucket)\n        except ibm_boto3.exceptions.ibm_botocore.client.ClientError as e:\n            print('Error: {}.'.format(e.response['Error']['Message']))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "bucket_obj = cos.Bucket(buckets[0])\n\nprint('Uploading data {}...'.format(score_filename))\nwith open(filename, 'rb') as f:\n    bucket_obj.upload_fileobj(f, score_filename)\nprint('{} is uploaded.'.format(score_filename))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create connections to a COS bucket"}, {"metadata": {}, "cell_type": "code", "source": "datasource_type = client.connections.get_datasource_type_uid_by_name('bluemixcloudobjectstorage')\n\nconn_meta_props= {\n    client.connections.ConfigurationMetaNames.NAME: \"COS connection - spark\",\n    client.connections.ConfigurationMetaNames.DATASOURCE_TYPE: datasource_type,\n    client.connections.ConfigurationMetaNames.PROPERTIES: {\n        'bucket': buckets[0],\n        'access_key': connection_access_key_id,\n        'secret_key': connection_secret_access_key,\n        'iam_url': auth_endpoint,\n        'url': service_endpoint\n    }\n}\n\nconn_details = client.connections.create(meta_props=conn_meta_props)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** The above connection can be initialized alternatively with api_key and resource_instance_id.\nThe above cell can be replaced with:\n\n```\nconn_meta_props= {\n    client.connections.ConfigurationMetaNames.NAME: f\"Connection to Database - {db_name} \",\n    client.connections.ConfigurationMetaNames.DATASOURCE_TYPE: client.connections.get_datasource_type_uid_by_name(db_name),\n    client.connections.ConfigurationMetaNames.DESCRIPTION: \"Connection to external Database\",\n    client.connections.ConfigurationMetaNames.PROPERTIES: {\n        'bucket': bucket_name,\n        'api_key': cos_credentials['apikey'],\n        'resource_instance_id': cos_credentials['resource_instance_id'],\n        'iam_url': 'https://iam.cloud.ibm.com/identity/token',\n        'url': 'https://s3.us.cloud-object-storage.appdomain.cloud'\n    }\n}\n\nconn_details = client.connections.create(meta_props=conn_meta_props)\n```"}, {"metadata": {}, "cell_type": "code", "source": "connection_id = client.connections.get_uid(conn_details)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.1: Save pipeline and model"}, {"metadata": {}, "cell_type": "markdown", "source": "In this subsection you will learn how to save pipeline and model artifacts to your Watson Machine Learning instance."}, {"metadata": {}, "cell_type": "markdown", "source": "**Download pipeline and model archives**"}, {"metadata": {}, "cell_type": "code", "source": "import os\nfrom wget import download\n\nsample_dir = 'spark_sample_model'\nif not os.path.isdir(sample_dir):\n    os.mkdir(sample_dir)\n    \npipeline_filename = os.path.join(sample_dir, 'credit_risk_spark_pipeline.tar.gz')\nif not os.path.isfile(pipeline_filename):\n    pipeline_filename = download('https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/models/spark/credit-risk/model/credit_risk_spark_pipeline.tar.gz', out=sample_dir)\nmodel_filename = os.path.join(sample_dir, 'credit_risk_spark_model.gz')\nif not os.path.isfile(model_filename):\n    model_filename = download('https://github.com/IBM/watson-machine-learning-samples/raw/master/cloud/models/spark/credit-risk/model/credit_risk_spark_model.gz', out=sample_dir)", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%% md\n"}}, "cell_type": "markdown", "source": "**Store piepline and model**"}, {"metadata": {}, "cell_type": "markdown", "source": "To be able to store your Spark model, you need to provide a training data reference, this will allow to read the model schema automatically."}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "training_data_references = [\n                {\n                    \"type\": \"connection_asset\",\n                    \"connection\": {\n                        \"id\": connection_id,\n                    },\n                    \"location\": {\n                        \"bucket\": buckets[0],\n                        \"file_name\": score_filename,\n                    },\n                    \"schema\": {\n                    \"id\": \"training_schema\",\n                    \"fields\": [\n                      {\n                        \"metadata\": {},\n                        \"name\": \"CheckingStatus\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"LoanDuration\",\n                        \"nullable\": True,\n                        \"type\": \"integer\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"CreditHistory\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"LoanPurpose\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"LoanAmount\",\n                        \"nullable\": True,\n                        \"type\": \"integer\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"ExistingSavings\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"EmploymentDuration\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"InstallmentPercent\",\n                        \"nullable\": True,\n                        \"type\": \"integer\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"Sex\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"OthersOnLoan\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"CurrentResidenceDuration\",\n                        \"nullable\": True,\n                        \"type\": \"integer\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"OwnsProperty\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"Age\",\n                        \"nullable\": True,\n                        \"type\": \"integer\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"InstallmentPlans\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"Housing\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"ExistingCreditsCount\",\n                        \"nullable\": True,\n                        \"type\": \"integer\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"Job\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"Dependents\",\n                        \"nullable\": True,\n                        \"type\": \"integer\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"Telephone\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {},\n                        \"name\": \"ForeignWorker\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      },\n                      {\n                        \"metadata\": {\n                          \"modeling_role\": \"target\"\n                        },\n                        \"name\": \"Risk\",\n                        \"nullable\": True,\n                        \"type\": \"string\"\n                      }\n                    ]\n                  }\n                }\n            ]\n", "execution_count": null, "outputs": []}, {"metadata": {"pycharm": {"name": "#%%\n"}}, "cell_type": "code", "source": "published_model_details = client.repository.store_model(\n    model=model_filename, \n    meta_props={\n        client.repository.ModelMetaNames.NAME:'Credit Risk model',\n        client.repository.ModelMetaNames.TYPE: \"mllib_3.3\",\n        client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: client.software_specifications.get_id_by_name('spark-mllib_3.3'),\n        client.repository.ModelMetaNames.TRAINING_DATA_REFERENCES: training_data_references,\n        client.repository.ModelMetaNames.LABEL_FIELD: \"Risk\",\n    }, \n    training_data=train_data, \n    pipeline=pipeline_filename)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "model_id = client.repository.get_model_id(published_model_details)\nprint(model_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "client.repository.get_model_details(model_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Get saved model metadata from Watson Machine Learning."}, {"metadata": {}, "cell_type": "markdown", "source": "**Tip**: Use `client.repository.ModelMetaNames.show()` to get the list of available props."}, {"metadata": {}, "cell_type": "code", "source": "client.repository.ModelMetaNames.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### 3.2: Load model"}, {"metadata": {}, "cell_type": "markdown", "source": "In this subsection you will learn how to load back saved model from specified instance of Watson Machine Learning."}, {"metadata": {}, "cell_type": "code", "source": "loaded_model = client.repository.load(model_id)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "You can print for example model name to make sure that model has been loaded correctly."}, {"metadata": {}, "cell_type": "code", "source": "print(type(loaded_model))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"visualization\"></a>\n## 4. Predict locally"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will learn how to score test data using loaded model."}, {"metadata": {}, "cell_type": "markdown", "source": "### 4.1: Make local prediction using previously loaded model and test data"}, {"metadata": {}, "cell_type": "markdown", "source": "In this subsection you will score *predict_data* data set."}, {"metadata": {}, "cell_type": "code", "source": "predictions = loaded_model.transform(predict_data)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Preview the results by calling the *show()* method on the predictions DataFrame."}, {"metadata": {}, "cell_type": "code", "source": "predictions.show(5, vertical=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "By tabulating a count, you can see which product line is the most popular."}, {"metadata": {}, "cell_type": "code", "source": "predictions.select(\"predictedLabel\").groupBy(\"predictedLabel\").count().show(truncate=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"scoring\"></a>\n## 5. Deploy and score in a Cloud"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will learn how to create online scoring and to score a new data record using `ibm-watson-machine-learning`."}, {"metadata": {}, "cell_type": "markdown", "source": "**Note:** You can also use REST API to deploy and score.\nFor more information about REST APIs, see the [Swagger Documentation](https://cloud.ibm.com/apidocs/machine-learning?code=python#deployments-create)."}, {"metadata": {}, "cell_type": "markdown", "source": "### 5.1: Create online scoring endpoint"}, {"metadata": {}, "cell_type": "markdown", "source": "Now you can create an online scoring endpoint. "}, {"metadata": {}, "cell_type": "markdown", "source": "#### Create online deployment for published model"}, {"metadata": {}, "cell_type": "code", "source": "deployment_details = client.deployments.create(\n    model_id, \n    meta_props={\n        client.deployments.ConfigurationMetaNames.NAME: \"Credit Risk model deployment\",\n        client.deployments.ConfigurationMetaNames.ONLINE: {}\n    }\n)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "deployment_details", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now, you can send new scoring records (new data) for which you would like to get predictions. To do that, execute the following sample code: "}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "fields = [\"CheckingStatus\", \"LoanDuration\", \"CreditHistory\", \"LoanPurpose\", \"LoanAmount\", \"ExistingSavings\",\n                  \"EmploymentDuration\", \"InstallmentPercent\", \"Sex\", \"OthersOnLoan\", \"CurrentResidenceDuration\",\n                  \"OwnsProperty\", \"Age\", \"InstallmentPlans\", \"Housing\", \"ExistingCreditsCount\", \"Job\", \"Dependents\",\n                  \"Telephone\", \"ForeignWorker\"]\nvalues = [\n    [\"no_checking\", 13, \"credits_paid_to_date\", \"car_new\", 1343, \"100_to_500\", \"1_to_4\", 2, \"female\", \"none\", 3,\n     \"savings_insurance\", 46, \"none\", \"own\", 2, \"skilled\", 1, \"none\", \"yes\"],\n    [\"no_checking\", 24, \"prior_payments_delayed\", \"furniture\", 4567, \"500_to_1000\", \"1_to_4\", 4, \"male\", \"none\",\n     4, \"savings_insurance\", 36, \"none\", \"free\", 2, \"management_self-employed\", 1, \"none\", \"yes\"],\n    [\"0_to_200\", 26, \"all_credits_paid_back\", \"car_new\", 863, \"less_100\", \"less_1\", 2, \"female\", \"co-applicant\",\n     2, \"real_estate\", 38, \"none\", \"own\", 1, \"skilled\", 1, \"none\", \"yes\"],\n    [\"0_to_200\", 14, \"no_credits\", \"car_new\", 2368, \"less_100\", \"1_to_4\", 3, \"female\", \"none\", 3, \"real_estate\",\n     29, \"none\", \"own\", 1, \"skilled\", 1, \"none\", \"yes\"],\n    [\"0_to_200\", 4, \"no_credits\", \"car_new\", 250, \"less_100\", \"unemployed\", 2, \"female\", \"none\", 3,\n     \"real_estate\", 23, \"none\", \"rent\", 1, \"management_self-employed\", 1, \"none\", \"yes\"],\n    [\"no_checking\", 17, \"credits_paid_to_date\", \"car_new\", 832, \"100_to_500\", \"1_to_4\", 2, \"male\", \"none\", 2,\n     \"real_estate\", 42, \"none\", \"own\", 1, \"skilled\", 1, \"none\", \"yes\"],\n    [\"no_checking\", 33, \"outstanding_credit\", \"appliances\", 5696, \"unknown\", \"greater_7\", 4, \"male\",\n     \"co-applicant\", 4, \"unknown\", 54, \"none\", \"free\", 2, \"skilled\", 1, \"yes\", \"yes\"],\n    [\"0_to_200\", 13, \"prior_payments_delayed\", \"retraining\", 1375, \"100_to_500\", \"4_to_7\", 3, \"male\", \"none\", 3,\n     \"real_estate\", 37, \"none\", \"own\", 2, \"management_self-employed\", 1, \"none\", \"yes\"]\n]\n\npayload_scoring = {\"input_data\": [{\"fields\": fields, \"values\": values}]}\ndeployment_id = client.deployments.get_id(deployment_details)\n\nclient.deployments.score(deployment_id, payload_scoring)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"cleanup\"></a>\n## 6. Clean up"}, {"metadata": {}, "cell_type": "markdown", "source": "If you want to clean up all created assets:\n- experiments\n- trainings\n- pipelines\n- model definitions\n- models\n- functions\n- deployments\n\nplease follow up this sample [notebook](https://github.com/IBM/watson-machine-learning-samples/blob/master/cloud/notebooks/python_sdk/instance-management/Machine%20Learning%20artifacts%20management.ipynb)."}, {"metadata": {}, "cell_type": "markdown", "source": "<a id=\"summary\"></a>\n## 7. Summary and next steps     "}, {"metadata": {}, "cell_type": "markdown", "source": " You successfully completed this notebook! You learned how to use Apache Spark machine learning as well as Watson Machine Learning for model creation and deployment. Check out our [Online Documentation](https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/data-science.html?context=cpdaas&audience=wdp) for more samples, tutorials, documentation, how-tos, and blog posts. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Authors\n\n**Amadeusz Masny**, Python Software Developer in Watson Machine Learning at IBM"}, {"metadata": {}, "cell_type": "markdown", "source": "Copyright \u00a9 2020-2024 IBM. This notebook and its source code are released under the terms of the MIT License."}], "metadata": {"kernelspec": {"display_name": "Python 3.10 with Spark", "language": "python3", "name": "python310"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.10.10"}, "pycharm": {"stem_cell": {"cell_type": "raw", "metadata": {"collapsed": false}, "source": []}}}, "nbformat": 4, "nbformat_minor": 4}
