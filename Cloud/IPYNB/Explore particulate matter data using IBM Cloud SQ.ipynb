{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore particulate matter data using IBM Cloud SQL Query <br>\n",
    "\n",
    "\n",
    "\n",
    "<b>IBM Cloud SQL Query</b> is IBM's <b>serverless SQL</b> service on data in <b>Cloud Object Storage</b>. It allows you to run ANSI SQL on Parquet, CSV and  JSON data sets. It is based on Apache Spark SQL as the query engine in the background. This means you do <b>not</b> have to provision and manage your own Apache Spark instance and think about resources you need to request, but a simple Python client (like the IBM Watson Studio Notebook) is sufficient. <br><br>\n",
    "This notebook is an introduction to using the SQL Query API to run SQL statements in a programmatic way. It uses the <a href=\"https://github.com/IBM-Cloud/sql-query-clients/tree/master/Python\" target=\"_blank\" rel=\"noopener noreferrer\">ibmcloudsql</a> Python library for this purpose. The notebook also demonstrates how you can combine SQL Query with visualization libraries such as folium</b> or <b>matplotlib</b> to explore particulate matter data. \n",
    "\n",
    "The visualization has been verified to work the Firefox.  \n",
    "\n",
    "The notebook runs on Python.",
    "## Table of contents\n",
    "1. [Set up the libraries](#setup)<br>\n",
    "2. [Configure SQL Query](#configure)<br>\n",
    "3. [About the particulate matter data ](#data)<br>\n",
    "4. [Explore the particulate matter data](#sql)<br>\n",
    "    4.1 [Understand data and schema](#sqlsimple)<br>\n",
    "    4.2 [Use geo spatial library to find sensors near by](#sqlspatial)<br>\n",
    "    4.3 [Explore humidity to validate particulate matter measurements](#sqlexplore)<br>\n",
    "    4.4 [Cleanse Data to filter out valid measurements ](#sqlcleansing)<br>\n",
    "5. [Next steps](#nextsteps)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"setup\"></a> 1. Set up the libraries\n",
    " \n",
    "This notebook needs several Python libraries. If the library is not pre-installed you have to install it first. Pre-installed libraries only need to be imported.\n",
    "\n",
    "\n",
    "**ibmcloudsql** - is an IBM-provided Python library which provides a Python wrapper around the SQL Query REST API.\n",
    "\n",
    "**getpass** - is an open source Python library which provides a way to prompt the user for a value, usually a password, without echoing what they type to the console. \n",
    "\n",
    "**pandas** - is an open source, BSD-licensed library providing high-performance, easy-to-use data structures and data analysis tools for the Python programming language.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pyarrow\n",
    "!pip -q install ibmcloudsql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ibmcloudsql\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "from IPython.display import display as dp  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## <a id=\"configure\"></a> 2. Configure SQL Query\n",
    "\n",
    "1. Get an **API key** for your IBM Cloud account. Log on to the IBM Cloud console and go to <a href=\"https://cloud.ibm.com/iam/apikeys\" target=\"_blank\">Manage > Access (IAM) > API Keys</a> give the key a custom name and click the **Create** button. In the next dialog click **Show** and copy the key to your clipboard and paste it below in this notebook.\n",
    "2. Get the **instance CRN** for your SQL Query instance. Go to  <a href=\"https://cloud.ibm.com/resources\" target=\"_blank\">IBM Cloud console dashboard</a>. Make sure you have **All Resources** selected as resource group. In the section **Services** you can see your instance of SQL Query. Select the SQL Query that you want to use. In the SQL Query dashboard page that opens up there is a section called **REST API** with a button labelled **Instance CRN**. Click the button to copy the CRN into your clipboard and paste it into the cell below.\n",
    "3. You need to specify the location on Cloud Object Storage where your `query results` should be written. Go to the UI for your IBM Cloud Object Storage instance and list the buckets. From the pull down menu of the bucket you want to use to store your query results, select \"Bucket SQL URL\"  and copy the  **URL** (format `cos://<endpoint>/<bucket>/[<prefix>]`).  \n",
    "<br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#enter your IBM Cloud platform apikey \n",
    "apikey=getpass.getpass('Enter IBM Cloud API Key (leave empty to use previous one): ') or apikey\n",
    "\n",
    "#Enter your instance crn\n",
    "instancecrn_encoded=input('Enter SQL Query Instance CRN (leave empty to use previous one): ') or instancecrn_encoded\n",
    "\n",
    "#Decode the crn \n",
    "instancecrn=urllib.parse.unquote(instancecrn_encoded)\n",
    "print(instancecrn)\n",
    "\n",
    "#Enter your target url \n",
    "targeturl=input('Enter target URL for SQL results (leave empty to use previous one): ') or targeturl\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Configure ibmcloudsql.SQLQuery and log on to IBM Cloud\n",
    "\n",
    "ibmcloudsql.SQLQuery is the class which wraps the REST API. You configure it with the parameters required to log on to the IBM Cloud and talk to the SQL Query service instance created upfront. You then log on to the IBM Cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlClient = ibmcloudsql.SQLQuery(apikey, instancecrn, targeturl, client_info='SQL Query Particulate Matter Data Exploration and Cleansing Notebook')\n",
    "sqlClient.logon()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Get the link to the SQL Query Instance UI\n",
    "\n",
    "You have logged on to IBM Cloud now and the output from the cell below provides you the link to your SQL Query Instance UI.\n",
    "\n",
    "In the joblist you will find not only the queries you submitted using the UI but also the jobs you will submit using the REST API in the next steps. \n",
    "\n",
    "For each query you submitted you can investigate the result as well as the query details which may help when you want to debug your SQL. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nYour SQL Query web console link:\\n')\n",
    "sqlClient.sql_ui_link() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"data\"></a> 3. About the particulate matter data from the Open Knowledge Lab Stuttgart \n",
    " \n",
    "In 2015 the <a href=\"https://codefor.de/stuttgart/\" target=\"_blank\">Open Knowledge Lab Stuttgart</a> (OK Lab) started an effort to collect particulate matter data with low budget sensors. The OK Lab provides installation instructions for sensor cards which use the sensor types dht22 (measuring temperature and humidity) and sds011 (measuring particles less than 10 µm (P1)and less than 2.5 µm (P2) in μg/m³). In most cases these two sensor types are installed in the same location. More than 4000 sensor cards have now been installed all over Germany. The data from the sensors are collected and the OK Lab Stuttgart makes them publicly available. \n",
    "\n",
    "IBM has made the these data available on IBM Cloud Object Storage as .csv and .parquet files accessible via SQL Query using these naming conventions: \n",
    "\n",
    "    oklabdata/csv/<sensor_type>/<yyyy>/<sensor_type>-<yyyy>-<mm>.csv \n",
    "    oklabdata/parquet/<sensor_type>/<yyyy>/<mm>/part-....snappy.parquet  \n",
    "    \n",
    "In the following you will focus on data collected by these two sensor types in 2017/07. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id=\"sql\"></a> 4. Explore particulate matter data \n",
    "\n",
    "In this section, you will:\n",
    "\n",
    "- [4.1 Understand the data schema](sqlsimple)\n",
    "- [4.2 Use the geospatial library to find sensors](sqlspatial)\n",
    "- [4.3 Explore humidity to validate particulate matter measurements](sqlexplore)\n",
    "- [4.4 Cleanse the data to filter out valid measurements](sqlcleansing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sqlsimple\"></a> 4.1 Understand the data schema \n",
    "\n",
    "**Number of installed dht22 sensors in 2017/07** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sql=\"\"\"\n",
    "SELECT COUNT(sid_dht22) as count_sids_dht22 \n",
    "FROM (SELECT DISTINCT sensor_id as sid_dht22 \n",
    "FROM cos://us-geo/sql/oklabdata/parquet/dht22/2017/07/  STORED AS PARQUET)\n",
    "\"\"\"\n",
    "\n",
    "result_df = sqlClient.run_sql(sql)\n",
    "if isinstance(result_df, str):\n",
    "    print(result_df)\n",
    "else:\n",
    "    dp(result_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Number of installed sds011 sensors in 2017/07** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "tableView"
     }
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "SELECT COUNT(sid_sds011) as count_sids_sds011 \n",
    "FROM (SELECT DISTINCT sensor_id as sid_sds011 FROM cos://us-geo/sql/oklabdata/parquet/sds011/2017/07/ STORED AS PARQUET)\"\"\" \n",
    "result_df = sqlClient.run_sql(sql)\n",
    "\n",
    "if isinstance(result_df, str):\n",
    "    print(result_df)\n",
    "else:\n",
    "    dp(result_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DHT22 schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql='SELECT * FROM cos://us-geo/sql/oklabdata/parquet/dht22/2017/07/ STORED AS PARQUET LIMIT 5' \n",
    "result_df = sqlClient.run_sql(sql)\n",
    "if isinstance(result_df, str):\n",
    "    print(result_df)\n",
    "else:\n",
    "    dp(result_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SDS011 schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sql='SELECT * FROM cos://us-geo/sql/oklabdata/parquet/sds011/2017/07/ STORED AS PARQUET LIMIT 5' \n",
    "result_df = sqlClient.run_sql(sql)\n",
    "\n",
    "if isinstance(result_df, str):\n",
    "    print(result_df)\n",
    "else:\n",
    "    dp(result_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sqlspatial\"></a> 4.2  Use the geospatial library to find sensors \n",
    "\n",
    "SQL Query supports a **geospatial library**. You use this library to find the sensors close to a given location. \n",
    "\n",
    "- <a href=\"https://console.stage1.bluemix.net/docs/services/sql-query/sql-query.html#overview\" target=\"_blank\" rel=\"noopener noreferrer\">SQL query overview</a> provides information about which geospatial functions you can use with SQLQuery  \n",
    "- <a href=\"https://console.stage1.bluemix.net/docs/services/sql-query/geospatial.html#geospatial-analytic\"  target=\"_blank\" rel=\"noopener noreferrer\">Geospatial analytics</a> provides samples that use geospatial functions\n",
    "\n",
    "We used the geo coordinates of Stuttgart. But you can of course get the geo coordinates for your home location and figure out if there are sensors in your area.\n",
    "\n",
    "\n",
    "Longitude of Stuttgart: 9.1829321\n",
    "\n",
    "Latitude of Stuttgart: 48.7758459\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find sds011 sensors close to this geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "WITH prefiltered AS \n",
    "     (SELECT s.sensor_id sensor_id, s.lon lon, s.lat lat, MAX(s.lon) max_lon, MAX(s.lat) max_lat , s.location location\n",
    "      FROM cos://us-geo/sql/oklabdata/parquet/sds011/2017/07/ STORED AS PARQUET s\n",
    "      WHERE isnotnull(s.lon) AND isnotnull(s.lon)GROUP BY s.sensor_id, s.lon, s.lat, s.location\n",
    "     )\n",
    "SELECT sensor_id, ST_Distance(ST_Point(max_lon, max_lat), ST_Point(9.1829321, 48.7758459)) AS distance , location as location, max_lon as lon, max_lat as lat \n",
    "FROM prefiltered \n",
    "WHERE ST_Distance(ST_Point(max_lon, max_lat), ST_Point(9.1829321, 48.7758459)) <= 1000.0 \n",
    "ORDER BY distance asc\"\"\"\n",
    "\n",
    "\n",
    "sds011_sensors = sqlClient.run_sql(sql)\n",
    "\n",
    "if isinstance( sds011_sensors, str):\n",
    "    print(sds011_sensors)\n",
    "else:\n",
    "    dp( sds011_sensors.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show location information for the sds011 sensors \n",
    "\n",
    "**Note:** You use the visualization library **folium** to this, but first you must install it and import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and import folium library \n",
    "\n",
    "!pip -q install folium\n",
    "import folium \n",
    " \n",
    "# Create a map of the Stuttgart area including location latitude = 48.7758459, longitude = 9.1829321\n",
    "m = folium.Map(location=[48.7758459, 9.1829321], zoom_start=15)\n",
    "\n",
    "# Add a circle marker to the map at location latitude = 48.7758459, longitude = 9.1829321\n",
    "\n",
    "marker=folium.CircleMarker(location=[48.7758459, 9.1829321])\n",
    "marker.add_to(m) \n",
    "\n",
    "# Add markers to the map where sensors of type sds011 within a 1000 meter distance to location [48.7758459, 9.182932] 1are installed\n",
    "\n",
    "for i in range(0,len(sds011_sensors)):\n",
    "    marker=folium.Marker([sds011_sensors.iloc[i]['lat'],  sds011_sensors.iloc[i]['lon']], popup=str( sds011_sensors.iloc[i]['sensor_id']),icon=folium.Icon(color='blue'))\n",
    "    marker.add_to(m)\n",
    "\n",
    "\n",
    "m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find dht22 sensors close to this geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "WITH prefiltered AS \n",
    "     (SELECT d.sensor_id sensor_id, d.lon lon, d.lat lat, MAX(d.lon) max_lon, MAX(d.lat) max_lat , d.location location\n",
    "      FROM cos://us-geo/sql/oklabdata/parquet/dht22/2017/07/ STORED AS PARQUET d\n",
    "      WHERE isnotnull(d.lon) AND isnotnull(d.lon)GROUP BY d.sensor_id, d.lon, d.lat, d.location\n",
    "     )\n",
    "SELECT sensor_id, ST_Distance(ST_Point(max_lon, max_lat), ST_Point(9.1829321, 48.7758459)) AS distance , location as location, max_lon as lon, max_lat as lat \n",
    "FROM prefiltered \n",
    "WHERE ST_Distance(ST_Point(max_lon, max_lat), ST_Point(9.1829321, 48.7758459)) <= 1000.0 \n",
    "ORDER BY distance asc\"\"\"\n",
    "\n",
    "\n",
    "dht22_sensors = sqlClient.run_sql(sql)\n",
    "\n",
    "if isinstance(dht22_sensors , str):\n",
    "    print(dht22_sensors)\n",
    "else:\n",
    "    dp(dht22_sensors.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show location information for the dht22 sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add markers to the map where sensors of type dht22 within a 1000 meter distance to location [48.7758459, 9.182932] are installed\n",
    "\n",
    "for i in range(0,len(dht22_sensors)):\n",
    "    marker=folium.Marker([dht22_sensors.iloc[i]['lat'], dht22_sensors.iloc[i]['lon']], popup=str(dht22_sensors.iloc[i]['sensor_id']), icon=folium.Icon(color='red'))\n",
    "    marker.add_to(m)\n",
    "\n",
    "m "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's work with the sensors <b>sensor_id 809</b> (sds011) and <b>sensor_id 810 </b> (dht22) which are closest to the geolocation and installed in the same <b>location 387</b>.\n",
    "For data visualization, you will use the matplotlib librarie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sqlexplore\"></a> 4.3 Explore humidity to validate particulate matter measurements\n",
    "\n",
    "The sds011 sensor data sheet states that it operates correctly when the humidity is less than 70%. When the humidity is higher it will condense as water at the particles and the particles will appear larger than they in fact are.\n",
    "\n",
    "So let's have a look at the hourly minimal, maximal, and average humidity values for different days in July. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "SELECT min(d.humidity) as min_hum , max(d.humidity) max_hum, avg(d.humidity) avg_hum, hour(d.timestamp) AS hour, datediff('2017-07-31', d.timestamp) day  \n",
    "FROM cos://us-geo/sql/oklabdata/parquet/dht22/2017/07/ STORED AS PARQUET d \n",
    "WHERE  d.sensor_id=810 \n",
    "GROUP BY day, hour\n",
    "ORDER BY day, hour asc\"\"\"\n",
    "\n",
    "dht22_201707_810_df = sqlClient.run_sql(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize the humidity changes during the course of the day in July 2017\n",
    "\n",
    "The matplotlib visualization library requires a matrix as input format. So in the next steps you will convert the dataframe into a matrix, which you can then use to visualize the humidity value changes during the day in July 2017. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dht22_201707_810_df2=dht22_201707_810_df.set_index(['day','hour'], drop = True)\n",
    "del dht22_201707_810_df2[\"max_hum\"]\n",
    "del dht22_201707_810_df2[\"min_hum\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "humMatrix =  np.zeros(shape=(31,24))\n",
    "\n",
    "for i in range(31):\n",
    "    for j in range(24): \n",
    "            humMatrix[i,j] = dht22_201707_810_df2.loc[(i,j),:]\n",
    "         \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the <a href=\"ttps://matplotlib.org/examples/color/colormaps_reference.html\" target=\"_blank\" rel=\"noopener noreferrer\">matplotlib colormaps reference</a> to see which color maps can be used with cmap in `imshow()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "plt.imshow(humMatrix,cmap='YlGnBu', aspect='auto')\n",
    "plt.title('Humidity  Stuttgart July 2018')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "x,y = np.mgrid[0:31,0:24]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = Axes3D(fig)\n",
    "ax.plot_surface(x,y,humMatrix, rstride=1,cstride=1,cmap='YlGnBu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that the humidity during the night is very high. So you can expect valid particulate matter measurements only during the day. Also there are some rainy days that may have to be excluded from the particulate matter measurements to ensure that you analyse only valid data. \n",
    "\n",
    "To confirm the observation for the average humidity let's have a look a specific day(s) in July but not only at the average humidity but also at the minimal and maximal humidity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "chartsize": "69",
      "handlerId": "lineChart",
      "keyFields": "hour",
      "lineChartType": "grouped",
      "logx": "false",
      "logy": "false",
      "orientation": "vertical",
      "rendererId": "matplotlib",
      "tableFields": "avg_hum,hour,max_hum,min_hum",
      "title": "Stuttgart - Humidity 2018/07/28",
      "valueFields": "min_hum,max_hum,avg_hum"
     }
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sql=\"\"\"\n",
    "SELECT min(d.humidity) as min_hum , max(d.humidity) max_hum, avg(d.humidity) avg_hum, hour(d.timestamp) AS hour\n",
    "FROM cos://us-geo/sql/oklabdata/parquet/dht22/2017/07/ STORED AS PARQUET d\n",
    "WHERE date(d.timestamp)=to_date(\"2017-07-28\") and d.sensor_id=810\n",
    "GROUP BY hour ORDER BY hour\"\"\"\n",
    "\n",
    "dht22_201707_810_df = sqlClient.run_sql(sql)\n",
    "\n",
    "subset_to_plot=dht22_201707_810_df[['min_hum','max_hum','avg_hum']]\n",
    " \n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(subset_to_plot)\n",
    "\n",
    "ax.set(xlabel='time of day(h)', ylabel='humidity (%)',\n",
    "       title='Stuttgart Humidity 2018/07/28')\n",
    "ax.grid()\n",
    "\n",
    "fig.savefig(\"test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id=\"sqlcleansing\"></a> 4.4  Cleanse the data to filter out valid measurements  \n",
    "\n",
    "The previous queries have shown that you have to filter out the valid P1, P2 measurements. Only the when the humidity is less than 70% can you rely on the particulate matter measurements. So let's cleanse the data and have another look at the particulate matter measurements in July. \n",
    " \n",
    "To do this, first filter out all timestamps for the dht22 sensor in location 387 where humidity <= 70 % was measured. Then find the particulate matter measurements that match in terms of time (10 second interval) for the sds011 sensor in the same location (temporal join). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "lineChart",
      "keyFields": "hour",
      "tableFields": "hour",
      "title": "Stuttgart - Particulate Matter 2017/07/28  (on cleansed data)",
      "valueFields": "avg_p1,avg_p2"
     }
    }
   },
   "outputs": [],
   "source": [
    "sql=\"\"\"\n",
    "WITH  hlt70 AS \n",
    "    (SELECT timestamp as ts_hlt70 FROM cos://us-geo/sql/oklabdata/parquet/dht22/2017/07/ STORED AS PARQUET \n",
    "     WHERE location=387  and humidity <= 70\n",
    "     ) \n",
    "SELECT avg(sds011.p1) as avg_p1,  max(sds011.p1) as max_p1,\n",
    "       avg(sds011.p2) as avg_p2,  max(sds011.p2) as  max_p2, \n",
    "       hour(sds011.timestamp) as hour , datediff('2017-07-31', sds011.timestamp) as  day \n",
    "FROM cos://us-geo/sql/oklabdata/parquet/sds011/2017/07/  STORED AS PARQUET sds011 , hlt70  \n",
    "WHERE  sds011.location=387 and \n",
    "       (to_unix_timestamp(sds011.timestamp) - 10 < to_unix_timestamp(hlt70.ts_hlt70)) and \n",
    "       (to_unix_timestamp(hlt70.ts_hlt70) < to_unix_timestamp(sds011.timestamp) + 10) \n",
    "GROUP BY day, hour \n",
    "ORDER BY day, hour asc\"\"\" \n",
    "\n",
    "sds011_201707_809 = sqlClient.run_sql(sql)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sds011_201707_809_df=sds011_201707_809.set_index(['day','hour'], drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sds011_201707_809_df[\"max_p1\"] \n",
    "del sds011_201707_809_df[\"max_p2\"] \n",
    "del sds011_201707_809_df[\"avg_p2\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finedustMatrix =  np.zeros(shape=(31,24))\n",
    "\n",
    "for i in range(31):\n",
    "    for j in range(24):\n",
    "        if (i,j) in sds011_201707_809_df.index:  \n",
    "            finedustMatrix[i,j] = sds011_201707_809_df.loc[(i,j),:]\n",
    "        else: \n",
    "            finedustMatrix[i,j] = -1\n",
    "\n",
    "plt.imshow(finedustMatrix,cmap='Oranges', aspect='auto')\n",
    "plt.title('Average fine dust values of P10 in Stuttgart July 2018')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Next steps<a id=\"nextsteps\"></a>\n",
    "\n",
    "Of course there are other ways to deal with the humidity issue. For example:\n",
    "\n",
    "- You can try to correct the particulate measurement data to make the measurements during periods of high humidity usable for the further evaluation.\n",
    "- You may want to look for outliers indicating that the sensor is not working correctly anymore. \n",
    "- Last but not least one may to want to compare the data with official measurements close by to gain trust in the data.\n",
    "\n",
    "After you have completed the data preparation step you can dive deeper into data analysis using, for example, Watson Studio with Watson Machine Learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Citation\n",
    "OK Lab Stuttgart particulate matter data - <a href=\"https://opendatacommons.org/licenses/dbcl/1-0/\" target=\"_blank\" rel=\"noopener noreferrer\">DbCL V1.0</a> and <a href=\"https://opendatacommons.org/licenses/odbl/1-0/\" target=\"_blank\" rel=\"noopener noreferrer\">ODbL V1.0</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author\n",
    "**Dr. Ute Schuerfeld** is a software developer for IBM Cloud SQL Query at IBM, Germany. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright © 2018-2021 IBM. This notebook and its source code are released under the terms of the MIT License."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
