{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ingest data from Event Streams in a streams flow\n",
    "\n",
    "\n",
    "If your organization uses Event Streams to communicate between applications, you can easily ingest streaming data from Event Streams for analysis in real time by using the streams flow tool in IBM Watson Studio. The tool is a <a href=\"https://developer.ibm.com/streamsdev/2017/11/28/quickly-create-streams-applications-using-new-streams-designer/\" target=\"_blank\" rel=\"noopener noreferrer\">web based graphical IDE</a> to help you create streaming analytics applications without having to write a lot of code. Applications are called *flows*. This tutorial will show you how to create a streams flow that uses streaming data from Event Streams. The final result is shown below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/messagehubflow.gif\" alt=\"Event Streams flow\"/>\n",
    "\n",
    "This tutorial builds on the Data Historian example streams flow. That application uses sample data as its data source. The sample data is a stream of readings from weather stations (temperature, amount of rain, etc.) and is used to compute the average of those readings in the last hour. \n",
    "\n",
    "In this tutorial, you'll modify that example to use Event Streams as its data source.\n",
    "\n",
    "This notebook runs on Python and Spark.\n",
    "\n",
    "\n",
    "## Table of Contents\n",
    "1. [Prerequisites](#prereq)\n",
    "1. [Set up a Event Streams instance](#step2)\n",
    "    <br/>2.1 [Create a Event Streams instance](#step21)\n",
    "    <br/>2.2 [Create a topic](#step22)\n",
    "1. [Publish data to the Event Streams service](#step3)\n",
    "    <br/>3.1 [Get your Event Streams credentials](#step31)\n",
    "    <br/>3.2 [Install Confluent’s Apache Kafka Python client](#step32)\n",
    "    <br/>3.3 [Generate sample data](#step33)\n",
    "    <br/>3.4 [Start publishing the sample data to Event Streams](#step34)\n",
    "1. [Use the data from Event Streams in your streams flow](#step4)\n",
    "1. [Troubleshooting](#step5)\n",
    "1. [Next steps - Send data to Event Streams from your streams flow](#next)\n",
    "\n",
    "\n",
    "<a id=\"prereq\"></a>\n",
    "## Prerequisites\n",
    "\n",
    "Since the goal of this notebook is to modify the Data Historian example flow to use Event Streams as its data source, you need to import the Data Historian example flow. If you already have the flow imported in a project, skip this step.\n",
    "Otherwise, <a href=\"https://www.youtube.com/watch?v=rCNgJopanrY\" target=\"_blank\" rel=\"noopener noreferrer\">watch this video</a> or follow the instructions below to import the Data Historian example flow.\n",
    "\n",
    "1. From the **Projects** menu, click **View All Projects**, and then click the name of the project to add your streams flow to. Alternatively, to create a project for the streams flow, click New project.\n",
    "1. In the Project page, click the **Add to projects** tab, and then choose the **Streams flow** asset type.\n",
    "1. Select **From example**. Choose a Streaming Analytics service, or create one if prompted to do so.  \n",
    "1. Click **Data Historian Example**. \n",
    "1. Select a connection to Cloud Object Storage. \n",
    "1. Under **File path**, click the slider button to select a bucket from your Object Storage instance. Enter a file name after the bucket name, like `/mybucket/data_historian_results_%TIME.txt`.\n",
    "1. Click **Create**. After the project is created, click *Run* to start the flow.\n",
    "1. Verify that data is being generated by logging into your Cloud Object Storage service to view the results file.\n",
    "\n",
    "\n",
    "You can learn more about this example [here](https://dataplatform.ibm.com/docs/content/streaming-pipelines/data_historian_example_pipeline.html?audience=wdp&context=analytics&linkInPage=true).\n",
    "\n",
    "\n",
    "<a id=\"step2\"></a>\n",
    "## 2. Set up your Event Streams instance\n",
    "\n",
    "<a id=\"step21\"></a>\n",
    "\n",
    "### 2.1 Create a Event Streams instance\n",
    "\n",
    "\n",
    "If you do not already have an instance of Event Streams, you must create one. You can do so from any page in the Watson Studio.   You must have already added a credit card to your IBM Cloud account to have access to Event Streams service.\n",
    " - Select **Services** from the toolbar, and then click **Data Services**. \n",
    " - Click **Add services** in the top right.\n",
    " - Choose **Event Streams** from the list of services that appear.\n",
    " - Click **Standard** to select the Standard pricing plan, and then click **Create**.\n",
    " - In the Confirm Creation dialog box that appears, change the Service Name if you wish.\n",
    " - Your service will be created, and then you will be returned to the Services page.\n",
    "\n",
    "\n",
    "<a id=\"step22\"></a>\n",
    "\n",
    "### Create a topic\n",
    "- In the Services page, find your Event Streams service. In the context menu under **Actions**, click **Manage in IBM Cloud**. \n",
    "<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/manage.png\" alt=\"Manage in Cloud\"/>\n",
    "\n",
    "<br/>\n",
    "- The Manage page of the service opens in a new tab. \n",
    "- A list of topics will appear. Click the **+** sign under **Topics** to add a new topic called `temperature`, and then click **Create topic**. \n",
    "<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/topic.png\" alt=\"Create new topic\"/>\n",
    "<br/><br/>\n",
    "\n",
    "<a id=\"step3\"></a>\n",
    "\n",
    "## 3. Publish data to your Event Streams instance\n",
    "Before you can use data *from* Event Streams in your streams flow, you'll need an application that is sending data *to* Event Streams.  The next few cells of this notebook will create a simple application that will generate and send data to Event Streams for use in your streams flow.\n",
    "\n",
    "The application needs to be configured with credentials of your Event Streams service so that it can securely send data.\n",
    "\n",
    "So, first you must retrieve credentials for your Event Streams instance.\n",
    "\n",
    "<a id=\"step31\"></a>\n",
    "\n",
    "### 3.1 Get your Event Streams service credentials\n",
    "- In the Manage page of your service, click **Service credentials**.\n",
    "- A set of service credentials should already be created. If not, click **New credential** and accept the defaults. \n",
    "- Click **View credentials**, and then click the copy button to copy them to the clipboard.\n",
    "<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/copycredentials.png\" alt=\"copy credentials\"/>\n",
    "\n",
    "- Paste them in the cell below where indicated.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"creds\"></a>\n",
    "#### Paste your Event Streams credentials here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "message_hub_credentials = #PASTE CREDENTIALS HERE#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"step32\"></a>\n",
    "### 3.2 Install Confluent’s Apache Kafka Python client\n",
    "This client contains the API used to communicate with Event Streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied (use --upgrade to upgrade): confluent-kafka in /gpfs/global_fs01/sym_shared/YPProdSpark/user/s623-f8b248de4f938e-3d6cff768616/.local/lib/python3.5/site-packages\r\n"
     ]
    }
   ],
   "source": [
    "!pip install confluent-kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step33\"></a>\n",
    "### 3.3 Generate sample data\n",
    "\n",
    "The next cell will generate simulated data for 4 weather stations. Each weather station is represented by the `Device` class. The `Device` class generates JSON data in the same input schema that is used in the Data Historian example pipeline:\n",
    "```\n",
    "{\n",
    "\"id\": \"IALBERTA598\",\n",
    "\"tz\": \"America/Edmonton\",\n",
    "\"dateutc\": \"2017-01-24 05:03:50\",\n",
    "\"latitude\": 50.88381958,\n",
    "\"longitude\": -113.98414612,\n",
    "\"temperature\": \"16.399999618530273\",\n",
    "\"baromin\": 30.69109211987511,\n",
    "\"humidity\": 109.53158304521693,\n",
    "\"rainin\": 0,\n",
    "\"time_stamp\": \"2017-11-21 01:41:05\"\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import random\n",
    "from datetime import datetime, timezone\n",
    "class Device(object):\n",
    "    def __init__(self, name, latitude, longitude, timezone):\n",
    "        self.id = name\n",
    "        self.latitude = latitude\n",
    "        self.longitude = longitude\n",
    "        self.tz = timezone\n",
    "        self.temp = random() * 50\n",
    "        self.humidity = 50*random() + 50\n",
    "        self.baromin = 75 *random()\n",
    "    def getrain(self, humidity):\n",
    "        if (humidity > 75.0 or self.temp > 30):\n",
    "            return humidity * (0.001 * random())\n",
    "    def getReadingAsJSON(self):\n",
    "        humidity = (self.humidity * 0.98) + random()\n",
    "        reading =   {\n",
    "            \"id\": self.id,\n",
    "            \"tz\": self.tz,\n",
    "            \"dateutc\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"latitude\": self.latitude,\n",
    "            \"longitude\": self.longitude,\n",
    "            \"temperature\": (self.temp * 0.95) + random(),\n",
    "            \"baromin\": (self.baromin * 0.95) + random(),\n",
    "            \"humidity\": humidity,\n",
    "            \"rainin\": self.getrain(humidity),\n",
    "            \"time_stamp\": datetime.now(timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "          }\n",
    "        return reading\n",
    "\n",
    "def list_as_str(lizt):\n",
    "    return \",\".join(lizt)\n",
    "\n",
    "\n",
    "def generate_data(duration=15):\n",
    "    sample_data = [(\"IALBERTA384\",\"America/Edmonton\",51.07976532,-115.33161163),\n",
    "            (\"IANDALUC208\", \"Europe/Madrid\", 37.62454224, -2.70604014), \n",
    "            (\"IANSEROY2\", \"Indian/Mahe\", -4.74099493,55.51583862), \n",
    "            (\"I1189\",\"Asia/Yekaterinburg\",57.15063477, 65.56357574)]\n",
    "    devices = []\n",
    "    for sample in sample_data:\n",
    "        name, timezone, latitude, longitude = sample\n",
    "        devices.append(Device(name,latitude,longitude, timezone))\n",
    "    \n",
    "    duration = duration * 60\n",
    "    if (duration is 0):\n",
    "        print(\"Will produce data indefinitely. Click Kernel >Interrupt Kernel to stop producing data.\")\n",
    "    count = 0\n",
    "    while count < duration or duration is 0:\n",
    "        for device in devices:\n",
    "            reading = device.getReadingAsJSON()\n",
    "            yield reading\n",
    "        time.sleep(1.0)\n",
    "        count = count + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step34\"></a>\n",
    "### 3.4 Start publishing  sample data to Event Streams\n",
    "\n",
    "We use the service credentials you pasted earlier to create an instance of the `Producer` class and then publish a stream of weather station readings to the *temperature* topic. You can use the `TOPIC` variable to change the topic.\n",
    "\n",
    "\n",
    "*Note*: This cell will run for 20 minutes in a background job. If your streams flow stops receiving data, re-run the cell to keep generating data. Alternatively, change the `DURATION_IN_MINUTES` variable. You can set the `DURATION_IN_MINUTES` to a larger value, or to `0` to run this cell indefinitely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "TOPIC = \"temperature\"\n",
    "DURATION_IN_MINUTES = 20\n",
    "\n",
    "mhub_config  = {\n",
    "    'debug': 'msg',\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanisms': 'PLAIN',\n",
    "    'sasl.password': message_hub_credentials[\"password\"],\n",
    "   'bootstrap.servers': list_as_str(message_hub_credentials[\"kafka_brokers_sasl\"]),                               \n",
    "   'sasl.username': message_hub_credentials[\"user\"]\n",
    "}\n",
    "                                    \n",
    "def start():\n",
    "    producer = Producer(**mhub_config)\n",
    "    for reading in generate_data(DURATION_IN_MINUTES):\n",
    "        producer.produce(TOPIC, value=json.dumps(reading).encode('utf-8'))\n",
    "    producer.flush()\n",
    "\n",
    "\n",
    "            \n",
    "from IPython.lib import backgroundjobs as bg\n",
    "\n",
    "if DURATION_IN_MINUTES > 0:\n",
    "    print(datetime.now().ctime() +\": Sending data to Event Streams for %d minutes\" % DURATION_IN_MINUTES)\n",
    "\n",
    "jobs = bg.BackgroundJobManager()\n",
    "jobs.new(start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"step4\"></a>\n",
    "\n",
    "## 4. Use the data from Event Streams in your streams flow\n",
    "\n",
    "Now that we have data being sent to the Event Streams instance, you can use it in your flow. If the flow is currently running, click the *stop* button to stop it, and then click *edit* to edit it in the canvas.\n",
    "\n",
    "\n",
    "\n",
    "### Add and configure the Event Streams operator \n",
    "\n",
    "- Drag the Event Streams operator from the Source list to the canvas.\n",
    "- Under **Connection**, click **Add connection**. In the Create Connection window, choose your service instance under **Your service instances in IBM Cloud**. The credentials will be populated in the dialog. Click **Create**.\n",
    "<img src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/dialog.png\" alt=\"Event Streams Connection dialog\"/>\n",
    "\n",
    "- Under **Topic**, the list will be populated with the topics you have created in Event Streams.  Select the *temperature* topic.\n",
    "- Select **Edit Schema**, and then click **Show Preview** to verify that the data from the notebook is being sent correctly. You should see a preview of the data. If you do not see any data, make sure that the service credentials and name match what you created earlier.  Also, verify that the previous cell is running.\n",
    "- Click **Detect schema** to automatically determine the output schema of the operator. Make sure the values are as follows:\n",
    "    - `id` : Text\n",
    "    - `tz`: Text\n",
    "    - `dateutc`: Date\n",
    "    - `latitude`: Number\n",
    "    - `longitude`: Number\n",
    "    - `temperature`: Number\n",
    "    - `baromin`: Number\n",
    "    - `rainin`:  Number\n",
    "    - `humidity` : Number\n",
    "    - `time_stamp`: Date\n",
    "   \n",
    "<img alt=\"Schema detection\" src=\"https://developer.ibm.com/streamsdev/wp-content/uploads/sites/15/2017/11/schema.png\"></img>\n",
    "- Click **Save**, and then **Close** to close the Edit Schema window.\n",
    "- Connect the Event Streams operator to the Aggregation operator, and then delete the Sample Data operator.  Your graph should look like this:\n",
    "<img alt=\"flow diagram with Event Streams\" src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/finalflow1.png\"></img>\n",
    "- •\tClick **Save**, and then click **Run** to start the streams flow. After it starts, you should see data flowing from Event Streams.\n",
    "\n",
    "<img alt=\"running streams flow\" src=\"https://raw.githubusercontent.com/IBMDataScience/sample-notebooks/master/Files/runningflow2.png\"></img>\n",
    "    \n",
    "    \n",
    "<a id=\"step5\"></a>\n",
    "## 5. Troubleshooting\n",
    "\n",
    "If you get any errors, check the following points:\n",
    "- Your Streaming Analytics service is started.\n",
    "- Credentials are correctly copied from Event Streams and <a href=\"#creds\">pasted in the cell</a>.\n",
    "- Data is being sent to Event Streams, [check step 3.4](#step34).\n",
    "You could modify the cell below to try to read from the `temperature` topic. Change `RESULTS_TOPIC_NAME` to `temperature`. If that cell prints data from Event Streams, then the problem you are having is likely a related to the configuration of your Streams flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"next\"></a>\n",
    "## 6. Next steps - Send data to Event Streams from your streams flow\n",
    "See if you can change the Streams flow to send the results  *to* Event Streams instead of Object Storage. \n",
    "If it is working correctly, the next cell will print out the results of the Aggregation operator.\n",
    " \n",
    "**Hint:** You will need to create a new topic in Event Streams. Call it `results`.\n",
    "You'll also need the Event Streams target operator, which you will find in the *Target* category in the canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Incoming data: { \"id\" : \"I1189\", \"tz\" : \"Asia/Yekaterinburg\", \"dateutc\" : \"2017-12-11T20:59:34\", \"time_stamp\" : \"2017-12-11T20:59:34\", \"longitude\" : 65.56357574, \"latitude\" : 57.15063477, \"rainin\" : 0, \"humidity\" : 49.2586993697516, \"baromin\" : 18.2346408820349, \"temperature\" : 0.0232874316157452 }\n",
      "Incoming data: { \"id\" : \"IALBERTA384\", \"tz\" : \"America/Edmonton\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : -115.33161163, \"latitude\" : 51.07976532, \"rainin\" : 0, \"humidity\" : 22.117086699143, \"baromin\" : 68.1079591166223, \"temperature\" : 0.0183134239080065 }\n",
      "Incoming data: { \"id\" : \"IANDALUC208\", \"tz\" : \"Europe/Madrid\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : -2.70604014, \"latitude\" : 37.62454224, \"rainin\" : 0, \"humidity\" : 25.9265311927769, \"baromin\" : 27.7391455939335, \"temperature\" : 0.0195746529946135 }\n",
      "Incoming data: { \"id\" : \"IANSEROY2\", \"tz\" : \"Indian/Mahe\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : 55.51583862, \"latitude\" : -4.74099493, \"rainin\" : 0, \"humidity\" : 81.9131858479234, \"baromin\" : 13.857782695754, \"temperature\" : 0.0528405408079244 }\n",
      "Incoming data: { \"id\" : \"I1189\", \"tz\" : \"Asia/Yekaterinburg\", \"dateutc\" : \"2017-12-11T20:59:36\", \"time_stamp\" : \"2017-12-11T20:59:36\", \"longitude\" : 65.56357574, \"latitude\" : 57.15063477, \"rainin\" : 0, \"humidity\" : 48.3786589338269, \"baromin\" : 18.5571523289841, \"temperature\" : 0.076790066048293 }\n",
      "Incoming data: { \"id\" : \"IALBERTA384\", \"tz\" : \"America/Edmonton\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : -115.33161163, \"latitude\" : 51.07976532, \"rainin\" : 0, \"humidity\" : 21.7735060513338, \"baromin\" : 67.9822640938181, \"temperature\" : 0.119233444854667 }\n",
      "Incoming data: { \"id\" : \"IANDALUC208\", \"tz\" : \"Europe/Madrid\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : -2.70604014, \"latitude\" : 37.62454224, \"rainin\" : 0, \"humidity\" : 25.4805076947617, \"baromin\" : 28.3991059646348, \"temperature\" : 0.415482454053425 }\n",
      "Incoming data: { \"id\" : \"IANSEROY2\", \"tz\" : \"Indian/Mahe\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : 55.51583862, \"latitude\" : -4.74099493, \"rainin\" : 0, \"humidity\" : 81.5506741343814, \"baromin\" : 13.3492433336868, \"temperature\" : 0.273526186452909 }\n",
      "Incoming data: { \"id\" : \"I1189\", \"tz\" : \"Asia/Yekaterinburg\", \"dateutc\" : \"2017-12-11T20:59:38\", \"time_stamp\" : \"2017-12-11T20:59:38\", \"longitude\" : 65.56357574, \"latitude\" : 57.15063477, \"rainin\" : 0, \"humidity\" : 48.6750845124346, \"baromin\" : 18.2073641964531, \"temperature\" : 0.0708896898852753 }\n",
      "Incoming data: { \"id\" : \"IALBERTA384\", \"tz\" : \"America/Edmonton\", \"dateutc\" : \"2017-12-11T20:59:40\", \"time_stamp\" : \"2017-12-11T20:59:40\", \"longitude\" : -115.33161163, \"latitude\" : 51.07976532, \"rainin\" : 0, \"humidity\" : 21.8635825021746, \"baromin\" : 68.1004201827024, \"temperature\" : 0.0145249467698025 }\n"
     ]
    }
   ],
   "source": [
    "from confluent_kafka import Consumer, KafkaError\n",
    "\n",
    "RESULTS_TOPIC_NAME = \"results\"\n",
    "mhub_config_consumer  = {\n",
    "     'group.id': 'mygroup',\n",
    "    'security.protocol': 'SASL_SSL',\n",
    "    'sasl.mechanisms': 'PLAIN',\n",
    "    'sasl.password': message_hub_credentials[\"password\"],\n",
    "   'bootstrap.servers': list_as_str(message_hub_credentials[\"kafka_brokers_sasl\"]),                               \n",
    "   'sasl.username': message_hub_credentials[\"user\"]\n",
    "}\n",
    "consumer = Consumer(**mhub_config_consumer)\n",
    "consumer.subscribe([RESULTS_TOPIC_NAME])\n",
    "for i in range(10): #only print the first 10 messages.\n",
    "    message = consumer.poll(timeout=2.0)\n",
    "    \n",
    "    if message is not None:\n",
    "        if message.error():\n",
    "            error = message.error()\n",
    "            if error.code() == KafkaError._PARTITION_EOF :\n",
    "                print(\"No more messages. Is data being sent to the %s topic? \" % RESULTS_TOPIC_NAME) \n",
    "                break\n",
    "            else:\n",
    "                print(\"Error: \" + error.str())\n",
    "                break\n",
    "        else:\n",
    "            print(\"Incoming data: %s\" % message.value().decode(\"utf-8\"))        \n",
    "        \n",
    "        \n",
    "consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More information\n",
    "- <a href=\"https://dataplatform.ibm.com/docs/content/streaming-pipelines/creating-pipeline-manually.html?audience=wdp\" target=\"_blank\" rel=\"noopener noreferrer\">Learn more about the other operators available in streams flow canvas</a>.\n",
    "- <a href=\"https://developer.ibm.com/streamsdev\" target=\"_blank\" rel=\"noopener noreferrer\">Visit Streamsdev</a> to learn more about Streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Author\n",
    "\n",
    "**Natasha D'Silva** is a software developer at IBM Canada who specializes in streaming technology and cloud solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Copyright © IBM Corp. 2017, 2018. This notebook and its source code are released under the terms of the Apache 2.0 License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n",
    "<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n",
    "<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n",
    "<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n",
    "<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
