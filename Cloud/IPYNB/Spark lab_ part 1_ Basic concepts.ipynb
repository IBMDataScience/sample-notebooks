{"nbformat_minor": 1, "cells": [{"source": "# Introduction to Spark lab, part 1: Basic concepts\nThis notebook guides you through the basic concepts to start working with Spark, including how to set up your environment, create and analyze data sets, and work with data files.\n\nThis notebook uses pySpark, the Python API for Spark. Some knowledge of Python is recommended. This notebook runs on Python and Spark.\n\nIf you are new to notebooks, here's how the user interface works: <a href=\"https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/parts-of-a-notebook.html\" target=\"_blank\" rel=\"noopener noreferrer\">The parts of a notebook</a>.", "cell_type": "markdown", "metadata": {}}, {"source": "## About Spark\nSpark is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for processing structured data, MLlib for machine learning, GraphX for graph processing, and Spark Streaming.\n\n<img src='https://github.com/carloapp2/SparkPOT/blob/master/spark.png?raw=true' width=\"50%\" height=\"50%\"></img>\n\n\nA Spark program has a driver program and worker programs. Worker programs run on cluster nodes or in local threads. Data sets are distributed\u001d across workers. \n\n<img src='https://github.com/carloapp2/SparkPOT/blob/master/Spark%20Architecture.png?raw=true' width=\"50%\" height=\"50%\"></img>", "cell_type": "markdown", "metadata": {}}, {"source": "## Table of Contents\nIn the first four sections of this notebook, you'll learn about Spark with very simple examples. In the last two sections, you'll use what you learned to analyze data files that have more realistic data sets.\n\n1. [Work with the SparkContext](#sparkcontext)<br>\n    1.1 [Invoke the SparkContext](#sparkcontext1)<br>\n    1.2 [Check the Spark version](#sparkcontext2)<br>\n2. [Work with RDDs](#rdd)<br>\n    2.1 [Create a collection](#rdd1)<br>\n    2.2 [Create an RDD](#rdd2)<br>\n    2.3 [View the data](#rdd3)<br>\n    2.4 [Create another RDD](#rdd4)<br>\n3. [Manipulate data in RDDs](#trans)<br>\n    3.1 [Update numeric values](#trans1)<br>\n    3.2 [Add numbers in an array](#trans2)<br>\n    3.3 [Split and count strings](#trans3)<br>\n    3.4 [Counts words with a pair RDD](#trans4)<br>\n4. [Filter data](#filter)<br>\n5. [Analyze text data from a file](#wordfile)<br>\n    5.1 [Get the data from a URL](#wordfile1)<br>\n    5.2 [Create an RDD from the file](#wordfile2)<br>\n    5.3 [Filter for a word](#wordfile3)<br>\n    5.4 [Count instances of a string at the beginning of words](#wordfile4)<br>\n    5.5 [Count instances of a string within words](#wordfile5)<br>\n6. [Analyze numeric data from a file](#numfile)<br>\n7. [Summary and next steps](#summary)", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"sparkcontext\"></a>\n## 1. Work with the SparkContext object\n\nThe Spark driver application uses the SparkContext object to allow a programming interface to interact with the driver application. The SparkContext object tells Spark how and where to access a cluster.\n\nThe Watson Studio notebook environment predefines the Spark context for you.\n\nIn other environments, you need to pick an interpreter (for example, pyspark for Python) and create a SparkConf object to initialize a SparkContext object. For example:\n<br>\n`from pyspark import SparkContext, SparkConf`<br>\n`conf = SparkConf().setAppName(appName).setMaster(master)`<br>\n`sc = SparkContext(conf=conf)`<br>\n\n<a id=\"sparkcontext1\"></a>\n### 1.1 Invoke the SparkContext\nRun the following cell to invoke the SparkContext:", "cell_type": "markdown", "metadata": {}}, {"source": "sc", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "Waiting for a Spark session to start...\nSpark Initialization Done! ApplicationId = app-20190629194315-0000\nKERNEL_ID = 567fa8f4-5d74-4e3f-9714-c0fcaf01dd3f\n"}, {"output_type": "execute_result", "data": {"text/html": "\n        <div>\n            <p><b>SparkContext</b></p>\n\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v2.3.3</code></dd>\n              <dt>Master</dt>\n                <dd><code>spark://jkg-deployment-567fa8f4-5d74-4e3f-9714-c0fcaf01dd3f-69556fk2v4r:7077</code></dd>\n              <dt>AppName</dt>\n                <dd><code>pyspark-shell</code></dd>\n            </dl>\n        </div>\n        ", "text/plain": "<SparkContext master=spark://jkg-deployment-567fa8f4-5d74-4e3f-9714-c0fcaf01dd3f-69556fk2v4r:7077 appName=pyspark-shell>"}, "execution_count": 1, "metadata": {}}], "execution_count": 1}, {"source": "<a id=\"sparkcontext2\"></a>\n### 1.2 Check the Spark version\nCheck the version of the Spark driver application:", "cell_type": "markdown", "metadata": {}}, {"source": "sc.version", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'2.3.3'"}, "execution_count": 2, "metadata": {}}], "execution_count": 2}, {"source": "<a id=\"rdd\"></a>\n## 2. Work with Resilient Distributed Datasets\nSpark uses an abstraction for working with data called a Resilient Distributed Dataset (RDD). An RDD is a collection of elements that can be operated on in parallel. RDDs are immutable, so you can't update the data in them. To update data in an RDD, you must create a new RDD. In Spark, all work is done by creating new RDDs, transforming existing RDDs, or using RDDs to compute results. When working with RDDs, the Spark driver application automatically distributes the work across the cluster.\n\nYou can construct RDDs by parallelizing existing Python collections (lists), by manipulating RDDs, or by manipulating files in HDFS or any other storage system.\n\nYou can run these types of methods on RDDs: \n - Actions: query the data and return values\n - Transformations: manipulate data values and return pointers to new RDDs. \n\nFind more information on Python methods in the <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html\" target=\"_blank\" rel=\"noopener noreferrer\">PySpark documentation</a>.\n\n<a id=\"rdd1\"></a>\n### 2.1 Create a collection\nCreate a Python collection of the numbers 1 - 10:", "cell_type": "markdown", "metadata": {}}, {"source": "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 3}, {"source": "<a id=\"rdd2\"></a>\n### 2.2 Create an RDD \nPut the collection into an RDD named `x_nbr_rdd` using the `parallelize` method:", "cell_type": "markdown", "metadata": {}}, {"source": "x_nbr_rdd = sc.parallelize(x)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 4}, {"source": "Notice that there's no return value. The `parallelize` method didn't compute a result, which means it's a transformation. Spark just recorded how to create the RDD.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"rdd3\"></a>\n### 2.3 View the data \nView the first element in the RDD:", "cell_type": "markdown", "metadata": {}}, {"source": "x_nbr_rdd.first()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "1"}, "execution_count": 5, "metadata": {}}], "execution_count": 5}, {"source": "Each number in the collection is in a different element in the RDD. Because the `first()` method returned a value, it is an action. \n\nNow view the first five elements in the RDD:", "cell_type": "markdown", "metadata": {}}, {"source": "x_nbr_rdd.take(5)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[1, 2, 3, 4, 5]"}, "execution_count": 6, "metadata": {}}], "execution_count": 6}, {"source": "<a id=\"rdd4\"></a>\n### 2.4 Create another RDD \nCreate a Python collection that contains strings:", "cell_type": "markdown", "metadata": {}}, {"source": "y = [\"Hello Human\", \"My Name is Spark\"]", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 7}, {"source": "Put the collection into an RDD:", "cell_type": "markdown", "metadata": {}}, {"source": "y_str_rdd = sc.parallelize(y)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 8}, {"source": "View the first element in the RDD:", "cell_type": "markdown", "metadata": {}}, {"source": "y_str_rdd.take(1)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "['Hello Human']"}, "execution_count": 9, "metadata": {}}], "execution_count": 9}, {"source": "You created the string \"Hello Human\" and you returned it as the first element of the RDD. To analyze a set of words, you can map each word into an RDD element.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"trans\"></a>\n## 3. Manipulate data in RDDs\n\nRemember that to manipulate data, you use transformation functions.\n\nHere are some common Python transformation functions that you'll be using in this notebook:\n\n - `map(func)`: returns a new RDD with the results of running the specified function on each element  \n - `filter(func)`: returns a new RDD with the elements for which the specified function returns true   \n - `distinct([numTasks]))`: returns a new RDD that contains the distinct elements of the source RDD\n - `flatMap(func)`: returns a new RDD by first running the specified function on all elements, returning 0 or more results for each original element, and then flattening the results into individual elements\n\nYou can also create functions that run a single expression and don't have a name with the Python `lambda` keyword. For example, this function returns the sum of its arguments: `lambda a , b : a + b`.\n\n<a id=\"trans1\"></a>\n### 3.1 Update numeric values\nRun the `map()` function with the `lambda` keyword to replace each element, X, in your first RDD (the one that has numeric values) with X+1. Because RDDs are immutable, you need to specify a new RDD name.", "cell_type": "markdown", "metadata": {}}, {"source": "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 10}, {"source": "Now look at the elements of the new RDD: ", "cell_type": "markdown", "metadata": {}}, {"source": "x_nbr_rdd_2.collect()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"}, "execution_count": 11, "metadata": {}}], "execution_count": 11}, {"source": "Be careful with the `collect` method! It returns __all__ elements of the RDD to the driver. Returning a large data set might be not be very useful. No-one wants to scroll through a million rows!", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"trans2\"></a>\n### 3.2 Add numbers in an array\nAn array of values is a common data format where multiple values are contained in one element. You can manipulate the individual values if you split them up into separate elements.\n\nCreate an array of numbers by including quotation marks around the whole set of numbers. If you omit the quotation marks, you get a collection of numbers instead of an array.", "cell_type": "markdown", "metadata": {}}, {"source": "X = [\"1,2,3,4,5,6,7,8,9,10\"]", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 12}, {"source": "Create an RDD for the array:", "cell_type": "markdown", "metadata": {}}, {"source": "y_rd = sc.parallelize(X)", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 13}, {"source": "Split the values at commas and add values in the positions 2 and 9 in the array. Keep in mind that an array starts with position 0. Use a backslash character, \\, to break the line of code for clarity.", "cell_type": "markdown", "metadata": {}}, {"source": "Sum_rd = y_rd.map(lambda y: y.split(\",\")).\\\nmap(lambda y: (int(y[2])+int(y[9])))", "cell_type": "code", "metadata": {}, "outputs": [], "execution_count": 14}, {"source": "Now return the value of the sum:", "cell_type": "markdown", "metadata": {}}, {"source": "Sum_rd.first()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "13"}, "execution_count": 15, "metadata": {}}], "execution_count": 15}, {"source": "<a id=\"trans3\"></a>\n### 3.3 Split and count text strings\n\nCreate an RDD with a text string and show the first element:", "cell_type": "markdown", "metadata": {}}, {"source": "Words = [\"Hello Human. I'm Spark and I love running analysis on data.\"]\nwords_rd = sc.parallelize(Words)\nwords_rd.first()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "\"Hello Human. I'm Spark and I love running analysis on data.\""}, "execution_count": 16, "metadata": {}}], "execution_count": 16}, {"source": "Split the string into separate lines at the space characters and look at the first element:", "cell_type": "markdown", "metadata": {}}, {"source": "Words_rd2 = words_rd.map(lambda line: line.split(\" \"))\nWords_rd2.first()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "['Hello',\n 'Human.',\n \"I'm\",\n 'Spark',\n 'and',\n 'I',\n 'love',\n 'running',\n 'analysis',\n 'on',\n 'data.']"}, "execution_count": 17, "metadata": {}}], "execution_count": 17}, {"source": "Count the number of elements in this RDD with the `count()` method:", "cell_type": "markdown", "metadata": {}}, {"source": "Words_rd2.count()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "1"}, "execution_count": 18, "metadata": {}}], "execution_count": 18}, {"source": "Of course, you already knew that there was only one element because you ran the `first()` method and it returned the whole string. Splitting the string into multiple lines did not create multiple elements.", "cell_type": "markdown", "metadata": {}}, {"source": "Now split the string again, but this time with the `flatmap()` method, and look at the first three elements:", "cell_type": "markdown", "metadata": {}}, {"source": "words_rd2 = words_rd.flatMap(lambda line: line.split(\" \"))\nwords_rd2.take(3)", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "['Hello', 'Human.', \"I'm\"]"}, "execution_count": 19, "metadata": {}}], "execution_count": 19}, {"source": "This time each word is separated into its own element.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"trans4\"></a>\n### 3.4 Count words with a pair RDD\nA common way to count the number of instances of words in an RDD is to create a pair RDD. A pair RDD converts each word into a key-value pair: the word is the key and the number 1 is the value. Because the values are all 1, when you add the  values for a particular word, you get the number of instances of that word.\n\nCreate an RDD:", "cell_type": "markdown", "metadata": {}}, {"source": "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\nz_str_rdd = sc.parallelize(z)\nz_str_rdd.first()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'First,Line'"}, "execution_count": 20, "metadata": {}}], "execution_count": 20}, {"source": "Split the elements into individual words with the `flatmap()` method:", "cell_type": "markdown", "metadata": {}}, {"source": "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\nz_str_rdd_split_flatmap.collect()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "['First', 'Line', 'Second', 'Line', 'and', 'Third', 'Line']"}, "execution_count": 21, "metadata": {}}], "execution_count": 21}, {"source": "Convert the elements into key-value pairs:", "cell_type": "markdown", "metadata": {}}, {"source": "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\ncountWords.collect()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[('First', 1),\n ('Line', 1),\n ('Second', 1),\n ('Line', 1),\n ('and', 1),\n ('Third', 1),\n ('Line', 1)]"}, "execution_count": 22, "metadata": {}}], "execution_count": 22}, {"source": "Now sum all the values by key to find the number of instances for each word: ", "cell_type": "markdown", "metadata": {}}, {"source": "from operator import add\ncountWords2 = countWords.reduceByKey(add)\ncountWords2.collect()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[('Second', 1), ('Line', 3), ('First', 1), ('and', 1), ('Third', 1)]"}, "execution_count": 23, "metadata": {}}], "execution_count": 23}, {"source": "Notice that the word `Line` has a count of 3.", "cell_type": "markdown", "metadata": {}}, {"source": "<a id=\"filter\"></a>\n## 4. Filter data\n\nThe filter command creates a new RDD from another RDD based on a filter criteria.\nThe filter syntax is: \n\n`.filter(lambda line: \"Filter Criteria Value\" in line)`\n\nHint: Use a simple python `print` command to add a string to your Spark results and to run multiple actions in single cell.\n\nFind the number of instances of the word `Line` in the `z_str_rdd_split_flatmap` RDD:", "cell_type": "markdown", "metadata": {}}, {"source": "words_rd3 = z_str_rdd_split_flatmap.filter(lambda line: \"Second\" in line) \n\nprint (\"The count of words \" + str(words_rd3.first()))\nprint (\"Is: \" + str(words_rd3.count()))", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "The count of words Second\nIs: 1\n"}], "execution_count": 24}, {"source": "<a id=\"wordfile\"></a>\n## 5. Analyze text data from a file\nIn this section, you'll download a file from a URL, create an RDD from it, and analyze the text in it.\n\n<a id=\"wordfile1\"></a>\n### 5.1 Get the file from a URL\n\nYou can run shell commands by prefacing them with an exclamation point (!).\n\nRemove any files with the same name as the file that you're going to download and then load a file named `README.md` from a URL into the filesystem for Spark:", "cell_type": "markdown", "metadata": {}}, {"source": "!rm README.md* -f\n!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/README.md", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2019-06-29 19:45:33--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/README.md\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3624 (3.5K) [text/plain]\nSaving to: 'README.md'\n\nREADME.md           100%[===================>]   3.54K  --.-KB/s    in 0s      \n\n2019-06-29 19:45:33 (96.7 MB/s) - 'README.md' saved [3624/3624]\n\n"}], "execution_count": 25}, {"source": "<a id=\"wordfile2\"></a>\n### 5.2 Create an RDD from the file\nUse the `textFile` method to create an RDD named `textfile_rdd` based on the `README.md` file. The RDD will contain one element for each line in the `README.md` file.\nAlso, count the number of lines in the RDD, which is the same as the number of lines in the text file. ", "cell_type": "markdown", "metadata": {}}, {"source": "textfile_rdd = sc.textFile(\"README.md\")\ntextfile_rdd.count()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "98"}, "execution_count": 26, "metadata": {}}], "execution_count": 26}, {"source": "<a id=\"wordfile3\"></a>\n### 5.3 Filter for a word \nFilter the RDD to keep only the elements that contain the word \"Spark\" with the `filter` transformation:", "cell_type": "markdown", "metadata": {}}, {"source": "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\nSpark_lines.first()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "'# Apache Spark'"}, "execution_count": 27, "metadata": {}}], "execution_count": 27}, {"source": "Count the number of elements in this filtered RDD and present the result as a concatenated string:", "cell_type": "markdown", "metadata": {}}, {"source": "print (\"The file README.md has \" + str(Spark_lines.count()) + \\\n\" of \" + str(textfile_rdd.count()) + \\\n\" Lines with word Spark in it.\")", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "The file README.md has 19 of 98 Lines with word Spark in it.\n"}], "execution_count": 28}, {"source": "<a id=\"wordfile4\"></a>\n### 5.4 Count the instances of a string at the beginning of words\nCount the number of times the substring \"Spark\" appears at the beginning of a word in the original text.\n\nHere's what you need to do: \n\n1. Run a `flatMap` transformation on the Spark_lines RDD and split on white spaces.\n2. Create an RDD with key-value pairs where the first element of the tuple is the word and the second element is the number 1.\n3. Run a `reduceByKey` method with the `add` function to count the number of instances of each word.<br>\n4. Filter the resulting RDD to keep only the elements that start with the word \"Spark\". In Python, the syntax to determine whether a string starts with a token is: `string.startswith(\"token\")` \n5. Display the resulting list of elements that start with \"Spark\".", "cell_type": "markdown", "metadata": {}}, {"source": "temp = Spark_lines.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1)).reduceByKey(add)\ntemp.filter(lambda x: x[0].startswith(\"Spark\")).collect()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[('SparkPi', 2),\n ('Spark](#building-spark).', 1),\n ('Spark', 14),\n ('Spark.', 1),\n ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1)]"}, "execution_count": 29, "metadata": {}}], "execution_count": 29}, {"source": "<a id=\"wordfile5\"></a>\n### 5.5 Count instances of a string within words\nNow filter and display the elements that contain the substring \"Spark\" anywhere in the word, instead of just at the beginning of words like the last section. Your result should be a superset of the previous result.\n\nThe Python syntax to determine whether a string contains a particular token is: `\"token\" in string`", "cell_type": "markdown", "metadata": {}}, {"source": "temp.filter(lambda x: \"Spark\" in x[0]).collect()", "cell_type": "code", "metadata": {}, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "[('SparkPi', 2),\n ('Spark](#building-spark).', 1),\n ('Spark', 14),\n ('Spark.', 1),\n ('Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n ('tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n  1)]"}, "execution_count": 30, "metadata": {}}], "execution_count": 30}, {"source": "<a id=\"numfile\"></a>\n## 6. Analyze numeric data from a file\nYou'll analyze a sample file that contains instructor names and scores. The file has the following format: Instructor Name,Score1,Score2,Score3,Score4. \nHere is an example line from the text file: \"Carlo,5,3,3,4\"\n\nAdd all scores and report on results:\n\n1. Download the file.\n1. Load the text file into an RDD.\n1. Run a transformation to create an RDD with the instructor names and the sum of the 4 scores per instructor.\n1. Run a second transformation to compute the average score for each instructor.\n1. Display the first 5 results.", "cell_type": "markdown", "metadata": {}}, {"source": "!rm Scores.txt* -f\n!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\n \nRaw_Rdd = sc.textFile(\"Scores.txt\")\n\nSumScores = Raw_Rdd.map(lambda l: l.split(\",\")).\\\nmap(lambda v : (v[0], int(v[1])+int(v[2])+int(v[3])+int(v[4])))\n\nFinal = SumScores.map(lambda avg: (avg[0],avg[1],avg[1]/4.0))\n\nFinal.take(5)", "cell_type": "code", "metadata": {"scrolled": true}, "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2019-06-29 19:46:07--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 75 [text/plain]\nSaving to: 'Scores.txt'\n\nScores.txt          100%[===================>]      75  --.-KB/s    in 0s      \n\n2019-06-29 19:46:08 (12.8 MB/s) - 'Scores.txt' saved [75/75]\n\n"}, {"output_type": "execute_result", "data": {"text/plain": "[('Carlo', 15, 3.75),\n ('Mokhtar', 15, 3.75),\n ('Jacques', 15, 3.75),\n ('Braden', 15, 3.75),\n ('Chris', 15, 3.75)]"}, "execution_count": 31, "metadata": {}}], "execution_count": 31}, {"source": "<a id=\"summary\"></a>\n## 7. Summary and next steps\n\nYou've learned how to work with data in RDDs to discover useful information.\n\nLook for the other notebooks in this series: \n - <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/5ad1c820f57809ddec9a040e37b2bd55\" target=\"_blank\" rel=\"noopener noreferrer\">Introduction to Spark lab, part 2: Querying data</a>\n - <a href=\"https://dataplatform.cloud.ibm.com/exchange/public/entry/view/5ad1c820f57809ddec9a040e37b4af08\" target=\"_blank\" rel=\"noopener noreferrer\">Introduction to Spark lab, part 3: Machine learning</a>\n\nDig deeper:\n - <a href=\"http://spark.apache.org/documentation.html\" target=\"_blank\" rel=\"noopener noreferrer\">Apache Spark documentation</a>\n - <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html\" target=\"_blank\" rel=\"noopener noreferrer\">PySpark documentation</a>", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "### Authors\n**Carlo Appugliese** is a Spark and Hadoop evangelist at IBM.<br>\n**Braden Callahan** is a Big Data Technical Specialist for IBM.<br>\n**Ross Lewis** is a Big Data Technical Sales Specialist for IBM.<br>\n**Mokhtar Kandil** is a World Wide Big Data Technical Specialist for IBM.", "cell_type": "markdown", "metadata": {}}, {"source": "<hr>\nCopyright &copy; IBM Corp. 2017-2019. This notebook and its source code are released under the terms of the MIT License.", "cell_type": "markdown", "metadata": {"collapsed": true}}, {"source": "<div style=\"background:#F5F7FA; height:110px; padding: 2em; font-size:14px;\">\n<span style=\"font-size:18px;color:#152935;\">Love this notebook? </span>\n<span style=\"font-size:15px;color:#152935;float:right;margin-right:40px;\">Don't have an account yet?</span><br>\n<span style=\"color:#5A6872;\">Share it with your colleagues and help them discover the power of Watson Studio!</span>\n<span style=\"border: 1px solid #3d70b2;padding:8px;float:right;margin-right:40px; color:#3d70b2;\"><a href=\"https://ibm.co/wsnotebooks\" target=\"_blank\" style=\"color: #3d70b2;text-decoration: none;\">Sign Up</a></span><br>\n</div>", "cell_type": "markdown", "metadata": {}}], "metadata": {"kernelspec": {"display_name": "Python 3.6 with Spark", "name": "python36", "language": "python3"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "3.6.8", "name": "python", "pygments_lexer": "ipython3", "file_extension": ".py", "codemirror_mode": {"version": 3, "name": "ipython"}}}, "nbformat": 4}
