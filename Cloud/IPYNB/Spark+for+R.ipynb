{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Spark for R to load data and run SQL queries\n",
    "This notebook introduces basic Spark concepts and helps you to start using Spark for R.\n",
    "\n",
    "Some familiarity with R is recommended.\n",
    "\n",
    "In this notebook, you'll use the publicly available **mtcars** data set from *Motor Trend* magazine to learn some basic R. You'll learn how to load data, create a Spark DataFrame, aggregate data, run mathematical formulas, and run SQL queries against the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "This notebook contains these main sections:\n",
    "\n",
    "1. [Load a DataFrame](#Load_a_DataFrame)\n",
    "2. [Initialize an SQLContext](#Initialize_an_SQLContext)\n",
    "3. [Create a Spark DataFrame](#Create_a_Spark_DataFrame)\n",
    "4. [Aggregate data after grouping by columns](#Aggregate_data_after_grouping_by_columns)\n",
    "5. [Operate on columns](#Operate_on_columns)\n",
    "6. [Run SQL queries from the Spark DataFrame](#Run_SQL_queries_from_the_Spark_DataFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Load_a_DataFrame'></a>\n",
    "## 1. Load a DataFrame\n",
    "A DataFrame is a distributed collection of data that is organized into named columns. The built-in R DataFrame called **mtcars** includes observations on the following 11 variables:\n",
    "\n",
    "`[, 1]\tmpg     Miles / (US) gallon`  \n",
    "`[, 2]\tcyl     Number of cylinders`  \n",
    "`[, 3]\tdisp\tDisplacement (cu. in.)`  \n",
    "`[, 4]\thp      Gross horsepower`  \n",
    "`[, 5]\tdrat    Rear axle ratio`  \n",
    "`[, 6]\twt      Weight (1000 lbs)`  \n",
    "`[, 7]\tqsec    1/4 mile time (seconds)`  \n",
    "`[, 8]\tvs      0 = V-engine, 1 = straight engine`  \n",
    "`[, 9]\tam      Transmission (0 = automatic, 1 = manual)`  \n",
    "`[,10]\tgear    Number of forward gears`  \n",
    "`[,11]\tcarb    Number of carburetors`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview the first 3 rows of the DataFrame by using the head() function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "head(mtcars, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the car name data, which appears in the row names, into an actual column so that Spark can read it as a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mtcars$car <- rownames(mtcars)\n",
    "mtcars <- mtcars[,c(12,1:11)]\n",
    "rownames(mtcars) <- 1:nrow(mtcars)\n",
    "head(mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Initialize_an_SQLContext'></a>\n",
    "## 2. Initialize an SQLContext\n",
    "To work with a DataFrame, you need an SQLContext. You create this SQLContext by using `sparkRSQL.init(sc)`. A SparkContext named sc, which has been created for you, is used to initialize the SQLContext:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sqlContext <- sparkRSQL.init(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Create_a_Spark_DataFrame'></a>\n",
    "## 3. Create a Spark DataFrame\n",
    "Using the SQLContext and the loaded local DataFrame, create a Spark DataFrame and print the schema, or structure, of the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdf <- createDataFrame(sqlContext, mtcars) \n",
    "printSchema(sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the content of the Spark DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SparkR::head(sdf, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try different ways of retrieving subsets of the data. For example, get the first 5 values in the **mpg** column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SparkR::head(select(sdf, sdf$mpg),5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the DataFrame to retain only rows with **mpg** values that are less than 18:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkR::head(SparkR::filter(sdf, sdf$mpg < 18))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Aggregate_data_after_grouping_by_columns'></a>\n",
    "## 4. Aggregate data after grouping by columns\n",
    "Spark DataFrames support a number of common functions to aggregate data after grouping. For example, you can compute the average weight of cars as a function of the number of cylinders:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SparkR::head(summarize(groupBy(sdf, sdf$cyl), wtavg = avg(sdf$wt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also sort the output from the aggregation to determine the most popular cylinder configuration in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "car_counts <-summarize(groupBy(sdf, sdf$cyl), count = n(sdf$wt))\n",
    "SparkR::head(arrange(car_counts, desc(car_counts$count)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Operate_on_columns'></a>\n",
    "## 5. Operate on columns\n",
    "SparkR provides a number of functions that you can apply directly to columns for data processing. In the following example, a basic arithmetic function converts lbs to metric tons:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sdf$wtTon <- sdf$wt * 0.45\n",
    "SparkR::head(select(sdf, sdf$car, sdf$wt, sdf$wtTon),6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Run_SQL_queries_from_the_Spark_DataFrame'></a>\n",
    "## 6. Run SQL queries from the Spark DataFrame\n",
    "You can register a Spark DataFrame as a temporary table and then run SQL queries over the data. The `sql` function enables an application to run SQL queries programmatically and returns the result as a DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "registerTempTable(sdf, \"cars\")\n",
    "\n",
    "highgearcars <- sql(sqlContext, \"SELECT car, gear FROM cars WHERE gear >= 5\")\n",
    "SparkR::head(highgearcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's it!\n",
    "You successfully completed this notebook! You learned how to load a DataFrame, view and filter the data, aggregate the data, perform operations on the data in specific columns, and run SQL queries against the data. For more information about Spark, see the [Spark Quick Start Guide](http://spark.apache.org/docs/latest/quick-start.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Want to learn more?\n",
    "### Free courses on <a href=\"https://bigdatauniversity.com/courses/?utm_source=tutorial-dashdb-python&utm_medium=github&utm_campaign=bdu/\" rel=\"noopener noreferrer\" target=\"_blank\">Big Data University</a>: <a href=\"https://bigdatauniversity.com/courses/?utm_source=tutorial-dashdb-python&utm_medium=github&utm_campaign=bdu\" rel=\"noopener noreferrer\" target=\"_blank\"><img src = \"https://ibm.box.com/shared/static/xomeu7dacwufkoawbg3owc8wzuezltn6.png\" width=600px> </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authors\n",
    "\n",
    "**Saeed Aghabozorgi**, PhD, is a Data Scientist in IBM with a track record of developing enterprise-level applications that substantially increases clients' ability to turn data into actionable knowledge. He is a researcher in the data mining field and an expert in developing advanced analytic methods like machine learning and statistical modelling on large data sets.\n",
    "\n",
    "**Polong Lin** is a Data Scientist at IBM in Canada. Under the Emerging Technologies division, Polong is responsible for educating the next generation of data scientists through Big Data University. Polong is a regular speaker in conferences and meetups, and holds an M.Sc. in Cognitive Psychology."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Â© 2016 Big Data University. This notebook and its source code are released under the terms of the <a href=\"https://bigdatauniversity.com/mit-license/\" rel=\"noopener noreferrer\" target=\"_blank\">MIT License</a>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R with Spark 1.6",
   "language": "R",
   "name": "r"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}