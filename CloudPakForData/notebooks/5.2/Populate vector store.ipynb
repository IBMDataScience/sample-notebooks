{"metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.11", "language": "python"}, "language_info": {"name": "python", "version": "3.11.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "#### -------------------------------------------------------------------------\n####\n#### Licensed Materials - Property of IBM\n#### \n#### Copyright IBM Corporation 2024, 2025. All Rights Reserved.\n#### \n#### Note to U.S. Government Users Restricted Rights:\n#### Use, duplication or disclosure restricted by GSA ADP Schedule\n#### Contract with IBM Corp.\n####\n#### -------------------------------------------------------------------------\n\n# Document Vectorization for Retrieval Augmented Generation\n\nThis Notebook is part of the watsonx Code Assistant on IBM Cloudpak for Data. It is  intended to be used to load code and documentation content from a Github repository into a vector store (here after referred to as indexing) for use with Retrieval Augmented Generation (here after refered to as RAG) feature in IBM watsonx Code Assistant (hereafter referred to as WCA). It pulls contents from the repository, chunks the content based on its type, enhances the (code) chunks with explanations,vectorizes the chunks and stores them in the vector store. It leverages the [wca_rag_lib Python library](https://github.com/IBMDataScience/sample-notebooks/blob/master/Files/wca_rag_lib-0.0.0.tar.gz) to perform these operations. You must download this library before running this notebook.\n\nContent that is vectorized through this Notebook is then used by watsonx Code Assistant to extract context for chat queries using the @repo and @docs references. \nFor details on how to configure RAG in WCA please refer to : https://www.ibm.com/docs/en/software-hub/5.2.x?topic=services-watsonx-code-assistant \n\n>**Please note <br><br> This Notebook populates code into your Opensearch vector store on IBM Cloudpak for Data. Please make sure you have all the approvals and permissions to to make a copy of your code outside the GitHub repository.**\n\n## How to use this Notebook \n\n**Indexing a repository for the first time:**<BR>\n\n1. Decide what is the content type that needs to be indexed  \n   There are two types of content that this Notebook can index:\n    - source code and \n    - documentation\n\n   Both of these cannot be done in the same run. Set the ```content_type``` variable accordingly to either ```code``` or ```docs```. (More details: [**Variables that control scope of the content to be indexed**](#scope) section below).\n\n2. Make sure the prerequsites are met<br>\n3. Review all the environment variables listed in this Notebook and make sure the mandatory ones are set.<br>\n4. Follow the instructions and run the rest of the cells.\n\nAs a best practice, if your repository contains over 10000 files, it is recommended to index specific folders and build the index incrementally. This can be done by setting the included_folders environment variable.\n   \n\n**Keeping the indexed content up to date**<br>\nFor accurate results with RAG it is essential that the contents of the vector store are updated periodically to keep the contents current with respect to changes in the Github repository.\n\nTo update files in an existing index : \n 1. Set the `content_type` to either `code` or `docs` based on the content.\n 2. Set the  ```reindex``` flag to ```True``` \n 3. Make sure the REPOSITORY_BRANCH is set to the same branch that was used to create the index.\n 4. Make sure you enter the ```index_name```. Typically the name of the github repository that was indexed.\n 5. Follow the instructions and run the cells.\n\n***Important Note:*** It is important that the REPOSITORY_BRANCH used for re-indexing matches with the branch that was used to create the index. Using a different branch for re-indexing will lead to content from two different branches to be indexed, leaving the vector store in a undesirable condition.\n\n## How Source Code Indexing Works<br>\n\nSource code is downloaded from the configured Github repository and chunked into functions.An explanation is generated for each function and attached to the respective chunk. The chunks are then vectorized using an embedding model. The vectorized chunk and the code in plain text are both stored in the vector store.\n\nSpecialized function extractors are used for code in Python, Java, C, C++, Javascript, Typescript and Go languages. Chunking is currently not supported for other languages through this notebook. \n\nIf you wish to index JSON, XML or other such files which are not documentation, you can use the file chunking strategy with ```content_type``` set to ```code```. Please refer to the environment variables section for more details.\n\n## How documentation indexing works<br>\nDocumentation chunking is supported for mark down, PDF, docx and pptx formats. The documentation content is downloaded from the configured Github repository and chunked based on a fixed chunk size. The chunks are vectorized and saved to a the vector store along with the plain text of the chunk.\n\n\n## Prerequisites : \n\nBefore you start running this Notebook, please make sure you have met the following requirements: \n\n - you have the latest instance of WCA setup on IBM Cloudpak for Data (5.2.2) and have followed the instructions for setting up Retrieval Augmented Generation (RAG). More details: https://www.ibm.com/docs/en/software-hub/5.2.x?topic=services-watsonx-code-assistant\n - you have an API key to access WCA.  More Details:  https://www.ibm.com/docs/en/software-hub/5.2.x?topic=wca-getting-started#getting-started__api-key__title__1\n \nIn addition to the above, you will also need the following : \n - a Personal Access Token (PAT) from your github account. You can obtain this from  https://github.com/settings/tokens.If you wish to use code from your organization repositories, please use the appropriate Enterprise Github URL.Please note that the generated PAT needs to have read permissions on your repositories. Make sure to select the repo permissions while generating the token.\n\n\n## Setting up the environment variables\n\n**Secrets and Vector Store Details (required)**<br>\nThe following environment variables are part of the parameter set that comes with the watsonx Code Assistant RAG project. These parameters often need to be set once and used for all the repositories that need to be indexed. These are : \n  - Vector store details (Note: Opensearch is the only supported vector store by WCA on IBM Cloudpak for Data ) \n  - WCA API Key\n  - WCA Username\n  - Github PAT\nLocate the parameter set RAG_INDEX_PARAMETERS in the Watson Studio project and update the above parameters.\n\n\nThe rest of the variables are to be set in the Notebook. The values of these variables may change based on the content that you index. \n\n**Github repository details (required)**<br>\nProvide the details of the repository that you want to index through the following variables:  \n- GITHUB_HOST\n- REPOSITORY_ORGANIZATION_NAME\n- REPOSITORY_NAME\n- REPOSITORY_BRANCH\n\nThis Notebook pulls content from Github based on these variables values. Note that the default branch is set to `main`. If you wish to use a different branch, please set up the REPOSITORY_BRANCH value accordingly.\n\n<a name=\"scope\"></a>  \n**Variables that control scope of the content to be indexed**<br>\nThe following optional variables can be setup in the  Notebook to take better control of the scope of the content that gets indexed. These variables may change based on what and how you wish to process the content of each repo. It is recommeneded to review these variable values before beginning to index any repository. No change is required if the default values are good enough for your repository.\n\n***content_type***<br>\nIndicates whether the content that is being indexed is code or documentation.<br>\nValid values : ```code```  or ```docs``` <br>\nDefault value : ```code```<br>\nChange this to ```docs``` if you are indexing documentation content such as markdown files, pdfs, docx or pptx.\n\n***included_folders***<br>\nThis is an optional parameter that limits the folders that are to be indexed. Specify the paths to the folders to be included in this list. If all the folders in the repo are not required to be indexed. This is useful for large applications which have all their code in a single repository.Indexing specific folders incrementally is recommended best practice for such repositories.<br>\nValid values : path to the folders to be included. Example : ```[\"src/java/app_server\" , \"src/java/messaging\"]``` <br>\nDefault value: empty list.<br>\n\n***excluded_folders***<br>\nThis is an optional parameter that excludes folders that are not to be indexed. Specify the paths to the folders to be excluded in this list. This is useful when you need one or two folders to be excluded from getting indexed. For example a test folder containing unit tests or functional tests is a typical candidate. Not indexing test content avoids using test content to being used as context instead of the actual code.<br>\nValid values: path to the folders to be included. Example : ```[\"src/tests\" , \"src/doc\"]``` <br>\nDefault value: empty list.<br>\n\n***ignore_file_keywords***<br>\nThis is an optional parameter that helps skip files with specific keywords in the file name from being indexed.<br>\nValid values: Any string that could be part of a file name<br>\nDefault value: empty list.<br>\n\n***chunking_strategy***<br>\nThis is an optional parameter that can be used when the ```content_type``` is ```code```. It is to be used when a file should not be chunked but indexed as-is.This is useful for files of type XML or JSON which may not have functions like code and also not natural language text like a mark down or PDF.<br>\nValid values: ```function``` or ```file```<br>\nDefault value: ```function```\n\n***file_types_for_file_strategy***<br>\nThis parameter is used in conjunction with ```chunking_strategy``` . If `chunking_strategy` is set to `file`, ```file_types_for_file_strategy``` indicates what file types should be processed with file strategy.<br>\nValid values: List of valid file extensions. Example : ```[\"json\" , \"xml\"]``` <br>\nDefault value: Empty list. No specific files are processed with file chunking strategy.\n\n**Indexing Variables**<br>\n\n***index_name (required)***<br>\nThis is a mandatory variable that needs to be set by the user. <br>\nThis indicates the name of the index in the vector store for the content that is being indexed. Typically this can be the name of the respository that is being indexed.However, when multiple repositories are being indexed into the same index (for example different documentation repositories grouped into one index), a more meaningful name can be chosen for the index. \nNote: If your vector store is Milvus, the index name should not have any hyphens. Replace the hyphens with an underscore.\n\n***create_new_index***\nThis flag indicates if the index mentioned in the `index_name` variable needs to be created in the vector store or is an existing index. Set this value to `False`, if you wish to reuse an existing index for the content that is being indexed in the current run.\nValid values: `True/False`\nDefault value: `True`\n\n***reindex***\nThis flag is used to indicate if the current run is for re-indexing an existing index in the vector store. When this is set to `True`, the changes in REPOSITORY_BRANCH since the previous push to the vector store are pulled from github and pushed to the vector store. The same index name that was used to index the repository previously needs to be mentioned in the `index_name` variable. \nValid values: `True/False`\nDefault value: `False`\n\n**Other variables**\n***root_url***\nThis is the host that is used to generate explanations for code chunks. Please refer to the RAG setup instructions in the WCA product documentation for instructions on how to obtain this URL: https://www.ibm.com/docs/en/software-hub/5.2.x?topic=services-watsonx-code-assistant\n\n## Troubleshooting\n\n- When something fails, it is generally due to some environment variable not being set correctly. This is the first thing to check if you see errors especically in the intial steps.\n\n- When the explanation generation step fails with some error but the notebook kernal is still running, try resuming it by re-running the cell once again. [Step 5b]\n\n- When the indexing step (6) fails but the notebook kernal is still running, this can be resumed by running the subsequent cell after setting the resume flag to True. [Step 6b]\n\n- If processing large repositories, especially those with PDF files is slow with the Notebook running on the default configuration of XXS then please use a higher environment configuration such as the XS or S for faster processing.\nThis can be done in the Watson Studio project by clicking on the vertical elipsis menu of the Notebook in the assets tab and selecting the Change environment option.\n\n- If you face issues while inserting project token, you can create a new project token by going to your Proejct -> Manage -> Switch to Access tokens tab -> Create new access token\n\nPlease try resuming the failed steps especially for large repositories so the entire process is not repeated once again.\n\n## Cleanup\nThis notebook creates intermediate jsonl files to help with resuming the steps if there are failures. These files can be found in the Watson Studio project. You can delete them manually after the indexing has been done.", "metadata": {}}, {"cell_type": "code", "source": "parameters_retrieved = {\n    \"GITHUB_API_TOKEN\": \"\",\n    \"WCA_APIKEY\": \"\",\n    \"WCA_USERNAME\": \"\",\n    \"DB_TYPE\": \"opensearch\",    \n    \"OS_URL\": \"\",\n    \"OS_USERNAME\": \"\",\n    \"OS_PASSWORD\": \"\"\n}", "metadata": {"id": "21ba0017-0fd1-4473-b66c-26b2d5a2ae5a"}, "outputs": [], "execution_count": 1}, {"cell_type": "code", "source": "# Set Required variables\n\n#########################################################################################################################\n# Please refer to the documentation in the beginning of this notebook and populate the following variables if required  #\n#########################################################################################################################\n\ncontent_type = \"code\"\n\ncreate_new_index = True\n\nindex_name = \"\" \n\n# set the boolean variable according to usecase\nreindex = False \nif reindex:\n    create_new_index =  False\n\n# These will be used to get explanations for code from WCA\nroot_url = \"https://<cpd hostname>\"\ndefault_base_url = f\"{root_url}/v2/wca/core/chat/text/generation\"\n\n# Repository information - this is the repository that will be processed and stored to the vector store.\n# provide the values for the following variables.\nGITHUB_HOST = \"\" #github.ibm.com, github.ibm.com\nREPOSITORY_ORGANIZATION_NAME = \"\"  #code-assistant\nREPOSITORY_NAME = \"\"\nREPOSITORY_BRANCH = \"main\"  # \"main\" # \"master\"\n\nexcluded_folders = []   # \"src/test\"\n\nincluded_folders = []   # \"docs\"\n\nignore_file_keywords = []\n\nchunking_strategy = \"function\"  # function # file\n\nfile_types_for_file_strategy = []   # 'XML',\"HTML\"\n", "metadata": {"id": "de1adbdd-07e4-4fd2-9642-0d75dca09e2a"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "if not index_name:\n    raise Exception(\"Index name not populated. Set index_name and try again.\")\nif set(excluded_folders) & set(included_folders):\n    raise Exception(\"`excluded_folders` and `included_folders` could not contain same folder path.\")", "metadata": {"id": "c62e980a-f5c5-479a-b131-ef56dbefefc9"}, "outputs": [], "execution_count": 29}, {"cell_type": "markdown", "source": "### 2. Install the Required Dependencies", "metadata": {}}, {"cell_type": "code", "source": "%pip install wca_rag_lib-0.0.0.tar.gz", "metadata": {"id": "94349c92-e40c-4e82-a58a-1505f24c8069", "tags": []}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "if GITHUB_HOST == 'github.com':\n    GITHUB_API_TOKEN = \"\"    \nelse:\n    GITHUB_API_TOKEN = parameters_retrieved['GITHUB_API_TOKEN']\n\napikey = parameters_retrieved['WCA_APIKEY']\nusername = parameters_retrieved['WCA_USERNAME']\ndb_type = parameters_retrieved['DB_TYPE']\n\nif db_type == \"opensearch\":    # Open Search Config\n    config = {\n        \"OS_URL\":parameters_retrieved[\"OS_URL\"],\n        \"OS_PASSWORD\":parameters_retrieved[\"OS_PASSWORD\"],\n        \"OS_USERNAME\":parameters_retrieved[\"OS_USERNAME\"]\n    }\n", "metadata": {"id": "a1b68022-c9b9-4b52-84f4-3f13dd3f2bc3"}, "outputs": [], "execution_count": 30}, {"cell_type": "markdown", "source": "### 3. Extract Repository Contents\n#### This step clones the repository contents and converts them into a parquet dataset.", "metadata": {}}, {"cell_type": "code", "source": "import dataclasses\nimport tempfile\nfrom pathlib import Path\nfrom typing import Callable, List\n\nfrom git import Repo\n\nfrom wca_rag_lib.extract_code.commands import (\n    extract_class_aware_functions_from_parquet,\n    extract_non_code_from_parquet,\n    extract_parquet\n)\nfrom wca_rag_lib.store_data.commands import encode_store\nfrom wca_rag_lib.store_data.es_reindex import update_es_index\nfrom wca_rag_lib.store_data.milvus_reindex import update_milvus_index\nimport wca_rag_lib.store_data.os_reindex as os_reindex\nimport importlib\nimportlib.reload(os_reindex)\nfrom wca_rag_lib.store_data.os_reindex import update_os_index\n\ndef get_latest_commit_id(file_dir):\n    try:\n        repo = Repo(file_dir, search_parent_directories=True)\n        commit_id = repo.head.commit.hexsha\n        print(\"Commit id found:\", commit_id)\n        return commit_id\n    except Exception as e:\n        print(\"Error retrieving commit ID:\", e)\n        return \"\"\n# Clone a git repository and extract parquet dataset.\n# Repo Details\nif GITHUB_API_TOKEN:\n    BASE_URL = f\"https://{GITHUB_API_TOKEN}@{GITHUB_HOST}\"\nelse:\n    BASE_URL = f\"https://{GITHUB_HOST}\"\n\nREPOSITORY_URL = f\"{BASE_URL}/{REPOSITORY_ORGANIZATION_NAME}/{REPOSITORY_NAME}\"\nOUTPUT_DIR = f\"{REPOSITORY_NAME}_ibm\"\nwith tempfile.TemporaryDirectory() as tmpdir:\n    dest = Path(tmpdir) / REPOSITORY_NAME\n    repo = Repo.clone_from(REPOSITORY_URL, dest)\n    repo.git.checkout(REPOSITORY_BRANCH)\n    latest_commit_id = get_latest_commit_id(dest)\n    repo_url = f\"https://{GITHUB_HOST}/{REPOSITORY_ORGANIZATION_NAME}/{REPOSITORY_NAME}/blob/{REPOSITORY_BRANCH}\"\n    new_entries_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_new_update_ibm.jsonl\"\n    extract_parquet(\n        input_path=tmpdir,\n        output_path=f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_ibm.parquet\",\n        repo_url=repo_url,\n        repo_name=REPOSITORY_NAME,\n        included_folders=included_folders,\n        excluded_folders=excluded_folders,\n        ignore_file_keywords=ignore_file_keywords,\n    )\n    \n    print(\"Parquet dataset output complete\")\n\n    if reindex == True:\n        content_type_lower = content_type.lower()\n        content_type_lower = 'noncode' if content_type_lower == 'docs' else content_type_lower\n        if db_type == \"elasticsearch\":\n            update_es_index(index_name,dest,repo_url,new_entries_path,config=config,db_type=db_type, chunking_strategy=chunking_strategy, file_types_for_file_strategy=file_types_for_file_strategy, content_type=content_type_lower)\n        elif db_type == \"opensearch\":\n            update_os_index(index_name,dest,repo_url,new_entries_path,connection_params=config, chunking_strategy=chunking_strategy, file_types_for_file_strategy=file_types_for_file_strategy, content_type=content_type_lower)\n        else:\n            update_milvus_index(index_name,dest,repo_url,new_entries_path,config=config, chunking_strategy=chunking_strategy, file_types_for_file_strategy=file_types_for_file_strategy, content_type=content_type_lower)", "metadata": {"id": "a7c10019-2a19-4874-b35f-819f292db7fe"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### 4. Extract Functions from Repository Content\n#### Transform the parquet dataset into a JSONL file for further processing.", "metadata": {}}, {"cell_type": "code", "source": "# This function is used to extract functions from a parquet dataset.\n\nif reindex == False :     \n    # This function is used to extract functions from a parquet dataset.\n    FUNCTIONS_DATASET_PATH = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_functions_ibm.jsonl\"\n\n    # reuse_input_path_parquet = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_ibm.parquet\".replace('-', '_').replace('/', '__')\n    if content_type.lower() == \"code\":\n        extract_class_aware_functions_from_parquet(\n            input_path=f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_ibm.parquet\",  # input_path = reuse_input_path_parquet,\n            output_path=FUNCTIONS_DATASET_PATH,\n            path_field='path',\n            code_field='contents',\n            language_field='language',\n            chunking_strategy = chunking_strategy,\n            file_types_for_file_strategy = file_types_for_file_strategy,\n        )\n\n        with open(FUNCTIONS_DATASET_PATH, 'r') as f:\n            num_lines = sum(1 for line in f)\n        print(f\"Total number of records to be processed : {num_lines}\")\n    else:\n        extract_non_code_from_parquet(\n        input_path=f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_ibm.parquet\",  # input_path = reuse_input_path_parquet,\n        output_path=FUNCTIONS_DATASET_PATH,\n        path_field='path',\n        code_field='contents',\n        language_field='language'\n        )\n\n        with open(FUNCTIONS_DATASET_PATH, 'r') as f:\n            num_lines = sum(1 for line in f)\n        print(f\"Total number of records to be processed : {num_lines}\")", "metadata": {"id": "7295524b-eb93-45eb-9e25-2609b9fefd1c"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### 5. Enhance functions with Explanations (CODE)\n\n#### In this step, we send the previously extracted functions to WCA to generate detailed explanations for each function. The generated explanations are then stored in the `explanation` column of the enhanced dataset, which will later be used for embedding. ", "metadata": {}}, {"cell_type": "markdown", "source": "5a. **Prepare for explanation generation**", "metadata": {}}, {"cell_type": "code", "source": "# This function retrieves explanations for code snippets in a functions-JSONL file and stores it into enchanced-functions-JSONL file.\n\nimport json\nimport os\n\nexplanation_payload = \"\"\"\nExplain this code : \n```{language}\n{text}\n```\n\"\"\"\n\ninput_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_functions_ibm.jsonl\" if reindex == False else new_entries_path\n\n# use the following file path if the kernel was restarted and the extracted jsonl file for the functions is lost\n# reuse_input_file_path_jsonl = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_functions_ibm.jsonl\".replace('-', '_').replace('/', '__')\n\noutput_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_enhanced_functions_ibm.jsonl\"  # location for the updated file\n\n# location for file which stores errored records. This contains records for which explanation could not be generated due to some error during explanation generation\nlog_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_error_functions_ibm.jsonl\" \n\nindex_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_index_ibm.jsonl\"\n\nif os.path.exists(output_file_path):\n    os.remove(output_file_path)\n\nif os.path.exists(log_file_path):\n    os.remove(log_file_path)\n\nif os.path.exists(index_file_path):\n    os.remove(index_file_path)", "metadata": {"id": "12277a14-bd6a-467e-8abe-55dd9062f0fa"}, "outputs": [], "execution_count": 33}, {"cell_type": "markdown", "source": "5b. **Explanation generation**<br><br>\n    **If the following step fails, simply re-run this cell to resume the enhancement process.**", "metadata": {}}, {"cell_type": "code", "source": "if content_type.lower() == \"code\":    \n    import asyncio\n    import aiofiles\n    import json\n    import os\n    import sys\n    import datetime\n    from tenacity import retry, wait_exponential, stop_after_attempt, RetryError\n    from tqdm.asyncio import tqdm\n    from wca_rag_lib.external.watson import get_code_chunks_explanation, explain_response_processor\n\n    try:\n        from IPython.display import display, HTML, clear_output\n        IS_JUPYTER = True\n    except ImportError:\n        IS_JUPYTER = False\n\n\n    # please do not increase this number, it may cause performance issues\n    MAX_CONCURRENT_TASKS = 10\n\n\n    RETRY_SETTINGS = {\n        \"wait\": wait_exponential(min=1, max=10),\n        \"stop\": stop_after_attempt(3)\n    }\n\n\n    LOG_BATCH_SIZE = 100\n    IO_FLUSH_INTERVAL = 5\n\n\n    stats = {\n        \"success\": 0,\n        \"no_exp\": 0,        \n        \"no_exp_e\": 0,      \n        \"skipped\": 0,\n        \"errors\": 0,\n        \"total_processed\": 0,\n        \"total_input\": 0\n    }\n\n    stats_lock = asyncio.Lock()\n\n    log_queue = asyncio.Queue()\n\n    error_messages = set()\n\n\n    @retry(**RETRY_SETTINGS)\n    async def explain_code(formatted_text, get_code_chunks_explanation, api_key):\n        \"\"\"\n        Calls the external function to get code explanations with retry logic.\n        This function assumes `get_code_chunks_explanation` is a synchronous or\n        asynchronous function that takes formatted_text and api_key and returns\n        (is_error: bool, response: str).\n        \"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, get_code_chunks_explanation, formatted_text, api_key, username, root_url, default_base_url)\n\n    async def count_processed_chunks(output_file_path):\n        \"\"\"Counts lines in the output file to determine how many chunks are already processed.\"\"\"\n        count = 0\n        if os.path.exists(output_file_path):\n            async with aiofiles.open(output_file_path, 'r') as outf:\n                async for _ in outf:\n                    count += 1\n        return count\n\n    async def count_input_records(input_file_path):\n        \"\"\"Counts total lines in the input file.\"\"\"\n        count = 0\n        async with aiofiles.open(input_file_path, 'r') as infile:\n            async for _ in infile:\n                count += 1\n        return count\n\n    async def read_input_records(input_file_path, start_index):\n        \"\"\"\n        Asynchronously reads and yields JSON records from the input file,\n        starting from a specified index. Handles JSON decoding errors.\n        \"\"\"\n        i = 0\n        async with aiofiles.open(input_file_path, 'r') as infile:\n            async for line in infile:\n                if i < start_index:\n                    i += 1\n                    continue\n                try:\n                    yield i, json.loads(line)\n                except json.JSONDecodeError as e:\n                    await log_error(f\"JSONDecodeError at line {i}: {str(e)} - Skipping record.\")\n                    async with stats_lock:\n                        stats[\"errors\"] += 1\n                        stats[\"total_processed\"] += 1\n                finally:\n                    i += 1\n            else:\n                if i == 0:\n                    raise Exception(f\"`{input_file_path}` has no data.\")\n\n    async def log_message(message: str, level: str):\n        \"\"\"Puts a log message into the log_queue.\"\"\"\n        timestamp = datetime.datetime.now().isoformat()\n        log_entry = f\"[{level}] {timestamp} - {message}\"\n        if level == \"ERROR\":\n            print(log_entry, flush=True)\n            global error_messages\n            error_messages.add(message.split(' - ')[0])\n        await log_queue.put(log_entry)\n\n    async def log_error(message: str):\n        await log_message(message, \"ERROR\")\n\n    async def log_info(message: str):\n        await log_message(message, \"INFO\")\n\n    def get_stats_html():\n        \"\"\"Generates an HTML string for displaying live statistics.\"\"\"\n        return f\"\"\"\n        <div style=\"font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; font-size: 14px; margin-top: 10px; padding: 10px; border: 1px solid #e0e0e0; border-radius: 8px; background-color: #f9f9f9;\">\n            <h4 style=\"margin-top: 0; color: #333;\">Processing Statistics:</h4>\n            <table style=\"width: 100%; border-collapse: collapse;\">\n                <tr><td style=\"padding: 4px 0;\">Total Input:</td><td style=\"text-align: right; font-weight: bold;\">{stats['total_input']}</td></tr>\n                <tr><td style=\"padding: 4px 0;\">Processed:</td><td style=\"text-align: right; font-weight: bold;\">{stats['total_processed']}</td></tr>\n                <tr><td style=\"padding: 4px 0;\">Success:</td><td style=\"text-align: right; color: #28a745; font-weight: bold;\">{stats['success']}</td></tr>\n                <tr><td style=\"padding: 4px 0;\">No Explanation (Lang):</td><td style=\"text-align: right; color: #ffc107; font-weight: bold;\">{stats['no_exp']}</td></tr>\n                <tr><td style=\"padding: 4px 0;\">No Explanation (API Err):</td><td style=\"text-align: right; color: #dc3545; font-weight: bold;\">{stats['no_exp_e']}</td></tr>\n                <tr><td style=\"padding: 4px 0;\">Skipped (Dir):</td><td style=\"text-align: right; color: #6c757d; font-weight: bold;\">{stats['skipped']}</td></tr>\n                <tr><td style=\"padding: 4px 0;\">Other Errors:</td><td style=\"text-align: right; color: #dc3545; font-weight: bold;\">{stats['errors']}</td></tr>\n            </table>\n        </div>\n        \"\"\"\n\n    async def log_writer_task(log_file_path: str, queue: asyncio.Queue, flush_interval: int):\n        \"\"\"\n        Dedicated task to write log messages to the log file in batches.\n        \"\"\"\n        buffer = []\n        last_flush_time = asyncio.get_event_loop().time()\n        async with aiofiles.open(log_file_path, \"a\") as logf:\n            while True:\n                try:\n                    item = await asyncio.wait_for(queue.get(), timeout=flush_interval)\n                    buffer.append(item + \"\\n\")\n                    queue.task_done()\n\n                    if len(buffer) >= LOG_BATCH_SIZE:\n                        await logf.write(\"\".join(buffer))\n                        buffer.clear()\n                        last_flush_time = asyncio.get_event_loop().time()\n                except asyncio.TimeoutError:\n                    if buffer:\n                        await logf.write(\"\".join(buffer))\n                        buffer.clear()\n                        last_flush_time = asyncio.get_event_loop().time()\n                except asyncio.CancelledError:\n                    if buffer:\n                        await logf.write(\"\".join(buffer))\n                    break\n\n    async def process_chunk(index, record, explanation_payload, output_file_path: str, # Changed to direct path\n                            get_code_chunks_explanation, api_key, pbar,\n                            explain_response_processor, stats_display_handle=None):\n        \"\"\"\n        Processes a single code chunk: checks for exclusion/support, calls API,\n        writes result to output file directly, and updates global statistics and progress bar.\n        \"\"\"\n        try:\n            path = record.get(\"path\", \"\")\n            lang = record.get(\"language\", \"\").lower()\n\n            formatted_text = explanation_payload.format(text=record['text'], language=record['language'])\n\n            async with asyncio.Semaphore(MAX_CONCURRENT_TASKS):\n                is_error, response = await explain_code(formatted_text, get_code_chunks_explanation, api_key)\n\n            if is_error:\n                record['explanation'] = explain_response_processor(lang, path, record.get(\"full_code\", \"\"), \"\")\n                async with stats_lock:\n                    stats[\"no_exp_e\"] += 1\n                await log_error(f\"Record {index}: No explanation - API error during explanation.\")\n            else:\n                record['explanation'] = explain_response_processor(lang, path, record.get(\"full_code\", \"\"), response)\n                async with stats_lock:\n                    stats[\"success\"] += 1\n                await log_info(f\"Record {index}: Successfully explained.\")\n\n            async with aiofiles.open(output_file_path, \"a\") as outf: # Direct write to output\n                await outf.write(json.dumps(record) + \"\\n\")\n\n        except RetryError as re:\n            async with stats_lock:\n                stats[\"errors\"] += 1\n            await log_error(f\"Record {index}: RetryError - {str(re)}\")\n        except Exception as e:\n            import traceback\n            async with stats_lock:\n                stats[\"errors\"] += 1\n            tb = traceback.format_exc()\n            await log_error(f\"Record {index}: Unhandled Exception - {str(e)}\\n{tb}\")\n        finally:\n            async with stats_lock:\n                stats[\"total_processed\"] += 1\n                pbar.set_description(f\"Processing ({stats['total_processed']}/{stats['total_input']})\")\n                if IS_JUPYTER and stats_display_handle:\n                    stats_display_handle.update(HTML(get_stats_html()))\n            pbar.update(1)\n\n\n    async def run_all(\n        input_file_path: str,\n        output_file_path: str,\n        log_file_path: str,\n        explanation_payload: str,\n        get_code_chunks_explanation,\n        api_key: str,\n        explain_response_processor\n    ):\n        \"\"\"\n        Main function to orchestrate the code chunk processing.\n        \"\"\"\n        await log_info(f\"Starting code explanation process at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n        await log_info(f\"Input file: {input_file_path}\")\n        await log_info(f\"Output will be written to: {output_file_path}\")\n        await log_info(f\"Log file: {log_file_path}\")\n        await log_info(f\"Max concurrent tasks: {MAX_CONCURRENT_TASKS}\")\n        await log_info(f\"Log batch size: {LOG_BATCH_SIZE}\")\n\n        if not os.path.exists(input_file_path):\n            print(f\"[ERROR] Input file not found: {input_file_path}\", flush=True)\n            return\n\n        processed_chunks_on_start = await count_processed_chunks(output_file_path)\n        total_chunks = await count_input_records(input_file_path)\n\n        async with stats_lock:\n            stats[\"total_input\"] = total_chunks\n            stats[\"total_processed\"] = processed_chunks_on_start\n\n        remaining_chunks = total_chunks - processed_chunks_on_start\n\n        print(f\"\\nResuming from chunk {processed_chunks_on_start}...\", flush=True)\n        print(f\"Total chunks in input: {total_chunks}, Remaining to process: {remaining_chunks}\\n\", flush=True)\n\n        stats_display_handle = None\n        if IS_JUPYTER:\n            clear_output(wait=True)\n            stats_display_handle = display(HTML(get_stats_html()), display_id=True)\n            print(\"\\n\", flush=True)\n\n        log_writer_task_obj = asyncio.create_task(log_writer_task(log_file_path, log_queue, IO_FLUSH_INTERVAL))\n\n        pbar = None\n        try:\n            pbar = tqdm(total=remaining_chunks, desc=\"Processing\", unit=\" chunk\", position=0, leave=True)\n            pbar.update(processed_chunks_on_start)\n\n            processing_tasks = []\n            async for i, record in read_input_records(input_file_path, processed_chunks_on_start):\n                task = process_chunk(\n                    i, record, explanation_payload, output_file_path,\n                    get_code_chunks_explanation, api_key, pbar,\n                    explain_response_processor, stats_display_handle\n                )\n                processing_tasks.append(asyncio.create_task(task))\n\n                if len(processing_tasks) >= MAX_CONCURRENT_TASKS * 2:\n                    done, pending = await asyncio.wait(processing_tasks, return_when=asyncio.FIRST_COMPLETED)\n                    processing_tasks = list(pending)\n\n            if processing_tasks:\n                await asyncio.gather(*processing_tasks)\n\n        finally:\n            if pbar:\n                pbar.close()\n\n        await log_queue.join()\n        log_writer_task_obj.cancel()\n        try:\n            await log_writer_task_obj\n        except asyncio.CancelledError:\n            pass\n\n        if IS_JUPYTER and stats_display_handle:\n            stats_display_handle.update(HTML(get_stats_html()))\n        elif not IS_JUPYTER:\n            print(get_stats_html())\n\n        print(\"\\n\" + \"=\" * 40, flush=True)\n        print(\"        Processing Complete!\", flush=True)\n        print(\"=\" * 40 + \"\\n\", flush=True)\n\n        print(f\"Total chunks processed: {stats['total_processed']}/{stats['total_input']}\", flush=True)\n        print(f\"  - Successful explanations: {stats['success']}\", flush=True)\n        print(f\"  - No explanation (Unsupported Language): {stats['no_exp']}\", flush=True)\n        print(f\"  - No explanation (API Error after retries): {stats['no_exp_e']}\", flush=True)\n        print(f\"  - Skipped (Excluded Directory): {stats['skipped']}\", flush=True)\n        print(f\"  - Other Errors (e.g., JSON parsing): {stats['errors']}\", flush=True)\n        print(f\"\\nOutput file: {output_file_path}\", flush=True)\n        print(f\"Log file: {log_file_path}\", flush=True)\n\n        if error_messages:\n            print(\"\\n--- Summary of Unique Errors Encountered ---\", flush=True)\n            for msg in sorted(list(error_messages)):\n                print(f\"- {msg}\", flush=True)\n        else:\n            print(\"\\nNo significant errors encountered during processing.\", flush=True)\n\n        print(\"\\n\" + \"=\" * 40, flush=True)\n        await log_info(f\"Process finished at {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n\n\n    await run_all(\n        input_file_path=input_file_path,\n        output_file_path=output_file_path,\n        log_file_path=log_file_path,\n        explanation_payload=explanation_payload,\n        get_code_chunks_explanation=get_code_chunks_explanation,\n        api_key=apikey,\n        explain_response_processor=explain_response_processor\n    )", "metadata": {"id": "103a5f70-e401-40e9-9e60-31bf46c20ab4", "tags": []}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### 6. Create Index and save enhanced chunks to vector store\n#### When content_type is \"code\", this step creates index using the enhanced JSONL file, where embeddings are generated based on the explanation column.\n#### When content_type is \"docs\", this step creates index using the functions JSONL file, where embeddings are generated based on the text column.", "metadata": {}}, {"cell_type": "markdown", "source": "6a. **Create or update existing index** <br><br>\n - **Populate the index-name below with a valid string**<br><br>\n - **Recommended index-name is REPOSITORY_NAME.lower()**<br><br>\n - **Note that index-name should not have hyphens or any special characters, use underscore as word separator if required.**", "metadata": {}}, {"cell_type": "code", "source": "# Creates or updates index for a given dataset.\n\nimport os\n\nEMBEDDING_MODEL = \"msmarco-MiniLM-L-6-v3\"\n\n# the JSON field name used to generate embeddings\nif content_type.lower() == \"code\":\n    EMBEDDING_FIELD = [\"explanation\"]  \n    dataset_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_enhanced_functions_ibm.jsonl\"\nelse:\n    EMBEDDING_FIELD = [\"text\"]  \n    dataset_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_functions_ibm.jsonl\"\n\ntry:\n    index = encode_store(\n        model_name=EMBEDDING_MODEL,\n        dataset_file_path=dataset_file_path,\n        index_file_path=index_file_path,\n        dataset_name=index_name,\n        content_key=EMBEDDING_FIELD,\n        db_type=db_type, \n        config=config,\n        create_new_index=create_new_index, # create new index\n        resume=False,\n    )\n    print(f\"INDEX NAME : {index}\")\nexcept KeyboardInterrupt:\n    print(\"Process interrupted by user.\")\nexcept Exception as e:\n    print(f\"An unexpected error occurred: {e}\")", "metadata": {"id": "2778a70f-e3c1-46b5-a61d-e8245e066f03"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "6b. **Retry failed indexing process**<br><br>\n- **Run the below step only if previous step has failed.**<br><br>\n- **Set the resume flag to True and index name same as previous step before running this step** ", "metadata": {}}, {"cell_type": "code", "source": "# Resume index for a given dataset.\n# Set to true incase indexing fails and you want to resume it\n\nresume = False\nimport os\nEMBEDDING_MODEL = \"msmarco-MiniLM-L-6-v3\"\n# the JSON field name used to generate embeddings\nif content_type.lower() == \"code\":\n    EMBEDDING_FIELD = [\"explanation\"]  \n    dataset_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_enhanced_functions_ibm.jsonl\"\nelse:\n    EMBEDDING_FIELD = [\"text\"]  \n    dataset_file_path = f\"{OUTPUT_DIR}/{REPOSITORY_NAME}_functions_ibm.jsonl\"\n\nif resume : \n    try:\n        index = encode_store(\n            model_name=EMBEDDING_MODEL,\n            dataset_file_path=dataset_file_path,\n            index_file_path=index_file_path,\n            dataset_name=index_name,\n            content_key=EMBEDDING_FIELD,\n            db_type=db_type, \n            config = config,\n            create_new_index=False, # update existing index\n            resume = resume\n        )\n        print(f\"INDEX NAME : {index}\")\n    except KeyboardInterrupt:\n        print(\"Process interrupted by user.\")\n    except Exception as e:\n        print(f\"An unexpected error occurred: {e}\")", "metadata": {"id": "43070a7e-55a6-438d-b4aa-5e525461445e"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# ADD MASTER RECORD\n\nimport importlib\nimport wca_rag_lib.external.records as records\n\n# Reload the whole module\nimportlib.reload(records)\n\n# Import fresh functions\nfrom wca_rag_lib.external.records import (\n    add_custom_record_to_milvus,\n    add_custom_record_to_es,\n    add_custom_record_to_os,\n)\n\n# Save latest commit id for recent update in milvus master record for future use.\nif db_type.lower() == \"milvus\":\n    add_custom_record_to_milvus(\n        texts=[\"WCA_MASTER_RECORD\"],\n        commit_ids=[latest_commit_id],\n        languages=[\"WCA_MASTER_RECORD\"],\n        urls=[\"\"],\n        model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n        config=config,\n        index=index\n    )\n# Save latest commit id for recent update in elasticsearch master record for future use.\nelif db_type.lower() == \"elasticsearch\":\n    add_custom_record_to_es(\n    texts=[\"WCA_MASTER_RECORD\"],\n    commit_ids=[latest_commit_id],\n    languages=[\"WCA_MASTER_RECORD\"],\n    urls=[\"\"],\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    config=config,\n    index_name=index\n)\n# Save latest commit id for recent update in opensearch master record for future use.\nelif db_type.lower() == \"opensearch\":\n    add_custom_record_to_os(\n    texts=[\"WCA_MASTER_RECORD\"],\n    commit_ids=[latest_commit_id],\n    languages=[\"WCA_MASTER_RECORD\"],\n    urls=[\"\"],\n    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n    config=config,\n    index_name=index\n)\nprint(f\"Latest Commit ID updated in MASTER record {latest_commit_id}\")", "metadata": {"id": "372fe1ac-2e59-4419-a42b-1785742205f3"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "Copyright \u00a9 2024, 2025. This notebook and its source code are released under the terms of the MIT License.\n\nAuthor: Rishi S ribalaji@in.ibm.com", "metadata": {"id": "7bc1e580-7887-47e1-8252-7b30bb6c59af"}, "attachments": {}}]}